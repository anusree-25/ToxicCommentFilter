{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "083af620-1cd2-4ca5-801d-ba2aadd254a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\anusr\\\\ToxicCommentFilter\\\\notebooks'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4422be-293f-4c12-a5eb-f96872c05443",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3eb951ab-5063-4757-89fb-217c4c15d55c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\anusr\\\\ToxicCommentFilter\\\\notebooks'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "836d5b6f-4d05-42e6-859b-e9b75057b7b5",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/english/english.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m english_df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m../data/english/english.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m malayalam_df = pd.read_csv(\u001b[33m'\u001b[39m\u001b[33m../data/malayalam/malayalam.csv\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      5\u001b[39m manglish_df = pd.read_csv(\u001b[33m'\u001b[39m\u001b[33m../data/manglish/manglish.csv\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '../data/english/english.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "english_df = pd.read_csv('../data/english/english.csv')\n",
    "malayalam_df = pd.read_csv('../data/malayalam/malayalam.csv')\n",
    "manglish_df = pd.read_csv('../data/manglish/manglish.csv')\n",
    "\n",
    "print(\"English:\", english_df.shape)\n",
    "print(\"Malayalam:\", malayalam_df.shape)\n",
    "print(\"Manglish:\", manglish_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463f5322-67c2-434c-85af-c656d654c963",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94207b93-eea8-4e3c-8b10-a7053b4056a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_df = pd.read_csv(\n",
    "    '../data/english/english.csv',\n",
    "    sep=',',\n",
    "    encoding='utf-8'\n",
    ")\n",
    "\n",
    "english_df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e231fc-cf9d-4cbd-8593-78e7eed0dd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107773ab-8d0f-4a19-83d4-b4f9e98aaf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load CSV (as-is)\n",
    "english_df = pd.read_csv(\"data/english/english.csv\")\n",
    "\n",
    "# Split the single column into multiple columns\n",
    "english_df = english_df[english_df.columns[0]].str.split(\",\", expand=True)\n",
    "\n",
    "# Rename columns\n",
    "english_df.columns = [\"ID\", \"Comment\", \"Label\", \"Category\"]\n",
    "\n",
    "# Check result\n",
    "english_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c884abea-cc1d-4965-bb22-68e36cdc61a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "english_df = pd.read_csv(\"data/english/english.csv\", sep=\",\")\n",
    "print(english_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291b0b08-60f3-440b-987e-04b895b35bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_df = pd.read_csv(\"data/english/english.csv\")\n",
    "\n",
    "english_df = english_df[english_df.columns[0]].str.split(\",\", expand=True)\n",
    "\n",
    "print(english_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43678d0-44e2-4ef1-9886-16a82a6fce88",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_df = pd.read_csv(\"data/english/english.csv\")\n",
    "\n",
    "english_df = english_df[english_df.columns[0]].str.split(\";\", expand=True)\n",
    "\n",
    "print(english_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2e6de2-ef34-4f51-8b1a-ad5c35359c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dad2700-68cc-44ad-ab55-d3705bfbd056",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "english_df = pd.read_csv(\"../data/english/english.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d62ff9-ea25-41c4-b0c9-625a5e142d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_df = pd.read_csv(\"../data/english/english.csv\", sep=\";\")\n",
    "print(english_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e903e2-cc1c-446a-9f0f-918803b499ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_df = english_df[english_df.columns[0]].str.split(\";\", expand=True)\n",
    "english_df.columns = [\"ID\", \"Comment\", \"Label\", \"Category\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f9b8ea-0953-4f41-82ba-2b506df5c420",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/english/english.csv\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for i in range(5):\n",
    "        print(f.readline())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54841a0c-216d-49a5-b9af-49933b871920",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Read file as single column\n",
    "df_raw = pd.read_csv(\"../data/english/english.csv\", header=None)\n",
    "\n",
    "# Step 2: Remove surrounding quotes\n",
    "df_raw[0] = df_raw[0].str.strip('\"')\n",
    "\n",
    "# Step 3: Split into columns using comma\n",
    "english_df = df_raw[0].str.split(\",\", expand=True)\n",
    "\n",
    "# Step 4: Assign column names\n",
    "english_df.columns = [\"ID\", \"Comment\", \"Label\", \"Category\"]\n",
    "\n",
    "# Step 5: View result\n",
    "english_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b80cd2a-cc07-4f7b-b6ae-40248f3fdd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the duplicated header row\n",
    "english_df = english_df[english_df[\"ID\"] != \"ID\"]\n",
    "\n",
    "# Reset index\n",
    "english_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Check result\n",
    "english_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891f93a0-3570-4bf1-a377-dba741d9e8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_df.to_csv(\"../data/english/english_clean.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cfa68c-70b1-43eb-bd39-4981caf6f1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_df.info()\n",
    "english_df[\"Label\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b9f413-e686-4d69-bdd6-8559110b605f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Read file as single column\n",
    "df_raw = pd.read_csv(\"../data/malayalam/malayalam.csv\", header=None)\n",
    "\n",
    "# Step 2: Remove surrounding quotes\n",
    "df_raw[0] = df_raw[0].str.strip('\"')\n",
    "\n",
    "# Step 3: Split into columns using comma\n",
    "malayalam_df = df_raw[0].str.split(\",\", expand=True)\n",
    "\n",
    "# Step 4: Assign column names\n",
    "malayalam_df.columns = [\"ID\", \"Comment\", \"Label\", \"Category\"]\n",
    "\n",
    "# Step 5: View result\n",
    "malayalam_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4165a2fa-cc93-46e7-b981-a14ecf9e68d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the duplicated header row\n",
    "malayalam_df = malayalam_df[malayalam_df[\"ID\"] != \"ID\"]\n",
    "\n",
    "# Reset index\n",
    "malayalam_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Check result\n",
    "malayalam_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f825bb2b-e4c0-43db-8df2-1939e2d99d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "malayalam_df.info()\n",
    "malayalam_df[\"Label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106cd51a-15fb-4b94-84d9-00cfeec257aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Read file as single column\n",
    "df_raw = pd.read_csv(\"../data/manglish/manglish.csv\", header=None)\n",
    "\n",
    "# Step 2: Remove surrounding quotes\n",
    "df_raw[0] = df_raw[0].str.strip('\"')\n",
    "\n",
    "# Step 3: Split into columns using comma\n",
    "manglish_df = df_raw[0].str.split(\",\", expand=True)\n",
    "\n",
    "# Step 4: Assign column names\n",
    "manglish_df.columns = [\"ID\", \"Comment\", \"Label\", \"Category\"]\n",
    "\n",
    "# Step 5: View result\n",
    "manglish_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba59cd5c-8086-4810-ac1a-b74494dd0dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the duplicated header row\n",
    "manglish_df = manglish_df[manglish_df[\"ID\"] != \"ID\"]\n",
    "\n",
    "# Reset index\n",
    "manglish_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Check result\n",
    "manglish_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d542ba7a-42d3-47bb-8d58-25430c357b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "manglish_df.info()\n",
    "manglish_df[\"Label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c91ff8-10c9-4655-93a4-1420a36c8832",
   "metadata": {},
   "outputs": [],
   "source": [
    "malayalam_df.to_csv(\"../data/malayalam/malayalam_clean.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff13c224-6710-44a3-bfd6-4adce8c6e1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "manglish_df.to_csv(\"../data/manglish/manglish_clean.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e87c607-f663-4b07-816c-edd04dbc118a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_label(label):\n",
    "    return 1 if label == \"Toxic\" else 0\n",
    "\n",
    "english_df[\"label_binary\"] = english_df[\"Label\"].apply(encode_label)\n",
    "malayalam_df[\"label_binary\"] = malayalam_df[\"Label\"].apply(encode_label)\n",
    "manglish_df[\"label_binary\"] = manglish_df[\"Label\"].apply(encode_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d0b94e-841b-428a-896f-5371a575d15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)   # remove links\n",
    "    text = re.sub(r\"[^a-zA-Z\\u0D00-\\u0D7F\\s]\", \"\", text)  # keep English + Malayalam\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9127567d-e319-4b77-966e-ac69bf00ca91",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_df[\"clean_comment\"] = english_df[\"Comment\"].apply(clean_text)\n",
    "malayalam_df[\"clean_comment\"] = malayalam_df[\"Comment\"].apply(clean_text)\n",
    "manglish_df[\"clean_comment\"] = manglish_df[\"Comment\"].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a86491c-7426-486e-867f-598b587ab036",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_df.to_csv(\"../data/english/english_clean.csv\", index=False)\n",
    "malayalam_df.to_csv(\"../data/malayalam/malayalam_clean.csv\", index=False)\n",
    "manglish_df.to_csv(\"../data/manglish/manglish_clean.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b98643c-21dc-4940-8315-bb2e9dd65de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard columns\n",
    "columns = ['text', 'label']\n",
    "english_df.columns = columns\n",
    "malayalam_df.columns = columns  # if you have Malayalam or other datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc2c5dd-595c-4d00-8b33-5d8a19af7c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11b33a5-8dc2-4988-8168-8855243a1054",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_df = pd.read_csv(\"../data/english/english_clean.csv\")\n",
    "malayalam_df = pd.read_csv(\"../data/malayalam/malayalam_clean.csv\")\n",
    "manglish_df = pd.read_csv(\"../data/manglish/manglish_clean.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f8e337-f70a-4245-a996-7bf3b709ec39",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_df[\"language\"] = \"english\"\n",
    "malayalam_df[\"language\"] = \"malayalam\"\n",
    "manglish_df[\"language\"] = \"manglish\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0fe4e9-026f-4dba-b049-5a2c9867e1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_to_binary(label):\n",
    "    return 1 if label.strip().lower() == \"toxic\" else 0\n",
    "\n",
    "english_df[\"label_binary\"] = english_df[\"Label\"].apply(label_to_binary)\n",
    "malayalam_df[\"label_binary\"] = malayalam_df[\"Label\"].apply(label_to_binary)\n",
    "manglish_df[\"label_binary\"] = manglish_df[\"Label\"].apply(label_to_binary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4ad3c4-ced2-44fa-bb91-b0dc5f5cbded",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_df = english_df[[\"Comment\", \"label_binary\", \"language\"]]\n",
    "malayalam_df = malayalam_df[[\"Comment\", \"label_binary\", \"language\"]]\n",
    "manglish_df = manglish_df[[\"Comment\", \"label_binary\", \"language\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b1a7d6-ca74-4c95-a8ef-42deec24f9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat(\n",
    "    [english_df, malayalam_df, manglish_df],\n",
    "    ignore_index=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed827f8-9e37-49a7-ac60-d429d523d507",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = combined_df.sample(frac=1, random_state=42).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef6382d-83ce-4821-bdfd-7ee1c0371f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c954508e-7e94-46c7-a7fd-69edff9985aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40f1946-17df-4cc2-bba1-2126821735d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df[\"language\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05f9dbe-cacf-4d05-9461-2f7db53424ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df[\"label_binary\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e6a173-36db-4e95-bf61-518e74650ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.to_csv(\"../data/combined_dataset_clean.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc25a39a-3ec3-4fe2-b05f-c7554591ea74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load merged dataset\n",
    "df = pd.read_csv(\"../data/combined_dataset_clean.csv\")\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()                    # lowercase\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)  # remove URLs\n",
    "    text = re.sub(r\"[^a-zA-Z\\u0D00-\\u0D7F\\s]\", \"\", text)  # keep English + Malayalam\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()    # remove extra spaces\n",
    "    return text\n",
    "\n",
    "# Apply cleaning\n",
    "df[\"clean_comment\"] = df[\"Comment\"].apply(clean_text)\n",
    "\n",
    "# View result\n",
    "df[[\"Comment\", \"clean_comment\"]].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6204cd07-6750-44ea-9614-ea4b238a8432",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../data/combined_dataset_preprocessed.csv\", index=False)\n",
    "print(\"✅ Preprocessed dataset saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a55d8e0-e0d2-41b1-bf42-49cac011ecfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load preprocessed dataset\n",
    "df = pd.read_csv(\"../data/combined_dataset_preprocessed.csv\")\n",
    "\n",
    "# Features and labels\n",
    "X = df[\"clean_comment\"]\n",
    "y = df[\"Label\"]\n",
    "\n",
    "# Train-test split (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "# Check sizes\n",
    "print(\"Training samples:\", len(X_train))\n",
    "print(\"Testing samples:\", len(X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e702e1-3298-4bff-8eb9-c9b976057d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097f4adb-715b-4b28-aafc-44d64e7911e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"label_binary\"]  ✅\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23022808-77a4-4f6c-921c-469f3e4813d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"../data/combined_dataset_preprocessed.csv\")\n",
    "\n",
    "# Features and labels\n",
    "X = df[\"clean_comment\"]\n",
    "y = df[\"label_binary\"]\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(\"Training samples:\", len(X_train))\n",
    "print(\"Testing samples:\", len(X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2767cba9-5141-4f90-bc7a-fa59f83bf980",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize TF-IDF vectorizer\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=5000,      # limit features\n",
    "    ngram_range=(1, 2),     # unigrams + bigrams\n",
    "    stop_words=\"english\"    # remove common English words\n",
    ")\n",
    "\n",
    "# Fit on training data, transform both\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "# Check shapes\n",
    "print(\"TF-IDF Train shape:\", X_train_tfidf.shape)\n",
    "print(\"TF-IDF Test shape:\", X_test_tfidf.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de51ca9-97eb-40fa-a734-28f6b1afa815",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Initialize model\n",
    "model = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    class_weight=\"balanced\"   # handles class imbalance\n",
    ")\n",
    "\n",
    "# Train model\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "print(\"Model training completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfeffb85-e573-4f45-94f6-8d294ba9b0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred = model.predict(X_test_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97092e99-259f-48f7-8d8e-4b4de76f8756",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Detailed metrics\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion matrix\n",
    "print(\"\\nConfusion Matrix:\\n\")\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c8d217-1756-4d5c-9c0b-4c9b967c749e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9d380e-d125-4fd7-bf50-2df024cf3ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(vectorizer, \"../models/tfidf_vectorizer.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b5d0dd-d327-4728-8a99-5e4d5acedf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf\n",
    "tfidf_vectorizer\n",
    "vectorizer_tfidf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a246b2-93e4-47a4-a9bc-dd11966607d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1020bd63-5369-4823-a181-75144e1fbc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(tfidf, \"../models/tfidf_vectorizer.pkl\")\n",
    "joblib.dump(model, \"../models/toxic_comment_model.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced463df-b315-459e-82d8-4b0bdb673e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1️⃣ Load the existing CSV\n",
    "csv_file = \"english_clean.csv\"  # replace with your actual file path\n",
    "data = pd.read_csv(csv_file)\n",
    "\n",
    "# 2️⃣ Define the new comment row\n",
    "# For example, a non-toxic comment\n",
    "new_row = {\"comment\": \"You are amazing!\", \"label\": 0}  # label 0 = Non-Toxic, 1 = Toxic\n",
    "\n",
    "# 3️⃣ Append the new row\n",
    "data = data.append(new_row, ignore_index=True)  # or use pd.concat in newer pandas\n",
    "\n",
    "# 4️⃣ Save back to CSV\n",
    "data.to_csv(csv_file, index=False)\n",
    "\n",
    "print(\"New comment added successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b34893-a7fd-4c76-a8e8-9c584d6e63cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1️⃣ Load the existing CSV\n",
    "csv_file = \"combined_dataset_clean.csv\"\n",
    "data = pd.read_csv(csv_file)\n",
    "\n",
    "# 2️⃣ Define the new row\n",
    "new_row = {\"comment\": \"You are amazing\", \"label_binary\": 0, \"language\": \"english\"}\n",
    "\n",
    "# 3️⃣ Append the new row\n",
    "data = pd.concat([data, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "# 4️⃣ Save back to CSV\n",
    "data.to_csv(csv_file, index=False)\n",
    "\n",
    "print(\"New row added successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc1ca6a-6cf8-4f2f-bfcd-b5333c265fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getcwd())  # This shows the folder Python is looking in\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b9c989-9c22-4f4a-980c-c59e37540028",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 1️⃣ Define folder and CSV file path\n",
    "folder = r\"C:\\Users\\anusr\\ToxicCommentFilter\\notebooks\\data\"\n",
    "csv_file = os.path.join(folder, \"combined_dataset_clean.csv\")\n",
    "\n",
    "# 2️⃣ Create folder if it doesn't exist\n",
    "os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "# 3️⃣ Check if CSV exists; if yes, load it; if no, create a new DataFrame\n",
    "if os.path.exists(csv_file):\n",
    "    data = pd.read_csv(csv_file)\n",
    "else:\n",
    "    # Create new DataFrame with the correct columns\n",
    "    data = pd.DataFrame(columns=[\"comment\", \"label_binary\", \"language\"])\n",
    "\n",
    "# 4️⃣ Define the new row\n",
    "new_row = {\"comment\": \"You are amazing\", \"label_binary\": 0, \"language\": \"english\"}\n",
    "\n",
    "# 5️⃣ Append the new row\n",
    "data = pd.concat([data, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "# 6️⃣ Save back to CSV\n",
    "data.to_csv(csv_file, index=False)\n",
    "\n",
    "print(f\"New row added successfully at {csv_file}!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf964614-5b44-4c1b-8eeb-761677dac81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show last 5 rows of the dataset\n",
    "print(data.tail())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac23d809-7785-406c-8a18-b9ea84cf2e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Load saved vectorizer and model\n",
    "tfidf = joblib.load(r\"C:\\Users\\anusr\\ToxicCommentFilter\\models\\tfidf_vectorizer.pkl\")\n",
    "model = joblib.load(r\"C:\\Users\\anusr\\ToxicCommentFilter\\models\\toxic_comment_model.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12289586-225d-4932-bec1-f07985a2aac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_toxicity(comment):\n",
    "    \"\"\"\n",
    "    Predict if a comment is Toxic or Non-Toxic\n",
    "    \"\"\"\n",
    "    comment_vec = tfidf.transform([comment])\n",
    "    prediction = model.predict(comment_vec)[0]  # 0 or 1\n",
    "    return \"Toxic\" if prediction == 1 else \"Non-Toxic\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f776f3f-427c-429f-945c-31aec63bda74",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_comment = \"You are amazing\"\n",
    "\n",
    "result = predict_toxicity(new_comment)\n",
    "print(f'Comment: \"{new_comment}\" → Prediction: {result}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe6aeb1-7dcf-45cc-bf85-7dcaf999b1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# 1️⃣ Define folder and CSV path\n",
    "folder = r\"C:\\Users\\anusr\\ToxicCommentFilter\\notebooks\\data\"\n",
    "csv_file = os.path.join(folder, \"combined_dataset_clean.csv\")\n",
    "\n",
    "# 2️⃣ Create folder if not exists\n",
    "os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "# 3️⃣ Load existing dataset if exists, else create new\n",
    "if os.path.exists(csv_file):\n",
    "    data = pd.read_csv(csv_file)\n",
    "else:\n",
    "    data = pd.DataFrame(columns=[\"comment\", \"label_binary\", \"language\"])\n",
    "\n",
    "# 4️⃣ List of 100+ non-toxic comments\n",
    "non_toxic_comments = [\n",
    "    \"You are amazing\",\n",
    "    \"I appreciate your help\",\n",
    "    \"Thank you so much\",\n",
    "    \"This is really helpful\",\n",
    "    \"I love this\",\n",
    "    \"Well done\",\n",
    "    \"Good job\",\n",
    "    \"Keep up the great work\",\n",
    "    \"You made my day\",\n",
    "    \"That was very kind of you\",\n",
    "    \"I really like this\",\n",
    "    \"Fantastic work\",\n",
    "    \"You are very talented\",\n",
    "    \"This is excellent\",\n",
    "    \"I am impressed\",\n",
    "    \"Such a nice effort\",\n",
    "    \"You are awesome\",\n",
    "    \"Great explanation\",\n",
    "    \"You are so creative\",\n",
    "    \"I am grateful for your support\",\n",
    "    \"This is very informative\",\n",
    "    \"You are a genius\",\n",
    "    \"I love your style\",\n",
    "    \"Keep going\",\n",
    "    \"So inspiring\",\n",
    "    \"I admire your dedication\",\n",
    "    \"You did a wonderful job\",\n",
    "    \"This is top quality\",\n",
    "    \"Thank you for sharing\",\n",
    "    \"You are very helpful\",\n",
    "    \"I appreciate your kindness\",\n",
    "    \"You are brilliant\",\n",
    "    \"I really admire this\",\n",
    "    \"This made me smile\",\n",
    "    \"You are very smart\",\n",
    "    \"You did great\",\n",
    "    \"Much appreciated\",\n",
    "    \"You are so thoughtful\",\n",
    "    \"You are very generous\",\n",
    "    \"I like this idea\",\n",
    "    \"This is very useful\",\n",
    "    \"Well explained\",\n",
    "    \"You are so supportive\",\n",
    "    \"I am thankful for you\",\n",
    "    \"You are very clever\",\n",
    "    \"Nice effort\",\n",
    "    \"I really appreciate this\",\n",
    "    \"This is wonderful\",\n",
    "    \"Excellent work\",\n",
    "    \"You are very encouraging\",\n",
    "    \"I like your approach\",\n",
    "    \"You are outstanding\",\n",
    "    \"So thoughtful\",\n",
    "    \"Very impressive\",\n",
    "    \"I respect your effort\",\n",
    "    \"You are so friendly\",\n",
    "    \"You did amazing work\",\n",
    "    \"This is fantastic\",\n",
    "    \"You are very polite\",\n",
    "    \"Great effort\",\n",
    "    \"This is remarkable\",\n",
    "    \"I admire your talent\",\n",
    "    \"You are very reliable\",\n",
    "    \"This is very encouraging\",\n",
    "    \"You are so inspiring\",\n",
    "    \"Thanks for your guidance\",\n",
    "    \"You are very kind\",\n",
    "    \"This is very professional\",\n",
    "    \"You are extremely helpful\",\n",
    "    \"I like how you did this\",\n",
    "    \"You are so intelligent\",\n",
    "    \"This is very well done\",\n",
    "    \"You are very responsible\",\n",
    "    \"You are very organized\",\n",
    "    \"This is extremely valuable\",\n",
    "    \"You are a true friend\",\n",
    "    \"This is amazing work\",\n",
    "    \"You are very patient\",\n",
    "    \"I appreciate your honesty\",\n",
    "    \"You are so understanding\",\n",
    "    \"You are excellent at this\",\n",
    "    \"This is very motivating\",\n",
    "    \"You are a role model\",\n",
    "    \"You are extremely talented\",\n",
    "    \"I appreciate your effort\",\n",
    "    \"You are very encouraging\",\n",
    "    \"This is highly impressive\",\n",
    "    \"You are very trustworthy\",\n",
    "    \"You are very creative\",\n",
    "    \"This is very supportive\",\n",
    "    \"You are very knowledgeable\",\n",
    "    \"I am thankful for your guidance\",\n",
    "    \"You are very polite\",\n",
    "    \"You are amazing at this\",\n",
    "    \"This is very positive\",\n",
    "    \"You are highly appreciated\",\n",
    "    \"I admire your work\",\n",
    "    \"You are so helpful\",\n",
    "    \"This is outstanding\",\n",
    "    \"You are very dependable\",\n",
    "    \"You did an excellent job\",\n",
    "    \"I am grateful for this\",\n",
    "    \"You are very cooperative\",\n",
    "    \"You are truly kind\",\n",
    "    \"This is extremely helpful\",\n",
    "    \"You are very friendly\",\n",
    "    \"You are very diligent\",\n",
    "    \"This is highly valuable\",\n",
    "    \"You are very professional\",\n",
    "    \"I appreciate your support\",\n",
    "    \"You are extremely impressive\",\n",
    "    \"You are very resourceful\",\n",
    "    \"This is so motivating\",\n",
    "    \"You are wonderful\",\n",
    "    \"You are very attentive\",\n",
    "    \"This is very encouraging\"\n",
    "]\n",
    "\n",
    "# 5️⃣ Convert list to DataFrame\n",
    "new_rows = pd.DataFrame({\n",
    "    \"comment\": non_toxic_comments,\n",
    "    \"label_binary\": [0]*len(non_toxic_comments),\n",
    "    \"language\": [\"english\"]*len(non_toxic_comments)\n",
    "})\n",
    "\n",
    "# 6️⃣ Append to existing data\n",
    "data = pd.concat([data, new_rows], ignore_index=True)\n",
    "\n",
    "# 7️⃣ Save back to CSV\n",
    "data.to_csv(csv_file, index=False)\n",
    "\n",
    "print(f\"{len(non_toxic_comments)} non-toxic comments added successfully to {csv_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876c337f-29a3-4cc0-b0d4-e5f02a36a55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "more_non_toxic_comments = [\n",
    "    \"You are very encouraging\", \n",
    "    \"You are so positive\", \n",
    "    \"I like your dedication\", \n",
    "    \"This is very creative\", \n",
    "    \"You are very thoughtful\", \n",
    "    \"I really admire this work\", \n",
    "    \"You are very understanding\", \n",
    "    \"You are very patient\", \n",
    "    \"This is really inspiring\", \n",
    "    \"You are extremely kind\", \n",
    "    \"You are a true inspiration\", \n",
    "    \"I appreciate your guidance\", \n",
    "    \"This is very friendly\", \n",
    "    \"You are extremely helpful\", \n",
    "    \"I like your approach\", \n",
    "    \"You are very motivating\", \n",
    "    \"You did a fantastic job\", \n",
    "    \"You are highly skilled\", \n",
    "    \"This is excellent work\", \n",
    "    \"You are very reliable\", \n",
    "    \"I admire your professionalism\", \n",
    "    \"You are so generous\", \n",
    "    \"This is very polite\", \n",
    "    \"You are very creative\", \n",
    "    \"I appreciate your contribution\", \n",
    "    \"You are very respectful\", \n",
    "    \"You are very supportive\", \n",
    "    \"This is amazing effort\", \n",
    "    \"You are extremely intelligent\", \n",
    "    \"I like your style\", \n",
    "    \"You are very talented\", \n",
    "    \"You are extremely thoughtful\", \n",
    "    \"This is very well explained\", \n",
    "    \"You are amazing at this\", \n",
    "    \"You are very clever\", \n",
    "    \"I really like this idea\", \n",
    "    \"You are very trustworthy\", \n",
    "    \"This is very positive\", \n",
    "    \"You are so encouraging\", \n",
    "    \"You are very dependable\", \n",
    "    \"I like your work ethic\", \n",
    "    \"This is very kind\", \n",
    "    \"You are very attentive\", \n",
    "    \"You are extremely insightful\", \n",
    "    \"I really like your explanation\", \n",
    "    \"You are very friendly\", \n",
    "    \"You are very diligent\", \n",
    "    \"This is highly impressive\", \n",
    "    \"You are very motivating\", \n",
    "    \"You did an outstanding job\", \n",
    "    \"You are very organized\", \n",
    "    \"This is very helpful\", \n",
    "    \"You are highly creative\", \n",
    "    \"I like your perspective\", \n",
    "    \"You are very responsible\", \n",
    "    \"You are very encouraging\", \n",
    "    \"This is very inspiring\", \n",
    "    \"You are very cooperative\", \n",
    "    \"You are highly appreciated\", \n",
    "    \"I really admire your dedication\", \n",
    "    \"You are very resourceful\", \n",
    "    \"This is excellent\", \n",
    "    \"You are extremely friendly\", \n",
    "    \"You are very generous\", \n",
    "    \"I like your efforts\", \n",
    "    \"You are very professional\", \n",
    "    \"This is very motivating\", \n",
    "    \"You are extremely thoughtful\", \n",
    "    \"You are very kind\", \n",
    "    \"You are very approachable\", \n",
    "    \"I really appreciate your work\", \n",
    "    \"You are very talented\", \n",
    "    \"You are very attentive\", \n",
    "    \"This is extremely useful\", \n",
    "    \"You are highly skilled\", \n",
    "    \"You are very creative\", \n",
    "    \"I admire your insights\", \n",
    "    \"You are very supportive\", \n",
    "    \"This is really valuable\", \n",
    "    \"You are very encouraging\", \n",
    "    \"You are very intelligent\", \n",
    "    \"I really like your approach\", \n",
    "    \"You are extremely reliable\", \n",
    "    \"You are very thoughtful\", \n",
    "    \"This is amazing work\", \n",
    "    \"You are extremely kind\", \n",
    "    \"You are very inspiring\", \n",
    "    \"I appreciate your dedication\", \n",
    "    \"You are very cooperative\", \n",
    "    \"You are very polite\", \n",
    "    \"This is very well done\", \n",
    "    \"You are extremely helpful\", \n",
    "    \"You are very creative\", \n",
    "    \"I like your guidance\", \n",
    "    \"You are very encouraging\", \n",
    "    \"You are very skilled\", \n",
    "    \"This is very professional\", \n",
    "    \"You are extremely positive\", \n",
    "    \"You are very motivating\", \n",
    "    \"I really appreciate this effort\", \n",
    "    \"You are very trustworthy\", \n",
    "    \"You are very talented\", \n",
    "    \"This is very insightful\", \n",
    "    \"You are extremely dependable\", \n",
    "    \"You are very friendly\", \n",
    "    \"I admire your work ethic\", \n",
    "    \"You are very diligent\", \n",
    "    \"This is very encouraging\", \n",
    "    \"You are extremely helpful\", \n",
    "    \"You are very generous\", \n",
    "    \"I really admire this effort\", \n",
    "    \"You are very attentive\", \n",
    "    \"You are highly professional\", \n",
    "    \"This is very thoughtful\", \n",
    "    \"You are very intelligent\", \n",
    "    \"You are extremely supportive\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660a3901-94ed-459f-81a0-6a331ad80781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame\n",
    "new_rows = pd.DataFrame({\n",
    "    \"comment\": more_non_toxic_comments,\n",
    "    \"label_binary\": [0]*len(more_non_toxic_comments),\n",
    "    \"language\": [\"english\"]*len(more_non_toxic_comments)\n",
    "})\n",
    "\n",
    "# Append to existing dataset\n",
    "data = pd.concat([data, new_rows], ignore_index=True)\n",
    "\n",
    "# Save back to CSV\n",
    "data.to_csv(csv_file, index=False)\n",
    "\n",
    "print(f\"{len(more_non_toxic_comments)} more non-toxic comments added successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f44273-2b30-40f5-8b23-b0debd98968c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# 1️⃣ Define folder and CSV path\n",
    "folder = r\"C:\\Users\\anusr\\ToxicCommentFilter\\notebooks\\data\"\n",
    "csv_file = os.path.join(folder, \"combined_dataset_clean.csv\")\n",
    "\n",
    "# 2️⃣ Create folder if not exists\n",
    "os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "# 3️⃣ Load existing dataset if exists, else create new\n",
    "if os.path.exists(csv_file):\n",
    "    data = pd.read_csv(csv_file)\n",
    "else:\n",
    "    data = pd.DataFrame(columns=[\"comment\", \"label_binary\", \"language\"])\n",
    "\n",
    "# 4️⃣ List of 200+ Malayalam non-toxic comments\n",
    "malayalam_non_toxic_comments = [\n",
    "    \"നിനക്ക് വളരെ നല്ലതാണ്\", \"നന്ദി സഹായത്തിന്\", \"നിനക്ക് നന്ദി\", \"ഇത് വളരെ ഉപകാരപ്രദമാണ്\",\n",
    "    \"ഞാൻ ഇത് ഇഷ്ടപ്പെടുന്നു\", \"ശ്രേഷ്ഠം ചെയ്തിരിക്കുന്നു\", \"നല്ല ജോബ്\", \"നിന്റെ മികച്ച പ്രവർത്തനം തുടരൂ\",\n",
    "    \"നിനക്ക് എന്റെ ദിനം സന്തോഷകരമാക്കി\", \"നീ വളരെ ദയവുള്ളവനാണ്\", \"ഇത് എനിക്ക് വളരെ ഇഷ്ടപ്പെട്ടു\",\n",
    "    \"ശ്രേഷ്ഠ പ്രവർത്തനം\", \"നീ വളരെ കഴിവുള്ളവനാണ്\", \"ഇത് വളരെ മികച്ചതാണ്\", \"ഞാൻ प्रभावितനാണ്\",\n",
    "    \"അതി മനോഹരമായ ശ്രമം\", \"നീ അസാധാരണമാണ്\", \"നല്ല وضിസങ്കലനം\", \"നീ വളരെ സൃഷ്ടിപരനാണ്\",\n",
    "    \"ഞാൻ നിന്റെ പിന്തുണയ്ക്ക് നന്ദിയുള്ളവൻ\", \"ഇത് വളരെ വിവരപരമാണ്\", \"നീ ഒരു ജീനിയസ് ആണ്\",\n",
    "    \"ഞാൻ നിന്റെ സ്റ്റൈൽ ഇഷ്ടപ്പെടുന്നു\", \"തുടരുക\", \"ഇത് പ്രചോദനമാണ്\", \"ഞാൻ നിന്റെ സമർപ്പണം അഭിനന്ദിക്കുന്നു\",\n",
    "    \"നീ അത്ഭുതകരമായി ചെയ്തു\", \"ഇത് ടോപ്പ് ക്വാളിറ്റി ആണ്\", \"നിന്റെ പങ്കുവെച്ചതിനുള്ള നന്ദി\", \"നീ വളരെ സഹായകരനാണ്\",\n",
    "    \"ഞാൻ നിന്റെ ദയയ്ക്ക് നന്ദിയുള്ളവൻ\", \"നീ ബുദ്ധിമാൻ ആണ്\", \"ഞാൻ ഇത് virkelig ആസ്വദിക്കുന്നു\", \"ഇത് എനിക്ക് സന്തോഷം നൽകി\",\n",
    "    \"നീ വളരെ നൂതനനാണ്\", \"നല്ല ശ്രമം\", \"ഞാൻ ഇത് വളരെ അഭിനന്ദിക്കുന്നു\", \"ഇത് അത്ഭുതകരമാണ്\", \n",
    "    \"ശ്രേഷ്ഠ പ്രവർത്തനം\", \"നീ വളരെ പ്രോത്സാഹകനാണ്\", \"ഞാൻ നിന്റെ സമീപനം ഇഷ്ടപ്പെടുന്നു\", \"നീ എത്ര മികച്ചവനാണ്\",\n",
    "    \"അത്ഭുതകരം\", \"മനോഹരം\", \"ഞാൻ നിന്റെ പരിശ്രമം ആദരിക്കുന്നു\", \"നീ വളരെ സൗഹൃദപരനാണ്\",\n",
    "    \"നീ അത്ഭുതകരമായി ചെയ്തു\", \"ഇത് വളരെ പ്രചോദനാത്മകമാണ്\", \"നിന്റെ മാർഗ്ഗനിർദ്ദേശത്തിന് നന്ദി\",\n",
    "    \"നീ വളരെ ദയാളു\", \"ഇത് വളരെ പ്രൊഫഷണൽ ആണ്\", \"നീ അത്ഭുതകരമായി സഹായിച്ചു\", \"ഞാൻ നിന്റെ സമീപനം ഇഷ്ടപ്പെടുന്നു\",\n",
    "    \"നീ വളരെ ബുദ്ധിമാനാണ്\", \"ഇത് വളരെ നല്ല വിവരമാണു\", \"നീ വളരെ ഉത്തരവാദിത്വമുള്ളവനാണ്\", \"നീ വളരെ ക്രമസമയനാണ്\",\n",
    "    \"ഇത് വളരെ മൂല്യവത്തായിരിക്കുന്നു\", \"നീ ഒരു സത്യസ്നേഹം സുഹൃത്ത് ആണ്\", \"ഇത് അസാധാരണ പ്രവർത്തനം\",\n",
    "    \"നീ വളരെ ക്ഷമയുള്ളവനാണ്\", \"ഞാൻ നിന്റെ സത്യസന്ധതക്ക് നന്ദിയുള്ളവൻ\", \"നീ വളരെ മനസ്സിലാക്കുന്നവനാണ്\",\n",
    "    \"നീ വളരെ മികച്ചത്\", \"ഇത് വളരെ പ്രചോദനാത്മകമാണ്\", \"നീ ഒരു മാതൃകാ വ്യക്തിയാണ്\", \"നീ അത്ഭുതകരമായ കഴിവുള്ളവനാണ്\",\n",
    "    \"ഞാൻ നിന്റെ ശ്രമം അഭിനന്ദിക്കുന്നു\", \"നീ വളരെ പ്രോത്സാഹകനാണ്\", \"ഇത് വളരെ ശ്രദ്ധേയമാണ്\", \"നീ വളരെ വിശ്വസനീയനാണ്\",\n",
    "    \"നീ വളരെ സൃഷ്ടിപരനാണ്\", \"ഇത് വളരെ പിന്തുണയുള്ളതാണ്\", \"നീ വളരെ അറിവുള്ളവനാണ്\", \"ഞാൻ നിന്റെ മാർഗ്ഗനിർദ്ദേശത്തിന് നന്ദിയുള്ളവൻ\",\n",
    "    \"നീ വളരെ നിഷ്പക്ഷനാണ്\", \"നീ അത്ഭുതകരമായി പ്രവർത്തിക്കുന്നു\", \"ഇത് വളരെ ഗുണകരമാണ്\", \"നീ വളരെ സഹായകരനാണ്\",\n",
    "    \"നീ വളരെ സുഹൃത്തായിരിക്കുന്നു\", \"ഇത് വളരെ പ്രചോദനമാണ്\", \"നീ വളരെ ശ്രദ്ധയുള്ളവനാണ്\", \"ഇത് വളരെ പ്രോത്സാഹകമാണ്\",\n",
    "    \"നീ വളരെ സജീവനാണ്\", \"നീ അത്ഭുതകരമായി നിർവഹിക്കുന്നു\", \"ഞാൻ നിന്റെ പ്രവർത്തനം അഭിനന്ദിക്കുന്നു\", \"നീ വളരെ പ്രൊഫഷണലാണ്\",\n",
    "    \"നീ വളരെ നല്ല വ്യക്തിയാണ്\", \"ഇത് വളരെ പ്രചോദനാത്മകമാണ്\", \"നീ വളരെ ബുദ്ധിമാനാണ്\", \"നീ അത്ഭുതകരമായി സഹായിക്കുന്നു\",\n",
    "    \"ഞാൻ നിന്റെ ശ്രമം അഭിനന്ദിക്കുന്നു\", \"നീ വളരെ സൃഷ്ടിപരനാണ്\", \"ഇത് വളരെ ഉൽസാഹജനകമാണ്\", \"നീ വളരെ വല്ലഭനാണ്\",\n",
    "    \"ഇത് വളരെ സഹായകരമാണ്\", \"നീ അത്ഭുതകരമായി ചെയ്തു\", \"നീ വളരെ കരുണയുള്ളവനാണ്\", \"ഞാൻ നിന്റെ കഠിനാധ്വാനം ആദരിക്കുന്നു\",\n",
    "    \"നീ വളരെ പ്രോത്സാഹനമാണ്\", \"ഇത് വളരെ മനോഹരമാണ്\", \"നീ അത്ഭുതകരമായി ചെയ്തു\", \"ഞാൻ നിന്റെ അറിവ് ആദരിക്കുന്നു\",\n",
    "    \"നീ വളരെ സഹായി ആണ്\", \"നീ വളരെ ശ്രദ്ധയുള്ളവനാണ്\", \"ഇത് വളരെ ഗുണകരമാണ്\", \"നീ അത്ഭുതകരമാണ്\", \n",
    "    \"നീ വളരെ പ്രതിബദ്ധനാണ്\", \"ഞാൻ നിന്റെ പ്രവർത്തനം അഭിനന്ദിക്കുന്നു\", \"നീ വളരെ നിഷ്പക്ഷനാണ്\", \"ഇത് വളരെ മനോഹരമാണ്\",\n",
    "    \"നീ വളരെ പ്രൊഫഷണലാണ്\", \"നീ അത്ഭുതകരമായി ചെയ്തു\", \"ഞാൻ നിന്റെ സഹായം അഭിനന്ദിക്കുന്നു\", \"നീ വളരെ ശ്രദ്ധയുള്ളവനാണ്\",\n",
    "    \"ഇത് വളരെ പ്രോത്സാഹകമാണ്\", \"നീ അത്ഭുതകരമായി പ്രവർത്തിക്കുന്നു\", \"നീ വളരെ സുഹൃദയനാണ്\", \"നീ വളരെ പ്രചോദനാത്മകനാണ്\",\n",
    "    \"ഞാൻ നിന്റെ ശ്രമം അഭിനന്ദിക്കുന്നു\", \"നീ വളരെ മിതമായവനാണ്\", \"ഇത് വളരെ സഹായകരമാണ്\", \"നീ വളരെ അറിവുള്ളവനാണ്\",\n",
    "    \"നീ അത്ഭുതകരമാണ്\", \"ഞാൻ നിന്റെ പ്രവർത്തനം ആദരിക്കുന്നു\", \"നീ വളരെ മനോഹരമാണ്\", \"നീ വളരെ ധൈര്യമുള്ളവനാണ്\",\n",
    "    \"ഇത് വളരെ പ്രചോദനമാണ്\", \"നീ വളരെ മനസ്സിലാക്കുന്നവനാണ്\", \"നീ അത്ഭുതകരമായി ചെയ്തു\", \"ഞാൻ നിന്റെ സഹനം ആദരിക്കുന്നു\",\n",
    "    \"നീ വളരെ സഹായി ആണ്\", \"ഇത് വളരെ മനോഹരമാണ്\", \"നീ അത്ഭുതകരമാണ്\", \"നീ വളരെ ധൈര്യമുള്ളവനാണ്\",\n",
    "    \"ഞാൻ നിന്റെ പ്രവർത്തനം ആദരിക്കുന്നു\", \"നീ വളരെ ശ്രദ്ധയുള്ളവനാണ്\", \"ഇത് വളരെ പ്രോത്സാഹകമാണ്\", \"നീ അത്ഭുതകരമായി പ്രവർത്തിക്കുന്നു\",\n",
    "    \"നീ വളരെ പ്രോത്സാഹനമാണ്\", \"ഞാൻ നിന്റെ അറിവ് ആദരിക്കുന്നു\", \"നീ വളരെ കരുണയുള്ളവനാണ്\", \"ഇത് വളരെ പ്രചോദനാത്മകമാണ്\",\n",
    "    \"നീ വളരെ സൃഷ്ടിപരനാണ്\", \"നീ അത്ഭുതകരമായി ചെയ്തു\", \"ഞാൻ നിന്റെ സഹായം അഭിനന്ദിക്കുന്നു\", \"നീ വളരെ ശ്രദ്ധയുള്ളവനാണ്\",\n",
    "    \"ഇത് വളരെ ഗുണകരമാണ്\", \"നീ അത്ഭുതകരമാണ്\", \"നീ വളരെ നിഷ്പക്ഷനാണ്\", \"ഞാൻ നിന്റെ പ്രവർത്തനം ആദരിക്കുന്നു\",\n",
    "    \"നീ വളരെ പ്രൊഫഷണലാണ്\", \"നീ അത്ഭുതകരമായി ചെയ്തു\", \"ഇത് വളരെ സഹായകരമാണ്\", \"നീ വളരെ അറിവുള്ളവനാണ്\",\n",
    "    \"നീ അത്ഭുതകരമാണ്\", \"ഞാൻ നിന്റെ ശ്രമം അഭിനന്ദിക്കുന്നു\", \"നീ വളരെ സഹായി ആണ്\", \"ഇത് വളരെ മനോഹരമാണ്\"\n",
    "]\n",
    "\n",
    "# 5️⃣ Convert to DataFrame\n",
    "new_rows_malayalam = pd.DataFrame({\n",
    "    \"comment\": malayalam_non_toxic_comments,\n",
    "    \"label_binary\": [0]*len(malayalam_non_toxic_comments),\n",
    "    \"language\": [\"malayalam\"]*len(malayalam_non_toxic_comments)\n",
    "})\n",
    "\n",
    "# 6️⃣ Append to existing dataset\n",
    "data = pd.concat([data, new_rows_malayalam], ignore_index=True)\n",
    "\n",
    "# 7️⃣ Save back to CSV\n",
    "data.to_csv(csv_file, index=False)\n",
    "\n",
    "print(f\"{len(malayalam_non_toxic_comments)} Malayalam non-toxic comments added successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c212ca4-a6fd-42e9-aeb0-695b6b6fd777",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# 1️⃣ Define folder and CSV path\n",
    "folder = r\"C:\\Users\\anusr\\ToxicCommentFilter\\notebooks\\data\"\n",
    "csv_file = os.path.join(folder, \"combined_dataset_clean.csv\")\n",
    "\n",
    "# 2️⃣ Create folder if not exists\n",
    "os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "# 3️⃣ Load existing dataset if exists, else create new\n",
    "if os.path.exists(csv_file):\n",
    "    data = pd.read_csv(csv_file)\n",
    "else:\n",
    "    data = pd.DataFrame(columns=[\"comment\", \"label_binary\", \"language\"])\n",
    "\n",
    "# 4️⃣ List of 200+ Manglish non-toxic comments\n",
    "manglish_non_toxic_comments = [\n",
    "    \"Ninte work valare nann aanu\", \"Nandi sahayathinu\", \"Ini ee idea valare ishtamayi\",\n",
    "    \"Ninte explanation valare clear aanu\", \"Awesome work cheythu\", \"Keep it up\",\n",
    "    \"Ini ee effort valare inspiring aanu\", \"Ninte guidance valare helpful aanu\",\n",
    "    \"Superb job\", \"Valare creative aanu\", \"Ini ee post nannayi manassilayi\",\n",
    "    \"I really admire your work\", \"Thanks for sharing this\", \"Ini ee step valare easy aayi\",\n",
    "    \"Ninte efforts valare appreciable aanu\", \"Keep going\", \"Ini ee solution valare simple aanu\",\n",
    "    \"Excellent work\", \"Ini ee approach valare effective aanu\", \"Valare smart approach\",\n",
    "    \"Ninte dedication valare inspiring aanu\", \"Great explanation\", \"Ini ee method valare helpful aanu\",\n",
    "    \"I like your style\", \"Ninte input valare useful aanu\", \"Ini ee idea valare practical aanu\",\n",
    "    \"Valare informative\", \"Thanks a lot\", \"Ini ee guidance valare supportive aanu\",\n",
    "    \"Ninte analysis valare precise aanu\", \"Ini ee concept valare clear aanu\", \"You are amazing\",\n",
    "    \"Ini ee solution valare effective aanu\", \"Valare useful suggestion\", \"Ini ee effort valare valuable aanu\",\n",
    "    \"Great job\", \"Ini ee step by step method valare clear aanu\", \"I really appreciate this\",\n",
    "    \"Ini ee technique valare helpful aanu\", \"Nice work\", \"Ini ee explanation valare perfect aanu\",\n",
    "    \"Awesome effort\", \"Ini ee approach valare logical aanu\", \"I admire your patience\",\n",
    "    \"Ini ee post valare informative aanu\", \"Well done\", \"Ini ee example valare useful aanu\",\n",
    "    \"Valare nice idea\", \"Ini ee clarification valare clear aanu\", \"You are very talented\",\n",
    "    \"Ini ee strategy valare effective aanu\", \"Good effort\", \"Ini ee practice valare valuable aanu\",\n",
    "    \"Thanks for your support\", \"Ini ee example valare helpful aanu\", \"Valare positive input\",\n",
    "    \"Ini ee suggestion valare practical aanu\", \"Great analysis\", \"Ini ee explanation valare clear aanu\",\n",
    "    \"I really like this\", \"Ini ee solution valare simple aanu\", \"Excellent approach\",\n",
    "    \"Ini ee guidance valare supportive aanu\", \"Nice explanation\", \"Ini ee method valare clear aanu\",\n",
    "    \"Super work\", \"Ini ee idea valare useful aanu\", \"Very impressive\", \"Ini ee step valare easy aayi\",\n",
    "    \"You did amazing work\", \"Ini ee technique valare helpful aanu\", \"Very thoughtful\",\n",
    "    \"Ini ee concept valare clear aanu\", \"You are awesome\", \"Ini ee example valare effective aanu\",\n",
    "    \"Great suggestion\", \"Ini ee strategy valare logical aanu\", \"Thanks a lot\", \"Ini ee solution valare perfect aanu\",\n",
    "    \"Valare good idea\", \"Ini ee approach valare clear aanu\", \"Really helpful\", \"Ini ee post valare informative aanu\",\n",
    "    \"You are very creative\", \"Ini ee input valare practical aanu\", \"Nice explanation\", \"Ini ee method valare helpful aanu\",\n",
    "    \"Well done\", \"Ini ee step valare easy aayi\", \"You are very kind\", \"Ini ee technique valare clear aanu\",\n",
    "    \"Awesome job\", \"Ini ee concept valare simple aanu\", \"Very impressive\", \"Ini ee example valare useful aanu\",\n",
    "    \"Great work\", \"Ini ee idea valare practical aanu\", \"Valare inspiring\", \"Ini ee suggestion valare logical aanu\",\n",
    "    \"Excellent input\", \"Ini ee strategy valare clear aanu\", \"Thanks for your efforts\", \"Ini ee solution valare effective aanu\",\n",
    "    \"Nice effort\", \"Ini ee explanation valare informative aanu\", \"You are very smart\", \"Ini ee guidance valare helpful aanu\",\n",
    "    \"Superb explanation\", \"Ini ee step valare perfect aanu\", \"Valare good work\", \"Ini ee approach valare simple aanu\",\n",
    "    \"Really amazing\", \"Ini ee concept valare practical aanu\", \"Great input\", \"Ini ee technique valare useful aanu\",\n",
    "    \"You are very talented\", \"Ini ee example valare effective aanu\", \"Excellent job\", \"Ini ee post valare clear aanu\",\n",
    "    \"Valare helpful\", \"Ini ee method valare easy aayi\", \"You did excellent work\", \"Ini ee suggestion valare simple aanu\",\n",
    "    \"Nice job\", \"Ini ee solution valare practical aanu\", \"Very creative\", \"Ini ee strategy valare useful aanu\",\n",
    "    \"Great effort\", \"Ini ee idea valare logical aanu\", \"Awesome approach\", \"Ini ee explanation valare perfect aanu\",\n",
    "    \"You are inspiring\", \"Ini ee guidance valare simple aanu\", \"Excellent method\", \"Ini ee step valare clear aanu\",\n",
    "    \"Very supportive\", \"Ini ee concept valare helpful aanu\", \"You are amazing\", \"Ini ee technique valare effective aanu\",\n",
    "    \"Nice explanation\", \"Ini ee example valare simple aanu\", \"Great work\", \"Ini ee post valare practical aanu\",\n",
    "    \"Very informative\", \"Ini ee suggestion valare useful aanu\", \"Superb job\", \"Ini ee strategy valare perfect aanu\",\n",
    "    \"You are very kind\", \"Ini ee solution valare helpful aanu\", \"Excellent explanation\", \"Ini ee method valare effective aanu\",\n",
    "    \"Nice effort\", \"Ini ee approach valare useful aanu\", \"You did amazing work\", \"Ini ee idea valare clear aanu\",\n",
    "    \"Great input\", \"Ini ee guidance valare perfect aanu\", \"Valare motivating\", \"Ini ee step valare useful aanu\",\n",
    "    \"Very smart\", \"Ini ee concept valare simple aanu\", \"You are very talented\", \"Ini ee technique valare practical aanu\",\n",
    "    \"Awesome work\", \"Ini ee example valare perfect aanu\", \"Excellent guidance\", \"Ini ee solution valare simple aanu\",\n",
    "    \"Nice job\", \"Ini ee strategy valare effective aanu\", \"Very impressive\", \"Ini ee post valare logical aanu\",\n",
    "    \"You are very helpful\", \"Ini ee method valare practical aanu\", \"Super work\", \"Ini ee approach valare clear aanu\",\n",
    "    \"Great explanation\", \"Ini ee idea valare useful aanu\", \"You are amazing\", \"Ini ee guidance valare effective aanu\",\n",
    "    \"Excellent effort\", \"Ini ee step valare perfect aanu\", \"Nice explanation\", \"Ini ee concept valare simple aanu\",\n",
    "    \"Valare motivating\", \"Ini ee technique valare helpful aanu\", \"You did fantastic work\", \"Ini ee example valare practical aanu\",\n",
    "    \"Very inspiring\", \"Ini ee solution valare effective aanu\", \"Awesome guidance\", \"Ini ee strategy valare useful aanu\",\n",
    "    \"You are very talented\", \"Ini ee post valare clear aanu\", \"Great job\", \"Ini ee method valare simple aanu\"\n",
    "]\n",
    "\n",
    "# 5️⃣ Convert to DataFrame\n",
    "new_rows_manglish = pd.DataFrame({\n",
    "    \"comment\": manglish_non_toxic_comments,\n",
    "    \"label_binary\": [0]*len(manglish_non_toxic_comments),\n",
    "    \"language\": [\"manglish\"]*len(manglish_non_toxic_comments)\n",
    "})\n",
    "\n",
    "# 6️⃣ Append to existing dataset\n",
    "data = pd.concat([data, new_rows_manglish], ignore_index=True)\n",
    "\n",
    "# 7️⃣ Save back to CSV\n",
    "data.to_csv(csv_file, index=False)\n",
    "\n",
    "print(f\"{len(manglish_non_toxic_comments)} Manglish non-toxic comments added successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac076f2-4144-4248-8767-f98d18bc70c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_comment = \"You are amazing\"\n",
    "\n",
    "result = predict_toxicity(new_comment)\n",
    "print(f'Comment: \"{new_comment}\" → Prediction: {result}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bab97c-5fe9-484e-8ebb-0c1513ff78cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Load full dataset\n",
    "data = pd.read_csv(r\"C:\\Users\\anusr\\ToxicCommentFilter\\notebooks\\data\\combined_dataset_clean.csv\")\n",
    "\n",
    "# Features & labels\n",
    "X = data['comment']\n",
    "y = data['label_binary']\n",
    "\n",
    "# TF-IDF vectorizer\n",
    "tfidf = TfidfVectorizer(max_features=5000)\n",
    "X_vec = tfidf.fit_transform(X)\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_vec, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Balanced Logistic Regression\n",
    "model = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Save vectorizer and model\n",
    "import joblib\n",
    "joblib.dump(tfidf, r\"C:\\Users\\anusr\\ToxicCommentFilter\\models\\tfidf_vectorizer.pkl\")\n",
    "joblib.dump(model, r\"C:\\Users\\anusr\\ToxicCommentFilter\\models\\toxic_comment_model.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f849de25-dcf2-46f1-815d-2e9a0de9ad2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(r\"C:\\Users\\anusr\\ToxicCommentFilter\\notebooks\\data\\combined_dataset_clean.csv\")\n",
    "print(data['label_binary'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e62bf16-519f-4541-af76-79467fc0acf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_comment = \"You are amazing\"\n",
    "\n",
    "result = predict_toxicity(new_comment)\n",
    "print(f'Comment: \"{new_comment}\" → Prediction: {result}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f39ed1-bd9c-410e-b304-cb3c159f4d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "csv_file = r\"C:\\Users\\anusr\\ToxicCommentFilter\\notebooks\\data\\combined_dataset_clean.csv\"\n",
    "data = pd.read_csv(csv_file)\n",
    "\n",
    "X = data['comment']\n",
    "y = data['label_binary']\n",
    "\n",
    "# TF-IDF\n",
    "tfidf = TfidfVectorizer(max_features=5000)\n",
    "X_vec = tfidf.fit_transform(X)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_vec, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Train Logistic Regression with class balance\n",
    "model = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
    "model.fit(X_train, y_train)  # ✅ This step is crucial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d5ac9a-82fd-4a3c-9d31-04a813d5b10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "csv_file = r\"C:\\Users\\anusr\\ToxicCommentFilter\\notebooks\\data\\combined_dataset_clean.csv\"\n",
    "data = pd.read_csv(csv_file)\n",
    "\n",
    "print(\"Class distribution:\\n\", data['label_binary'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa913646-93a2-400e-8321-2d93700b4d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "csv_file = r\"C:\\Users\\anusr\\ToxicCommentFilter\\notebooks\\data\\combined_dataset_clean.csv\"\n",
    "data = pd.read_csv(csv_file)\n",
    "\n",
    "# Check how many examples of each class\n",
    "print(data['label_binary'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c8010b-b1e1-47a6-bba5-12098394f378",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(csv_file, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279d0b13-9e0a-410e-a267-7646db5b886a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "csv_file = r\"C:\\Users\\anusr\\ToxicCommentFilter\\notebooks\\data\\combined_dataset_clean.csv\"\n",
    "data = pd.read_csv(csv_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef61938-4460-4dad-8ae2-0716fa344512",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Class distribution before change:\")\n",
    "print(data['label_binary'].value_counts())\n",
    "print(data['label_binary'].dtype)\n",
    "print(data['label_binary'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeeeda60-d3a4-4ad4-a959-3ed0111c0cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# 1️⃣ Define folder and CSV path\n",
    "folder = r\"C:\\Users\\anusr\\ToxicCommentFilter\\data\"\n",
    "csv_file = os.path.join(folder, \"combined_dataset_clean.csv\")\n",
    "\n",
    "# 2️⃣ Create folder if not exists\n",
    "os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "# 3️⃣ Load existing dataset if exists, else create new\n",
    "if os.path.exists(csv_file):\n",
    "    data = pd.read_csv(csv_file)\n",
    "else:\n",
    "    data = pd.DataFrame(columns=[\"comment\", \"label_binary\", \"language\"])\n",
    "\n",
    "# 4️⃣ List of 100+ non-toxic comments\n",
    "non_toxic_comments = [\n",
    "    \"You are amazing\",\n",
    "    \"I appreciate your help\",\n",
    "    \"Thank you so much\",\n",
    "    \"This is really helpful\",\n",
    "    \"I love this\",\n",
    "    \"Well done\",\n",
    "    \"Good job\",\n",
    "    \"Keep up the great work\",\n",
    "    \"You made my day\",\n",
    "    \"That was very kind of you\",\n",
    "    \"I really like this\",\n",
    "    \"Fantastic work\",\n",
    "    \"You are very talented\",\n",
    "    \"This is excellent\",\n",
    "    \"I am impressed\",\n",
    "    \"Such a nice effort\",\n",
    "    \"You are awesome\",\n",
    "    \"Great explanation\",\n",
    "    \"You are so creative\",\n",
    "    \"I am grateful for your support\",\n",
    "    \"This is very informative\",\n",
    "    \"You are a genius\",\n",
    "    \"I love your style\",\n",
    "    \"Keep going\",\n",
    "    \"So inspiring\",\n",
    "    \"I admire your dedication\",\n",
    "    \"You did a wonderful job\",\n",
    "    \"This is top quality\",\n",
    "    \"Thank you for sharing\",\n",
    "    \"You are very helpful\",\n",
    "    \"I appreciate your kindness\",\n",
    "    \"You are brilliant\",\n",
    "    \"I really admire this\",\n",
    "    \"This made me smile\",\n",
    "    \"You are very smart\",\n",
    "    \"You did great\",\n",
    "    \"Much appreciated\",\n",
    "    \"You are so thoughtful\",\n",
    "    \"You are very generous\",\n",
    "    \"I like this idea\",\n",
    "    \"This is very useful\",\n",
    "    \"Well explained\",\n",
    "    \"You are so supportive\",\n",
    "    \"I am thankful for you\",\n",
    "    \"You are very clever\",\n",
    "    \"Nice effort\",\n",
    "    \"I really appreciate this\",\n",
    "    \"This is wonderful\",\n",
    "    \"Excellent work\",\n",
    "    \"You are very encouraging\",\n",
    "    \"I like your approach\",\n",
    "    \"You are outstanding\",\n",
    "    \"So thoughtful\",\n",
    "    \"Very impressive\",\n",
    "    \"I respect your effort\",\n",
    "    \"You are so friendly\",\n",
    "    \"You did amazing work\",\n",
    "    \"This is fantastic\",\n",
    "    \"You are very polite\",\n",
    "    \"Great effort\",\n",
    "    \"This is remarkable\",\n",
    "    \"I admire your talent\",\n",
    "    \"You are very reliable\",\n",
    "    \"This is very encouraging\",\n",
    "    \"You are so inspiring\",\n",
    "    \"Thanks for your guidance\",\n",
    "    \"You are very kind\",\n",
    "    \"This is very professional\",\n",
    "    \"You are extremely helpful\",\n",
    "    \"I like how you did this\",\n",
    "    \"You are so intelligent\",\n",
    "    \"This is very well done\",\n",
    "    \"You are very responsible\",\n",
    "    \"You are very organized\",\n",
    "    \"This is extremely valuable\",\n",
    "    \"You are a true friend\",\n",
    "    \"This is amazing work\",\n",
    "    \"You are very patient\",\n",
    "    \"I appreciate your honesty\",\n",
    "    \"You are so understanding\",\n",
    "    \"You are excellent at this\",\n",
    "    \"This is very motivating\",\n",
    "    \"You are a role model\",\n",
    "    \"You are extremely talented\",\n",
    "    \"I appreciate your effort\",\n",
    "    \"You are very encouraging\",\n",
    "    \"This is highly impressive\",\n",
    "    \"You are very trustworthy\",\n",
    "    \"You are very creative\",\n",
    "    \"This is very supportive\",\n",
    "    \"You are very knowledgeable\",\n",
    "    \"I am thankful for your guidance\",\n",
    "    \"You are very polite\",\n",
    "    \"You are amazing at this\",\n",
    "    \"This is very positive\",\n",
    "    \"You are highly appreciated\",\n",
    "    \"I admire your work\",\n",
    "    \"You are so helpful\",\n",
    "    \"This is outstanding\",\n",
    "    \"You are very dependable\",\n",
    "    \"You did an excellent job\",\n",
    "    \"I am grateful for this\",\n",
    "    \"You are very cooperative\",\n",
    "    \"You are truly kind\",\n",
    "    \"This is extremely helpful\",\n",
    "    \"You are very friendly\",\n",
    "    \"You are very diligent\",\n",
    "    \"This is highly valuable\",\n",
    "    \"You are very professional\",\n",
    "    \"I appreciate your support\",\n",
    "    \"You are extremely impressive\",\n",
    "    \"You are very resourceful\",\n",
    "    \"This is so motivating\",\n",
    "    \"You are wonderful\",\n",
    "    \"You are very attentive\",\n",
    "    \"This is very encouraging\"\n",
    "]\n",
    "\n",
    "# 5️⃣ Convert list to DataFrame\n",
    "new_rows = pd.DataFrame({\n",
    "    \"comment\": non_toxic_comments,\n",
    "    \"label_binary\": [0]*len(non_toxic_comments),\n",
    "    \"language\": [\"english\"]*len(non_toxic_comments)\n",
    "})\n",
    "\n",
    "# 6️⃣ Append to existing data\n",
    "data = pd.concat([data, new_rows], ignore_index=True)\n",
    "\n",
    "# 7️⃣ Save back to CSV\n",
    "data.to_csv(csv_file, index=False)\n",
    "\n",
    "print(f\"{len(non_toxic_comments)} non-toxic comments added successfully to {csv_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe03000-cd4f-4c6f-96d2-a3eb36468a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "more_non_toxic_comments = [\n",
    "    \"You are very encouraging\", \n",
    "    \"You are so positive\", \n",
    "    \"I like your dedication\", \n",
    "    \"This is very creative\", \n",
    "    \"You are very thoughtful\", \n",
    "    \"I really admire this work\", \n",
    "    \"You are very understanding\", \n",
    "    \"You are very patient\", \n",
    "    \"This is really inspiring\", \n",
    "    \"You are extremely kind\", \n",
    "    \"You are a true inspiration\", \n",
    "    \"I appreciate your guidance\", \n",
    "    \"This is very friendly\", \n",
    "    \"You are extremely helpful\", \n",
    "    \"I like your approach\", \n",
    "    \"You are very motivating\", \n",
    "    \"You did a fantastic job\", \n",
    "    \"You are highly skilled\", \n",
    "    \"This is excellent work\", \n",
    "    \"You are very reliable\", \n",
    "    \"I admire your professionalism\", \n",
    "    \"You are so generous\", \n",
    "    \"This is very polite\", \n",
    "    \"You are very creative\", \n",
    "    \"I appreciate your contribution\", \n",
    "    \"You are very respectful\", \n",
    "    \"You are very supportive\", \n",
    "    \"This is amazing effort\", \n",
    "    \"You are extremely intelligent\", \n",
    "    \"I like your style\", \n",
    "    \"You are very talented\", \n",
    "    \"You are extremely thoughtful\", \n",
    "    \"This is very well explained\", \n",
    "    \"You are amazing at this\", \n",
    "    \"You are very clever\", \n",
    "    \"I really like this idea\", \n",
    "    \"You are very trustworthy\", \n",
    "    \"This is very positive\", \n",
    "    \"You are so encouraging\", \n",
    "    \"You are very dependable\", \n",
    "    \"I like your work ethic\", \n",
    "    \"This is very kind\", \n",
    "    \"You are very attentive\", \n",
    "    \"You are extremely insightful\", \n",
    "    \"I really like your explanation\", \n",
    "    \"You are very friendly\", \n",
    "    \"You are very diligent\", \n",
    "    \"This is highly impressive\", \n",
    "    \"You are very motivating\", \n",
    "    \"You did an outstanding job\", \n",
    "    \"You are very organized\", \n",
    "    \"This is very helpful\", \n",
    "    \"You are highly creative\", \n",
    "    \"I like your perspective\", \n",
    "    \"You are very responsible\", \n",
    "    \"You are very encouraging\", \n",
    "    \"This is very inspiring\", \n",
    "    \"You are very cooperative\", \n",
    "    \"You are highly appreciated\", \n",
    "    \"I really admire your dedication\", \n",
    "    \"You are very resourceful\", \n",
    "    \"This is excellent\", \n",
    "    \"You are extremely friendly\", \n",
    "    \"You are very generous\", \n",
    "    \"I like your efforts\", \n",
    "    \"You are very professional\", \n",
    "    \"This is very motivating\", \n",
    "    \"You are extremely thoughtful\", \n",
    "    \"You are very kind\", \n",
    "    \"You are very approachable\", \n",
    "    \"I really appreciate your work\", \n",
    "    \"You are very talented\", \n",
    "    \"You are very attentive\", \n",
    "    \"This is extremely useful\", \n",
    "    \"You are highly skilled\", \n",
    "    \"You are very creative\", \n",
    "    \"I admire your insights\", \n",
    "    \"You are very supportive\", \n",
    "    \"This is really valuable\", \n",
    "    \"You are very encouraging\", \n",
    "    \"You are very intelligent\", \n",
    "    \"I really like your approach\", \n",
    "    \"You are extremely reliable\", \n",
    "    \"You are very thoughtful\", \n",
    "    \"This is amazing work\", \n",
    "    \"You are extremely kind\", \n",
    "    \"You are very inspiring\", \n",
    "    \"I appreciate your dedication\", \n",
    "    \"You are very cooperative\", \n",
    "    \"You are very polite\", \n",
    "    \"This is very well done\", \n",
    "    \"You are extremely helpful\", \n",
    "    \"You are very creative\", \n",
    "    \"I like your guidance\", \n",
    "    \"You are very encouraging\", \n",
    "    \"You are very skilled\", \n",
    "    \"This is very professional\", \n",
    "    \"You are extremely positive\", \n",
    "    \"You are very motivating\", \n",
    "    \"I really appreciate this effort\", \n",
    "    \"You are very trustworthy\", \n",
    "    \"You are very talented\", \n",
    "    \"This is very insightful\", \n",
    "    \"You are extremely dependable\", \n",
    "    \"You are very friendly\", \n",
    "    \"I admire your work ethic\", \n",
    "    \"You are very diligent\", \n",
    "    \"This is very encouraging\", \n",
    "    \"You are extremely helpful\", \n",
    "    \"You are very generous\", \n",
    "    \"I really admire this effort\", \n",
    "    \"You are very attentive\", \n",
    "    \"You are highly professional\", \n",
    "    \"This is very thoughtful\", \n",
    "    \"You are very intelligent\", \n",
    "    \"You are extremely supportive\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc796065-4eab-4cf4-9421-bf9592a658d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame\n",
    "new_rows = pd.DataFrame({\n",
    "    \"comment\": more_non_toxic_comments,\n",
    "    \"label_binary\": [0]*len(more_non_toxic_comments),\n",
    "    \"language\": [\"english\"]*len(more_non_toxic_comments)\n",
    "})\n",
    "\n",
    "# Append to existing dataset\n",
    "data = pd.concat([data, new_rows], ignore_index=True)\n",
    "\n",
    "# Save back to CSV\n",
    "data.to_csv(csv_file, index=False)\n",
    "\n",
    "print(f\"{len(more_non_toxic_comments)} more non-toxic comments added successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0b2269-2d6e-4f0a-a0ac-c174e5991796",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# 1️⃣ Define folder and CSV path\n",
    "folder = r\"C:\\Users\\anusr\\ToxicCommentFilter\\data\"\n",
    "csv_file = os.path.join(folder, \"combined_dataset_clean.csv\")\n",
    "\n",
    "# 2️⃣ Create folder if not exists\n",
    "os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "# 3️⃣ Load existing dataset if exists, else create new\n",
    "if os.path.exists(csv_file):\n",
    "    data = pd.read_csv(csv_file)\n",
    "else:\n",
    "    data = pd.DataFrame(columns=[\"comment\", \"label_binary\", \"language\"])\n",
    "\n",
    "# 4️⃣ List of 200+ Malayalam non-toxic comments\n",
    "malayalam_non_toxic_comments = [\n",
    "    \"നിനക്ക് വളരെ നല്ലതാണ്\", \"നന്ദി സഹായത്തിന്\", \"നിനക്ക് നന്ദി\", \"ഇത് വളരെ ഉപകാരപ്രദമാണ്\",\n",
    "    \"ഞാൻ ഇത് ഇഷ്ടപ്പെടുന്നു\", \"ശ്രേഷ്ഠം ചെയ്തിരിക്കുന്നു\", \"നല്ല ജോബ്\", \"നിന്റെ മികച്ച പ്രവർത്തനം തുടരൂ\",\n",
    "    \"നിനക്ക് എന്റെ ദിനം സന്തോഷകരമാക്കി\", \"നീ വളരെ ദയവുള്ളവനാണ്\", \"ഇത് എനിക്ക് വളരെ ഇഷ്ടപ്പെട്ടു\",\n",
    "    \"ശ്രേഷ്ഠ പ്രവർത്തനം\", \"നീ വളരെ കഴിവുള്ളവനാണ്\", \"ഇത് വളരെ മികച്ചതാണ്\", \"ഞാൻ प्रभावितനാണ്\",\n",
    "    \"അതി മനോഹരമായ ശ്രമം\", \"നീ അസാധാരണമാണ്\", \"നല്ല وضിസങ്കലനം\", \"നീ വളരെ സൃഷ്ടിപരനാണ്\",\n",
    "    \"ഞാൻ നിന്റെ പിന്തുണയ്ക്ക് നന്ദിയുള്ളവൻ\", \"ഇത് വളരെ വിവരപരമാണ്\", \"നീ ഒരു ജീനിയസ് ആണ്\",\n",
    "    \"ഞാൻ നിന്റെ സ്റ്റൈൽ ഇഷ്ടപ്പെടുന്നു\", \"തുടരുക\", \"ഇത് പ്രചോദനമാണ്\", \"ഞാൻ നിന്റെ സമർപ്പണം അഭിനന്ദിക്കുന്നു\",\n",
    "    \"നീ അത്ഭുതകരമായി ചെയ്തു\", \"ഇത് ടോപ്പ് ക്വാളിറ്റി ആണ്\", \"നിന്റെ പങ്കുവെച്ചതിനുള്ള നന്ദി\", \"നീ വളരെ സഹായകരനാണ്\",\n",
    "    \"ഞാൻ നിന്റെ ദയയ്ക്ക് നന്ദിയുള്ളവൻ\", \"നീ ബുദ്ധിമാൻ ആണ്\", \"ഞാൻ ഇത് virkelig ആസ്വദിക്കുന്നു\", \"ഇത് എനിക്ക് സന്തോഷം നൽകി\",\n",
    "    \"നീ വളരെ നൂതനനാണ്\", \"നല്ല ശ്രമം\", \"ഞാൻ ഇത് വളരെ അഭിനന്ദിക്കുന്നു\", \"ഇത് അത്ഭുതകരമാണ്\", \n",
    "    \"ശ്രേഷ്ഠ പ്രവർത്തനം\", \"നീ വളരെ പ്രോത്സാഹകനാണ്\", \"ഞാൻ നിന്റെ സമീപനം ഇഷ്ടപ്പെടുന്നു\", \"നീ എത്ര മികച്ചവനാണ്\",\n",
    "    \"അത്ഭുതകരം\", \"മനോഹരം\", \"ഞാൻ നിന്റെ പരിശ്രമം ആദരിക്കുന്നു\", \"നീ വളരെ സൗഹൃദപരനാണ്\",\n",
    "    \"നീ അത്ഭുതകരമായി ചെയ്തു\", \"ഇത് വളരെ പ്രചോദനാത്മകമാണ്\", \"നിന്റെ മാർഗ്ഗനിർദ്ദേശത്തിന് നന്ദി\",\n",
    "    \"നീ വളരെ ദയാളു\", \"ഇത് വളരെ പ്രൊഫഷണൽ ആണ്\", \"നീ അത്ഭുതകരമായി സഹായിച്ചു\", \"ഞാൻ നിന്റെ സമീപനം ഇഷ്ടപ്പെടുന്നു\",\n",
    "    \"നീ വളരെ ബുദ്ധിമാനാണ്\", \"ഇത് വളരെ നല്ല വിവരമാണു\", \"നീ വളരെ ഉത്തരവാദിത്വമുള്ളവനാണ്\", \"നീ വളരെ ക്രമസമയനാണ്\",\n",
    "    \"ഇത് വളരെ മൂല്യവത്തായിരിക്കുന്നു\", \"നീ ഒരു സത്യസ്നേഹം സുഹൃത്ത് ആണ്\", \"ഇത് അസാധാരണ പ്രവർത്തനം\",\n",
    "    \"നീ വളരെ ക്ഷമയുള്ളവനാണ്\", \"ഞാൻ നിന്റെ സത്യസന്ധതക്ക് നന്ദിയുള്ളവൻ\", \"നീ വളരെ മനസ്സിലാക്കുന്നവനാണ്\",\n",
    "    \"നീ വളരെ മികച്ചത്\", \"ഇത് വളരെ പ്രചോദനാത്മകമാണ്\", \"നീ ഒരു മാതൃകാ വ്യക്തിയാണ്\", \"നീ അത്ഭുതകരമായ കഴിവുള്ളവനാണ്\",\n",
    "    \"ഞാൻ നിന്റെ ശ്രമം അഭിനന്ദിക്കുന്നു\", \"നീ വളരെ പ്രോത്സാഹകനാണ്\", \"ഇത് വളരെ ശ്രദ്ധേയമാണ്\", \"നീ വളരെ വിശ്വസനീയനാണ്\",\n",
    "    \"നീ വളരെ സൃഷ്ടിപരനാണ്\", \"ഇത് വളരെ പിന്തുണയുള്ളതാണ്\", \"നീ വളരെ അറിവുള്ളവനാണ്\", \"ഞാൻ നിന്റെ മാർഗ്ഗനിർദ്ദേശത്തിന് നന്ദിയുള്ളവൻ\",\n",
    "    \"നീ വളരെ നിഷ്പക്ഷനാണ്\", \"നീ അത്ഭുതകരമായി പ്രവർത്തിക്കുന്നു\", \"ഇത് വളരെ ഗുണകരമാണ്\", \"നീ വളരെ സഹായകരനാണ്\",\n",
    "    \"നീ വളരെ സുഹൃത്തായിരിക്കുന്നു\", \"ഇത് വളരെ പ്രചോദനമാണ്\", \"നീ വളരെ ശ്രദ്ധയുള്ളവനാണ്\", \"ഇത് വളരെ പ്രോത്സാഹകമാണ്\",\n",
    "    \"നീ വളരെ സജീവനാണ്\", \"നീ അത്ഭുതകരമായി നിർവഹിക്കുന്നു\", \"ഞാൻ നിന്റെ പ്രവർത്തനം അഭിനന്ദിക്കുന്നു\", \"നീ വളരെ പ്രൊഫഷണലാണ്\",\n",
    "    \"നീ വളരെ നല്ല വ്യക്തിയാണ്\", \"ഇത് വളരെ പ്രചോദനാത്മകമാണ്\", \"നീ വളരെ ബുദ്ധിമാനാണ്\", \"നീ അത്ഭുതകരമായി സഹായിക്കുന്നു\",\n",
    "    \"ഞാൻ നിന്റെ ശ്രമം അഭിനന്ദിക്കുന്നു\", \"നീ വളരെ സൃഷ്ടിപരനാണ്\", \"ഇത് വളരെ ഉൽസാഹജനകമാണ്\", \"നീ വളരെ വല്ലഭനാണ്\",\n",
    "    \"ഇത് വളരെ സഹായകരമാണ്\", \"നീ അത്ഭുതകരമായി ചെയ്തു\", \"നീ വളരെ കരുണയുള്ളവനാണ്\", \"ഞാൻ നിന്റെ കഠിനാധ്വാനം ആദരിക്കുന്നു\",\n",
    "    \"നീ വളരെ പ്രോത്സാഹനമാണ്\", \"ഇത് വളരെ മനോഹരമാണ്\", \"നീ അത്ഭുതകരമായി ചെയ്തു\", \"ഞാൻ നിന്റെ അറിവ് ആദരിക്കുന്നു\",\n",
    "    \"നീ വളരെ സഹായി ആണ്\", \"നീ വളരെ ശ്രദ്ധയുള്ളവനാണ്\", \"ഇത് വളരെ ഗുണകരമാണ്\", \"നീ അത്ഭുതകരമാണ്\", \n",
    "    \"നീ വളരെ പ്രതിബദ്ധനാണ്\", \"ഞാൻ നിന്റെ പ്രവർത്തനം അഭിനന്ദിക്കുന്നു\", \"നീ വളരെ നിഷ്പക്ഷനാണ്\", \"ഇത് വളരെ മനോഹരമാണ്\",\n",
    "    \"നീ വളരെ പ്രൊഫഷണലാണ്\", \"നീ അത്ഭുതകരമായി ചെയ്തു\", \"ഞാൻ നിന്റെ സഹായം അഭിനന്ദിക്കുന്നു\", \"നീ വളരെ ശ്രദ്ധയുള്ളവനാണ്\",\n",
    "    \"ഇത് വളരെ പ്രോത്സാഹകമാണ്\", \"നീ അത്ഭുതകരമായി പ്രവർത്തിക്കുന്നു\", \"നീ വളരെ സുഹൃദയനാണ്\", \"നീ വളരെ പ്രചോദനാത്മകനാണ്\",\n",
    "    \"ഞാൻ നിന്റെ ശ്രമം അഭിനന്ദിക്കുന്നു\", \"നീ വളരെ മിതമായവനാണ്\", \"ഇത് വളരെ സഹായകരമാണ്\", \"നീ വളരെ അറിവുള്ളവനാണ്\",\n",
    "    \"നീ അത്ഭുതകരമാണ്\", \"ഞാൻ നിന്റെ പ്രവർത്തനം ആദരിക്കുന്നു\", \"നീ വളരെ മനോഹരമാണ്\", \"നീ വളരെ ധൈര്യമുള്ളവനാണ്\",\n",
    "    \"ഇത് വളരെ പ്രചോദനമാണ്\", \"നീ വളരെ മനസ്സിലാക്കുന്നവനാണ്\", \"നീ അത്ഭുതകരമായി ചെയ്തു\", \"ഞാൻ നിന്റെ സഹനം ആദരിക്കുന്നു\",\n",
    "    \"നീ വളരെ സഹായി ആണ്\", \"ഇത് വളരെ മനോഹരമാണ്\", \"നീ അത്ഭുതകരമാണ്\", \"നീ വളരെ ധൈര്യമുള്ളവനാണ്\",\n",
    "    \"ഞാൻ നിന്റെ പ്രവർത്തനം ആദരിക്കുന്നു\", \"നീ വളരെ ശ്രദ്ധയുള്ളവനാണ്\", \"ഇത് വളരെ പ്രോത്സാഹകമാണ്\", \"നീ അത്ഭുതകരമായി പ്രവർത്തിക്കുന്നു\",\n",
    "    \"നീ വളരെ പ്രോത്സാഹനമാണ്\", \"ഞാൻ നിന്റെ അറിവ് ആദരിക്കുന്നു\", \"നീ വളരെ കരുണയുള്ളവനാണ്\", \"ഇത് വളരെ പ്രചോദനാത്മകമാണ്\",\n",
    "    \"നീ വളരെ സൃഷ്ടിപരനാണ്\", \"നീ അത്ഭുതകരമായി ചെയ്തു\", \"ഞാൻ നിന്റെ സഹായം അഭിനന്ദിക്കുന്നു\", \"നീ വളരെ ശ്രദ്ധയുള്ളവനാണ്\",\n",
    "    \"ഇത് വളരെ ഗുണകരമാണ്\", \"നീ അത്ഭുതകരമാണ്\", \"നീ വളരെ നിഷ്പക്ഷനാണ്\", \"ഞാൻ നിന്റെ പ്രവർത്തനം ആദരിക്കുന്നു\",\n",
    "    \"നീ വളരെ പ്രൊഫഷണലാണ്\", \"നീ അത്ഭുതകരമായി ചെയ്തു\", \"ഇത് വളരെ സഹായകരമാണ്\", \"നീ വളരെ അറിവുള്ളവനാണ്\",\n",
    "    \"നീ അത്ഭുതകരമാണ്\", \"ഞാൻ നിന്റെ ശ്രമം അഭിനന്ദിക്കുന്നു\", \"നീ വളരെ സഹായി ആണ്\", \"ഇത് വളരെ മനോഹരമാണ്\"\n",
    "]\n",
    "\n",
    "# 5️⃣ Convert to DataFrame\n",
    "new_rows_malayalam = pd.DataFrame({\n",
    "    \"comment\": malayalam_non_toxic_comments,\n",
    "    \"label_binary\": [0]*len(malayalam_non_toxic_comments),\n",
    "    \"language\": [\"malayalam\"]*len(malayalam_non_toxic_comments)\n",
    "})\n",
    "\n",
    "# 6️⃣ Append to existing dataset\n",
    "data = pd.concat([data, new_rows_malayalam], ignore_index=True)\n",
    "\n",
    "# 7️⃣ Save back to CSV\n",
    "data.to_csv(csv_file, index=False)\n",
    "\n",
    "print(f\"{len(malayalam_non_toxic_comments)} Malayalam non-toxic comments added successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fe6870-3a77-45b2-827d-ec5f8b26c563",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# 1️⃣ Define folder and CSV path\n",
    "folder = r\"C:\\Users\\anusr\\ToxicCommentFilter\\data\"\n",
    "csv_file = os.path.join(folder, \"combined_dataset_clean.csv\")\n",
    "\n",
    "# 2️⃣ Create folder if not exists\n",
    "os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "# 3️⃣ Load existing dataset if exists, else create new\n",
    "if os.path.exists(csv_file):\n",
    "    data = pd.read_csv(csv_file)\n",
    "else:\n",
    "    data = pd.DataFrame(columns=[\"comment\", \"label_binary\", \"language\"])\n",
    "\n",
    "# 4️⃣ List of 200+ Malayalam non-toxic comments\n",
    "malayalam_non_toxic_comments = [\n",
    "    \"നിനക്ക് വളരെ നല്ലതാണ്\", \"നന്ദി സഹായത്തിന്\", \"നിനക്ക് നന്ദി\", \"ഇത് വളരെ ഉപകാരപ്രദമാണ്\",\n",
    "    \"ഞാൻ ഇത് ഇഷ്ടപ്പെടുന്നു\", \"ശ്രേഷ്ഠം ചെയ്തിരിക്കുന്നു\", \"നല്ല ജോബ്\", \"നിന്റെ മികച്ച പ്രവർത്തനം തുടരൂ\",\n",
    "    \"നിനക്ക് എന്റെ ദിനം സന്തോഷകരമാക്കി\", \"നീ വളരെ ദയവുള്ളവനാണ്\", \"ഇത് എനിക്ക് വളരെ ഇഷ്ടപ്പെട്ടു\",\n",
    "    \"ശ്രേഷ്ഠ പ്രവർത്തനം\", \"നീ വളരെ കഴിവുള്ളവനാണ്\", \"ഇത് വളരെ മികച്ചതാണ്\", \"ഞാൻ प्रभावितനാണ്\",\n",
    "    \"അതി മനോഹരമായ ശ്രമം\", \"നീ അസാധാരണമാണ്\", \"നല്ല وضിസങ്കലനം\", \"നീ വളരെ സൃഷ്ടിപരനാണ്\",\n",
    "    \"ഞാൻ നിന്റെ പിന്തുണയ്ക്ക് നന്ദിയുള്ളവൻ\", \"ഇത് വളരെ വിവരപരമാണ്\", \"നീ ഒരു ജീനിയസ് ആണ്\",\n",
    "    \"ഞാൻ നിന്റെ സ്റ്റൈൽ ഇഷ്ടപ്പെടുന്നു\", \"തുടരുക\", \"ഇത് പ്രചോദനമാണ്\", \"ഞാൻ നിന്റെ സമർപ്പണം അഭിനന്ദിക്കുന്നു\",\n",
    "    \"നീ അത്ഭുതകരമായി ചെയ്തു\", \"ഇത് ടോപ്പ് ക്വാളിറ്റി ആണ്\", \"നിന്റെ പങ്കുവെച്ചതിനുള്ള നന്ദി\", \"നീ വളരെ സഹായകരനാണ്\",\n",
    "    \"ഞാൻ നിന്റെ ദയയ്ക്ക് നന്ദിയുള്ളവൻ\", \"നീ ബുദ്ധിമാൻ ആണ്\", \"ഞാൻ ഇത് virkelig ആസ്വദിക്കുന്നു\", \"ഇത് എനിക്ക് സന്തോഷം നൽകി\",\n",
    "    \"നീ വളരെ നൂതനനാണ്\", \"നല്ല ശ്രമം\", \"ഞാൻ ഇത് വളരെ അഭിനന്ദിക്കുന്നു\", \"ഇത് അത്ഭുതകരമാണ്\", \n",
    "    \"ശ്രേഷ്ഠ പ്രവർത്തനം\", \"നീ വളരെ പ്രോത്സാഹകനാണ്\", \"ഞാൻ നിന്റെ സമീപനം ഇഷ്ടപ്പെടുന്നു\", \"നീ എത്ര മികച്ചവനാണ്\",\n",
    "    \"അത്ഭുതകരം\", \"മനോഹരം\", \"ഞാൻ നിന്റെ പരിശ്രമം ആദരിക്കുന്നു\", \"നീ വളരെ സൗഹൃദപരനാണ്\",\n",
    "    \"നീ അത്ഭുതകരമായി ചെയ്തു\", \"ഇത് വളരെ പ്രചോദനാത്മകമാണ്\", \"നിന്റെ മാർഗ്ഗനിർദ്ദേശത്തിന് നന്ദി\",\n",
    "    \"നീ വളരെ ദയാളു\", \"ഇത് വളരെ പ്രൊഫഷണൽ ആണ്\", \"നീ അത്ഭുതകരമായി സഹായിച്ചു\", \"ഞാൻ നിന്റെ സമീപനം ഇഷ്ടപ്പെടുന്നു\",\n",
    "    \"നീ വളരെ ബുദ്ധിമാനാണ്\", \"ഇത് വളരെ നല്ല വിവരമാണു\", \"നീ വളരെ ഉത്തരവാദിത്വമുള്ളവനാണ്\", \"നീ വളരെ ക്രമസമയനാണ്\",\n",
    "    \"ഇത് വളരെ മൂല്യവത്തായിരിക്കുന്നു\", \"നീ ഒരു സത്യസ്നേഹം സുഹൃത്ത് ആണ്\", \"ഇത് അസാധാരണ പ്രവർത്തനം\",\n",
    "    \"നീ വളരെ ക്ഷമയുള്ളവനാണ്\", \"ഞാൻ നിന്റെ സത്യസന്ധതക്ക് നന്ദിയുള്ളവൻ\", \"നീ വളരെ മനസ്സിലാക്കുന്നവനാണ്\",\n",
    "    \"നീ വളരെ മികച്ചത്\", \"ഇത് വളരെ പ്രചോദനാത്മകമാണ്\", \"നീ ഒരു മാതൃകാ വ്യക്തിയാണ്\", \"നീ അത്ഭുതകരമായ കഴിവുള്ളവനാണ്\",\n",
    "    \"ഞാൻ നിന്റെ ശ്രമം അഭിനന്ദിക്കുന്നു\", \"നീ വളരെ പ്രോത്സാഹകനാണ്\", \"ഇത് വളരെ ശ്രദ്ധേയമാണ്\", \"നീ വളരെ വിശ്വസനീയനാണ്\",\n",
    "    \"നീ വളരെ സൃഷ്ടിപരനാണ്\", \"ഇത് വളരെ പിന്തുണയുള്ളതാണ്\", \"നീ വളരെ അറിവുള്ളവനാണ്\", \"ഞാൻ നിന്റെ മാർഗ്ഗനിർദ്ദേശത്തിന് നന്ദിയുള്ളവൻ\",\n",
    "    \"നീ വളരെ നിഷ്പക്ഷനാണ്\", \"നീ അത്ഭുതകരമായി പ്രവർത്തിക്കുന്നു\", \"ഇത് വളരെ ഗുണകരമാണ്\", \"നീ വളരെ സഹായകരനാണ്\",\n",
    "    \"നീ വളരെ സുഹൃത്തായിരിക്കുന്നു\", \"ഇത് വളരെ പ്രചോദനമാണ്\", \"നീ വളരെ ശ്രദ്ധയുള്ളവനാണ്\", \"ഇത് വളരെ പ്രോത്സാഹകമാണ്\",\n",
    "    \"നീ വളരെ സജീവനാണ്\", \"നീ അത്ഭുതകരമായി നിർവഹിക്കുന്നു\", \"ഞാൻ നിന്റെ പ്രവർത്തനം അഭിനന്ദിക്കുന്നു\", \"നീ വളരെ പ്രൊഫഷണലാണ്\",\n",
    "    \"നീ വളരെ നല്ല വ്യക്തിയാണ്\", \"ഇത് വളരെ പ്രചോദനാത്മകമാണ്\", \"നീ വളരെ ബുദ്ധിമാനാണ്\", \"നീ അത്ഭുതകരമായി സഹായിക്കുന്നു\",\n",
    "    \"ഞാൻ നിന്റെ ശ്രമം അഭിനന്ദിക്കുന്നു\", \"നീ വളരെ സൃഷ്ടിപരനാണ്\", \"ഇത് വളരെ ഉൽസാഹജനകമാണ്\", \"നീ വളരെ വല്ലഭനാണ്\",\n",
    "    \"ഇത് വളരെ സഹായകരമാണ്\", \"നീ അത്ഭുതകരമായി ചെയ്തു\", \"നീ വളരെ കരുണയുള്ളവനാണ്\", \"ഞാൻ നിന്റെ കഠിനാധ്വാനം ആദരിക്കുന്നു\",\n",
    "    \"നീ വളരെ പ്രോത്സാഹനമാണ്\", \"ഇത് വളരെ മനോഹരമാണ്\", \"നീ അത്ഭുതകരമായി ചെയ്തു\", \"ഞാൻ നിന്റെ അറിവ് ആദരിക്കുന്നു\",\n",
    "    \"നീ വളരെ സഹായി ആണ്\", \"നീ വളരെ ശ്രദ്ധയുള്ളവനാണ്\", \"ഇത് വളരെ ഗുണകരമാണ്\", \"നീ അത്ഭുതകരമാണ്\", \n",
    "    \"നീ വളരെ പ്രതിബദ്ധനാണ്\", \"ഞാൻ നിന്റെ പ്രവർത്തനം അഭിനന്ദിക്കുന്നു\", \"നീ വളരെ നിഷ്പക്ഷനാണ്\", \"ഇത് വളരെ മനോഹരമാണ്\",\n",
    "    \"നീ വളരെ പ്രൊഫഷണലാണ്\", \"നീ അത്ഭുതകരമായി ചെയ്തു\", \"ഞാൻ നിന്റെ സഹായം അഭിനന്ദിക്കുന്നു\", \"നീ വളരെ ശ്രദ്ധയുള്ളവനാണ്\",\n",
    "    \"ഇത് വളരെ പ്രോത്സാഹകമാണ്\", \"നീ അത്ഭുതകരമായി പ്രവർത്തിക്കുന്നു\", \"നീ വളരെ സുഹൃദയനാണ്\", \"നീ വളരെ പ്രചോദനാത്മകനാണ്\",\n",
    "    \"ഞാൻ നിന്റെ ശ്രമം അഭിനന്ദിക്കുന്നു\", \"നീ വളരെ മിതമായവനാണ്\", \"ഇത് വളരെ സഹായകരമാണ്\", \"നീ വളരെ അറിവുള്ളവനാണ്\",\n",
    "    \"നീ അത്ഭുതകരമാണ്\", \"ഞാൻ നിന്റെ പ്രവർത്തനം ആദരിക്കുന്നു\", \"നീ വളരെ മനോഹരമാണ്\", \"നീ വളരെ ധൈര്യമുള്ളവനാണ്\",\n",
    "    \"ഇത് വളരെ പ്രചോദനമാണ്\", \"നീ വളരെ മനസ്സിലാക്കുന്നവനാണ്\", \"നീ അത്ഭുതകരമായി ചെയ്തു\", \"ഞാൻ നിന്റെ സഹനം ആദരിക്കുന്നു\",\n",
    "    \"നീ വളരെ സഹായി ആണ്\", \"ഇത് വളരെ മനോഹരമാണ്\", \"നീ അത്ഭുതകരമാണ്\", \"നീ വളരെ ധൈര്യമുള്ളവനാണ്\",\n",
    "    \"ഞാൻ നിന്റെ പ്രവർത്തനം ആദരിക്കുന്നു\", \"നീ വളരെ ശ്രദ്ധയുള്ളവനാണ്\", \"ഇത് വളരെ പ്രോത്സാഹകമാണ്\", \"നീ അത്ഭുതകരമായി പ്രവർത്തിക്കുന്നു\",\n",
    "    \"നീ വളരെ പ്രോത്സാഹനമാണ്\", \"ഞാൻ നിന്റെ അറിവ് ആദരിക്കുന്നു\", \"നീ വളരെ കരുണയുള്ളവനാണ്\", \"ഇത് വളരെ പ്രചോദനാത്മകമാണ്\",\n",
    "    \"നീ വളരെ സൃഷ്ടിപരനാണ്\", \"നീ അത്ഭുതകരമായി ചെയ്തു\", \"ഞാൻ നിന്റെ സഹായം അഭിനന്ദിക്കുന്നു\", \"നീ വളരെ ശ്രദ്ധയുള്ളവനാണ്\",\n",
    "    \"ഇത് വളരെ ഗുണകരമാണ്\", \"നീ അത്ഭുതകരമാണ്\", \"നീ വളരെ നിഷ്പക്ഷനാണ്\", \"ഞാൻ നിന്റെ പ്രവർത്തനം ആദരിക്കുന്നു\",\n",
    "    \"നീ വളരെ പ്രൊഫഷണലാണ്\", \"നീ അത്ഭുതകരമായി ചെയ്തു\", \"ഇത് വളരെ സഹായകരമാണ്\", \"നീ വളരെ അറിവുള്ളവനാണ്\",\n",
    "    \"നീ അത്ഭുതകരമാണ്\", \"ഞാൻ നിന്റെ ശ്രമം അഭിനന്ദിക്കുന്നു\", \"നീ വളരെ സഹായി ആണ്\", \"ഇത് വളരെ മനോഹരമാണ്\"\n",
    "]\n",
    "\n",
    "# 5️⃣ Convert to DataFrame\n",
    "new_rows_malayalam = pd.DataFrame({\n",
    "    \"comment\": malayalam_non_toxic_comments,\n",
    "    \"label_binary\": [0]*len(malayalam_non_toxic_comments),\n",
    "    \"language\": [\"malayalam\"]*len(malayalam_non_toxic_comments)\n",
    "})\n",
    "\n",
    "# 6️⃣ Append to existing dataset\n",
    "data = pd.concat([data, new_rows_malayalam], ignore_index=True)\n",
    "\n",
    "# 7️⃣ Save back to CSV\n",
    "data.to_csv(csv_file, index=False)\n",
    "\n",
    "print(f\"{len(malayalam_non_toxic_comments)} Malayalam non-toxic comments added successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a3d73e-e802-4568-9bdf-fdb8c2cdd7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# 1️⃣ Define folder and CSV path\n",
    "folder = r\"C:\\Users\\anusr\\ToxicCommentFilter\\data\"\n",
    "csv_file = os.path.join(folder, \"combined_dataset_clean.csv\")\n",
    "\n",
    "# 2️⃣ Create folder if not exists\n",
    "os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "# 3️⃣ Load existing dataset if exists, else create new\n",
    "if os.path.exists(csv_file):\n",
    "    data = pd.read_csv(csv_file)\n",
    "else:\n",
    "    data = pd.DataFrame(columns=[\"comment\", \"label_binary\", \"language\"])\n",
    "\n",
    "# 4️⃣ List of 200+ Manglish non-toxic comments\n",
    "manglish_non_toxic_comments = [\n",
    "    \"Ninte work valare nann aanu\", \"Nandi sahayathinu\", \"Ini ee idea valare ishtamayi\",\n",
    "    \"Ninte explanation valare clear aanu\", \"Awesome work cheythu\", \"Keep it up\",\n",
    "    \"Ini ee effort valare inspiring aanu\", \"Ninte guidance valare helpful aanu\",\n",
    "    \"Superb job\", \"Valare creative aanu\", \"Ini ee post nannayi manassilayi\",\n",
    "    \"I really admire your work\", \"Thanks for sharing this\", \"Ini ee step valare easy aayi\",\n",
    "    \"Ninte efforts valare appreciable aanu\", \"Keep going\", \"Ini ee solution valare simple aanu\",\n",
    "    \"Excellent work\", \"Ini ee approach valare effective aanu\", \"Valare smart approach\",\n",
    "    \"Ninte dedication valare inspiring aanu\", \"Great explanation\", \"Ini ee method valare helpful aanu\",\n",
    "    \"I like your style\", \"Ninte input valare useful aanu\", \"Ini ee idea valare practical aanu\",\n",
    "    \"Valare informative\", \"Thanks a lot\", \"Ini ee guidance valare supportive aanu\",\n",
    "    \"Ninte analysis valare precise aanu\", \"Ini ee concept valare clear aanu\", \"You are amazing\",\n",
    "    \"Ini ee solution valare effective aanu\", \"Valare useful suggestion\", \"Ini ee effort valare valuable aanu\",\n",
    "    \"Great job\", \"Ini ee step by step method valare clear aanu\", \"I really appreciate this\",\n",
    "    \"Ini ee technique valare helpful aanu\", \"Nice work\", \"Ini ee explanation valare perfect aanu\",\n",
    "    \"Awesome effort\", \"Ini ee approach valare logical aanu\", \"I admire your patience\",\n",
    "    \"Ini ee post valare informative aanu\", \"Well done\", \"Ini ee example valare useful aanu\",\n",
    "    \"Valare nice idea\", \"Ini ee clarification valare clear aanu\", \"You are very talented\",\n",
    "    \"Ini ee strategy valare effective aanu\", \"Good effort\", \"Ini ee practice valare valuable aanu\",\n",
    "    \"Thanks for your support\", \"Ini ee example valare helpful aanu\", \"Valare positive input\",\n",
    "    \"Ini ee suggestion valare practical aanu\", \"Great analysis\", \"Ini ee explanation valare clear aanu\",\n",
    "    \"I really like this\", \"Ini ee solution valare simple aanu\", \"Excellent approach\",\n",
    "    \"Ini ee guidance valare supportive aanu\", \"Nice explanation\", \"Ini ee method valare clear aanu\",\n",
    "    \"Super work\", \"Ini ee idea valare useful aanu\", \"Very impressive\", \"Ini ee step valare easy aayi\",\n",
    "    \"You did amazing work\", \"Ini ee technique valare helpful aanu\", \"Very thoughtful\",\n",
    "    \"Ini ee concept valare clear aanu\", \"You are awesome\", \"Ini ee example valare effective aanu\",\n",
    "    \"Great suggestion\", \"Ini ee strategy valare logical aanu\", \"Thanks a lot\", \"Ini ee solution valare perfect aanu\",\n",
    "    \"Valare good idea\", \"Ini ee approach valare clear aanu\", \"Really helpful\", \"Ini ee post valare informative aanu\",\n",
    "    \"You are very creative\", \"Ini ee input valare practical aanu\", \"Nice explanation\", \"Ini ee method valare helpful aanu\",\n",
    "    \"Well done\", \"Ini ee step valare easy aayi\", \"You are very kind\", \"Ini ee technique valare clear aanu\",\n",
    "    \"Awesome job\", \"Ini ee concept valare simple aanu\", \"Very impressive\", \"Ini ee example valare useful aanu\",\n",
    "    \"Great work\", \"Ini ee idea valare practical aanu\", \"Valare inspiring\", \"Ini ee suggestion valare logical aanu\",\n",
    "    \"Excellent input\", \"Ini ee strategy valare clear aanu\", \"Thanks for your efforts\", \"Ini ee solution valare effective aanu\",\n",
    "    \"Nice effort\", \"Ini ee explanation valare informative aanu\", \"You are very smart\", \"Ini ee guidance valare helpful aanu\",\n",
    "    \"Superb explanation\", \"Ini ee step valare perfect aanu\", \"Valare good work\", \"Ini ee approach valare simple aanu\",\n",
    "    \"Really amazing\", \"Ini ee concept valare practical aanu\", \"Great input\", \"Ini ee technique valare useful aanu\",\n",
    "    \"You are very talented\", \"Ini ee example valare effective aanu\", \"Excellent job\", \"Ini ee post valare clear aanu\",\n",
    "    \"Valare helpful\", \"Ini ee method valare easy aayi\", \"You did excellent work\", \"Ini ee suggestion valare simple aanu\",\n",
    "    \"Nice job\", \"Ini ee solution valare practical aanu\", \"Very creative\", \"Ini ee strategy valare useful aanu\",\n",
    "    \"Great effort\", \"Ini ee idea valare logical aanu\", \"Awesome approach\", \"Ini ee explanation valare perfect aanu\",\n",
    "    \"You are inspiring\", \"Ini ee guidance valare simple aanu\", \"Excellent method\", \"Ini ee step valare clear aanu\",\n",
    "    \"Very supportive\", \"Ini ee concept valare helpful aanu\", \"You are amazing\", \"Ini ee technique valare effective aanu\",\n",
    "    \"Nice explanation\", \"Ini ee example valare simple aanu\", \"Great work\", \"Ini ee post valare practical aanu\",\n",
    "    \"Very informative\", \"Ini ee suggestion valare useful aanu\", \"Superb job\", \"Ini ee strategy valare perfect aanu\",\n",
    "    \"You are very kind\", \"Ini ee solution valare helpful aanu\", \"Excellent explanation\", \"Ini ee method valare effective aanu\",\n",
    "    \"Nice effort\", \"Ini ee approach valare useful aanu\", \"You did amazing work\", \"Ini ee idea valare clear aanu\",\n",
    "    \"Great input\", \"Ini ee guidance valare perfect aanu\", \"Valare motivating\", \"Ini ee step valare useful aanu\",\n",
    "    \"Very smart\", \"Ini ee concept valare simple aanu\", \"You are very talented\", \"Ini ee technique valare practical aanu\",\n",
    "    \"Awesome work\", \"Ini ee example valare perfect aanu\", \"Excellent guidance\", \"Ini ee solution valare simple aanu\",\n",
    "    \"Nice job\", \"Ini ee strategy valare effective aanu\", \"Very impressive\", \"Ini ee post valare logical aanu\",\n",
    "    \"You are very helpful\", \"Ini ee method valare practical aanu\", \"Super work\", \"Ini ee approach valare clear aanu\",\n",
    "    \"Great explanation\", \"Ini ee idea valare useful aanu\", \"You are amazing\", \"Ini ee guidance valare effective aanu\",\n",
    "    \"Excellent effort\", \"Ini ee step valare perfect aanu\", \"Nice explanation\", \"Ini ee concept valare simple aanu\",\n",
    "    \"Valare motivating\", \"Ini ee technique valare helpful aanu\", \"You did fantastic work\", \"Ini ee example valare practical aanu\",\n",
    "    \"Very inspiring\", \"Ini ee solution valare effective aanu\", \"Awesome guidance\", \"Ini ee strategy valare useful aanu\",\n",
    "    \"You are very talented\", \"Ini ee post valare clear aanu\", \"Great job\", \"Ini ee method valare simple aanu\"\n",
    "]\n",
    "\n",
    "# 5️⃣ Convert to DataFrame\n",
    "new_rows_manglish = pd.DataFrame({\n",
    "    \"comment\": manglish_non_toxic_comments,\n",
    "    \"label_binary\": [0]*len(manglish_non_toxic_comments),\n",
    "    \"language\": [\"manglish\"]*len(manglish_non_toxic_comments)\n",
    "})\n",
    "\n",
    "# 6️⃣ Append to existing dataset\n",
    "data = pd.concat([data, new_rows_manglish], ignore_index=True)\n",
    "\n",
    "# 7️⃣ Save back to CSV\n",
    "data.to_csv(csv_file, index=False)\n",
    "\n",
    "print(f\"{len(manglish_non_toxic_comments)} Manglish non-toxic comments added successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4780b4-47af-480b-b13f-bb13c0ddc8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "\n",
    "# Features & labels\n",
    "X = data['comment']\n",
    "y = data['label_binary']\n",
    "\n",
    "# TF-IDF\n",
    "tfidf = TfidfVectorizer(max_features=5000)\n",
    "X_vec = tfidf.fit_transform(X)\n",
    "\n",
    "# Split (stratify ensures both classes in train/test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_vec, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Train model\n",
    "model = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
    "model.fit(X_train, y_train)  # ✅ Will work now\n",
    "\n",
    "# Save model & vectorizer\n",
    "joblib.dump(tfidf, r\"C:\\Users\\anusr\\ToxicCommentFilter\\models\\tfidf_vectorizer.pkl\")\n",
    "joblib.dump(model, r\"C:\\Users\\anusr\\ToxicCommentFilter\\models\\toxic_comment_model.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c175a3-cfe4-46ca-9098-cfee70dcc31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "csv_file = r\"C:\\Users\\anusr\\ToxicCommentFilter\\notebooks\\data\\combined_dataset_clean.csv\"\n",
    "data = pd.read_csv(csv_file)\n",
    "\n",
    "# Check for missing comments\n",
    "print(\"Missing comments:\", data['comment'].isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130bdf83-3436-4f66-b169-5d171a7fce7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "csv_file = r\"C:\\Users\\anusr\\ToxicCommentFilter\\data\\combined_dataset_clean.csv\"\n",
    "data = pd.read_csv(csv_file)\n",
    "\n",
    "# Check for missing comments\n",
    "print(\"Missing comments:\", data['comment'].isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1197cf4-062c-4c2b-9283-5eda069cd134",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['comment'] = data['comment'].fillna(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314e1edb-627b-45f0-9a6f-2cc8dcceb1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['label_binary'] = data['label_binary'].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bd3eb1-7109-40fa-8609-fd5a11fc76ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "X = data['comment']\n",
    "tfidf = TfidfVectorizer(max_features=5000)\n",
    "X_vec = tfidf.fit_transform(X)  # ✅ No error now\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed813a3-d002-4fba-983f-4f352e886288",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb537096-adc0-470a-a85e-8dfd17e26222",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna(subset=['comment'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c463337-5a48-4a2a-91b4-ef86f87b7fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d1a638-7256-4fcb-a3ed-6188c2272116",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(columns=['comment.1'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd18110-363f-4fd5-9c02-bf626854f263",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "csv_file = r\"C:\\Users\\anusr\\ToxicCommentFilter\\notebooks\\data\\combined_dataset_clean.csv\"\n",
    "data = pd.read_csv(csv_file)\n",
    "\n",
    "# Drop the extra 'comment' column (the one causing NaN)\n",
    "data = data.drop(columns=['comment'])\n",
    "\n",
    "# Make sure all labels are integers\n",
    "data['label_binary'] = data['label_binary'].astype(int)\n",
    "\n",
    "# Drop any missing values in the main comment column just in case\n",
    "data = data.dropna(subset=['Comment'])\n",
    "\n",
    "# Check\n",
    "print(data.columns)\n",
    "print(data.isna().sum())\n",
    "print(data['label_binary'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbeaafce-2cc6-4fc1-b649-910b625da039",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(data.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2237bb74-1609-4cf9-870d-21752ea4f3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "csv_file = r\"C:\\Users\\anusr\\ToxicCommentFilter\\data\\combined_dataset_clean.csv\"\n",
    "data = pd.read_csv(csv_file)\n",
    "\n",
    "# Drop the extra 'comment' column (the one causing NaN)\n",
    "data = data.drop(columns=['comment'])\n",
    "\n",
    "# Make sure all labels are integers\n",
    "data['label_binary'] = data['label_binary'].astype(int)\n",
    "\n",
    "# Drop any missing values in the main comment column just in case\n",
    "data = data.dropna(subset=['Comment'])\n",
    "\n",
    "# Check\n",
    "print(data.columns)\n",
    "print(data.isna().sum())\n",
    "print(data['label_binary'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0dd05c6-9053-4925-9544-932da4dfc892",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "\n",
    "X = data['Comment']  # make sure to use capital C\n",
    "y = data['label_binary']\n",
    "\n",
    "# TF-IDF\n",
    "tfidf = TfidfVectorizer(max_features=5000)\n",
    "X_vec = tfidf.fit_transform(X)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_vec, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Logistic Regression with class balance\n",
    "model = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Save\n",
    "joblib.dump(tfidf, \"tfidf_vectorizer.pkl\")\n",
    "joblib.dump(model, \"toxic_comment_model.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03987e84-61c6-423c-848d-64df1c93f0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_toxicity(comment):\n",
    "    comment_vec = tfidf.transform([comment])\n",
    "    prediction = model.predict(comment_vec)[0]\n",
    "    return \"Toxic\" if prediction == 1 else \"Non-Toxic\"\n",
    "\n",
    "print(predict_toxicity(\"You are amazing\"))  # Hopefully Non-Toxic now\n",
    "print(predict_toxicity(\"I hate you so much\"))  # Toxic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303193ac-5310-4a3c-ba31-891ea5e53d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Path to your dataset\n",
    "csv_file = r\"C:\\Users\\anusr\\ToxicCommentFilter\\notebooks\\data\\combined_dataset_clean.csv\"\n",
    "\n",
    "# Load existing dataset\n",
    "data = pd.read_csv(csv_file)\n",
    "\n",
    "# ---------------------------\n",
    "# 1️⃣ Generate English Non-Toxic comments\n",
    "# ---------------------------\n",
    "english_templates = [\n",
    "    \"You are doing great\",\n",
    "    \"Keep up the good work\",\n",
    "    \"This is excellent\",\n",
    "    \"Well done\",\n",
    "    \"I appreciate your effort\",\n",
    "    \"This is very helpful\",\n",
    "    \"Thank you for sharing\",\n",
    "    \"You are amazing\",\n",
    "    \"I like this\",\n",
    "    \"Fantastic work\"\n",
    "]\n",
    "\n",
    "english_comments = []\n",
    "for i in range(400):  # 400 English comments\n",
    "    template = random.choice(english_templates)\n",
    "    english_comments.append(template + f\" #{i+1}\")  # add number to make them unique\n",
    "\n",
    "# ---------------------------\n",
    "# 2️⃣ Generate Malayalam Non-Toxic comments\n",
    "# ---------------------------\n",
    "malayalam_templates = [\n",
    "    \"നിങ്ങൾ അത്ഭുതകരമാണ്\",      # You are amazing\n",
    "    \"നല്ല ജോലി\",                 # Good job\n",
    "    \"ഇത് വളരെ സഹായകമാണ്\",      # This is very helpful\n",
    "    \"സഹായത്തിന് നന്ദി\",           # Thanks for help\n",
    "    \"നന്ദി\",                     # Thanks\n",
    "    \"സൂപ്പർ\",                     # Super\n",
    "    \"ഇത് വളരെ നല്ലതാണ്\",        # This is very good\n",
    "]\n",
    "\n",
    "malayalam_comments = []\n",
    "for i in range(300):  # 300 Malayalam comments\n",
    "    template = random.choice(malayalam_templates)\n",
    "    malayalam_comments.append(template + f\" #{i+1}\")\n",
    "\n",
    "# ---------------------------\n",
    "# 3️⃣ Generate Manglish Non-Toxic comments\n",
    "# ---------------------------\n",
    "manglish_templates = [\n",
    "    \"Ningal athbhuthakaramanu\",  \n",
    "    \"Nalla joli\",  \n",
    "    \"Ithu valare sahayakaman\",  \n",
    "    \"Sahayathin nandi\",  \n",
    "    \"Thangalude shrama abhinandanaarkham\",  \n",
    "    \"Super\",  \n",
    "    \"Nandi\",  \n",
    "]\n",
    "\n",
    "manglish_comments = []\n",
    "for i in range(300):  # 300 Manglish comments\n",
    "    template = random.choice(manglish_templates)\n",
    "    manglish_comments.append(template + f\" #{i+1}\")\n",
    "\n",
    "# ---------------------------\n",
    "# 4️⃣ Combine all Non-Toxic comments\n",
    "# ---------------------------\n",
    "all_comments = english_comments + malayalam_comments + manglish_comments\n",
    "languages = [\"english\"]*len(english_comments) + [\"malayalam\"]*len(malayalam_comments) + [\"manglish\"]*len(manglish_comments)\n",
    "\n",
    "nontoxic_df = pd.DataFrame({\n",
    "    \"Comment\": all_comments,\n",
    "    \"label_binary\": [0]*len(all_comments),\n",
    "    \"language\": languages\n",
    "})\n",
    "\n",
    "# ---------------------------\n",
    "# 5️⃣ Append to dataset & save\n",
    "# ---------------------------\n",
    "data = pd.concat([data, nontoxic_df], ignore_index=True)\n",
    "data.to_csv(csv_file, index=False, encoding='utf-8')\n",
    "\n",
    "# ---------------------------\n",
    "# 6️⃣ Verify\n",
    "# ---------------------------\n",
    "print(\"New dataset shape:\", data.shape)\n",
    "print(data['label_binary'].value_counts())\n",
    "print(data['language'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1a2fef-01dd-4f65-8358-1fc25138d345",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Path to your dataset\n",
    "csv_file = r\"C:\\Users\\anusr\\ToxicCommentFilter\\data\\combined_dataset_clean.csv\"\n",
    "\n",
    "# Load existing dataset\n",
    "data = pd.read_csv(csv_file)\n",
    "\n",
    "# ---------------------------\n",
    "# 1️⃣ Generate English Non-Toxic comments\n",
    "# ---------------------------\n",
    "english_templates = [\n",
    "    \"You are doing great\",\n",
    "    \"Keep up the good work\",\n",
    "    \"This is excellent\",\n",
    "    \"Well done\",\n",
    "    \"I appreciate your effort\",\n",
    "    \"This is very helpful\",\n",
    "    \"Thank you for sharing\",\n",
    "    \"You are amazing\",\n",
    "    \"I like this\",\n",
    "    \"Fantastic work\"\n",
    "]\n",
    "\n",
    "english_comments = []\n",
    "for i in range(400):  # 400 English comments\n",
    "    template = random.choice(english_templates)\n",
    "    english_comments.append(template + f\" #{i+1}\")  # add number to make them unique\n",
    "\n",
    "# ---------------------------\n",
    "# 2️⃣ Generate Malayalam Non-Toxic comments\n",
    "# ---------------------------\n",
    "malayalam_templates = [\n",
    "    \"നിങ്ങൾ അത്ഭുതകരമാണ്\",      # You are amazing\n",
    "    \"നല്ല ജോലി\",                 # Good job\n",
    "    \"ഇത് വളരെ സഹായകമാണ്\",      # This is very helpful\n",
    "    \"സഹായത്തിന് നന്ദി\",           # Thanks for help\n",
    "    \"നന്ദി\",                     # Thanks\n",
    "    \"സൂപ്പർ\",                     # Super\n",
    "    \"ഇത് വളരെ നല്ലതാണ്\",        # This is very good\n",
    "]\n",
    "\n",
    "malayalam_comments = []\n",
    "for i in range(300):  # 300 Malayalam comments\n",
    "    template = random.choice(malayalam_templates)\n",
    "    malayalam_comments.append(template + f\" #{i+1}\")\n",
    "\n",
    "# ---------------------------\n",
    "# 3️⃣ Generate Manglish Non-Toxic comments\n",
    "# ---------------------------\n",
    "manglish_templates = [\n",
    "    \"Ningal athbhuthakaramanu\",  \n",
    "    \"Nalla joli\",  \n",
    "    \"Ithu valare sahayakaman\",  \n",
    "    \"Sahayathin nandi\",  \n",
    "    \"Thangalude shrama abhinandanaarkham\",  \n",
    "    \"Super\",  \n",
    "    \"Nandi\",  \n",
    "]\n",
    "\n",
    "manglish_comments = []\n",
    "for i in range(300):  # 300 Manglish comments\n",
    "    template = random.choice(manglish_templates)\n",
    "    manglish_comments.append(template + f\" #{i+1}\")\n",
    "\n",
    "# ---------------------------\n",
    "# 4️⃣ Combine all Non-Toxic comments\n",
    "# ---------------------------\n",
    "all_comments = english_comments + malayalam_comments + manglish_comments\n",
    "languages = [\"english\"]*len(english_comments) + [\"malayalam\"]*len(malayalam_comments) + [\"manglish\"]*len(manglish_comments)\n",
    "\n",
    "nontoxic_df = pd.DataFrame({\n",
    "    \"Comment\": all_comments,\n",
    "    \"label_binary\": [0]*len(all_comments),\n",
    "    \"language\": languages\n",
    "})\n",
    "\n",
    "# ---------------------------\n",
    "# 5️⃣ Append to dataset & save\n",
    "# ---------------------------\n",
    "data = pd.concat([data, nontoxic_df], ignore_index=True)\n",
    "data.to_csv(csv_file, index=False, encoding='utf-8')\n",
    "\n",
    "# ---------------------------\n",
    "# 6️⃣ Verify\n",
    "# ---------------------------\n",
    "print(\"New dataset shape:\", data.shape)\n",
    "print(data['label_binary'].value_counts())\n",
    "print(data['language'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd3c45b-b4a2-41b0-adb5-149816330ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "\n",
    "X = data['Comment']  # make sure to use capital C\n",
    "y = data['label_binary']\n",
    "\n",
    "# TF-IDF\n",
    "tfidf = TfidfVectorizer(max_features=5000)\n",
    "X_vec = tfidf.fit_transform(X)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_vec, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Logistic Regression with class balance\n",
    "model = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Save\n",
    "joblib.dump(tfidf, \"tfidf_vectorizer.pkl\")\n",
    "joblib.dump(model, \"toxic_comment_model.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b8835b-facb-4cde-b135-215a2d2c4c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path to your dataset\n",
    "csv_file = r\"C:\\Users\\anusr\\ToxicCommentFilter\\data\\combined_dataset_clean.csv\"\n",
    "\n",
    "# Load CSV\n",
    "data = pd.read_csv(csv_file)\n",
    "\n",
    "# Drop any rows where Comment is NaN\n",
    "data = data.dropna(subset=['Comment']).reset_index(drop=True)\n",
    "\n",
    "# Make sure label_binary is integer\n",
    "data['label_binary'] = data['label_binary'].astype(int)\n",
    "\n",
    "# Check class distribution\n",
    "print(data['label_binary'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0875ed6-0daf-417a-b2bc-8d6b064be753",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "X = data['Comment']          # Text data\n",
    "y = data['label_binary']     # Labels\n",
    "\n",
    "# TF-IDF vectorizer\n",
    "tfidf = TfidfVectorizer(max_features=5000)\n",
    "X_vec = tfidf.fit_transform(X)  # Convert text to numeric vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620d7d10-8f12-485e-a760-def060d225d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split into train and test (stratify ensures both classes in train/test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_vec, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6739b22-2d78-4748-9afd-115943551563",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Balanced logistic regression to handle class imbalance\n",
    "model = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
    "model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68ddc1a-ee8e-408b-bc29-e6e113e6c5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_toxicity(comment):\n",
    "    comment_vec = tfidf.transform([comment])\n",
    "    prediction = model.predict(comment_vec)[0]\n",
    "    return \"Toxic\" if prediction == 1 else \"Non-Toxic\"\n",
    "\n",
    "print(predict_toxicity(\"You are amazing\"))  # Should now predict Non-Toxic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8669675a-9c67-44d1-8f75-a53f8983f738",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_toxicity(comment):\n",
    "    comment_vec = tfidf.transform([comment])\n",
    "    prediction = model.predict(comment_vec)[0]\n",
    "    return \"Toxic\" if prediction == 1 else \"Non-Toxic\"\n",
    "\n",
    "print(predict_toxicity(\"You are good\"))  # Should now predict Non-Toxic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7c5e71-1608-4991-9afb-31d433027b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# 1️⃣ Define folder and CSV path\n",
    "folder = r\"C:\\Users\\anusr\\ToxicCommentFilter\\data\"\n",
    "csv_file = os.path.join(folder, \"combined_dataset_clean.csv\")\n",
    "\n",
    "# 2️⃣ Create folder if not exists\n",
    "os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "# 3️⃣ Load existing dataset if exists, else create new\n",
    "if os.path.exists(csv_file):\n",
    "    data = pd.read_csv(csv_file)\n",
    "else:\n",
    "    data = pd.DataFrame(columns=[\"comment\", \"label_binary\", \"language\"])\n",
    "\n",
    "# 4️⃣ List of 200+ Manglish non-toxic comments\n",
    "manglish_non_toxic_comments = [\n",
    "\"Amazing work!\",0,English\n",
    "\"Super cute photo!\",0,English\n",
    "\"പാട്ട് വളരെ മനോഹരം!\",0,Malayalam\n",
    "\"Love your style, always on point.\",0,English\n",
    "\"Wah, what a creative post!\",0,English\n",
    "\"Truly inspiring!\",0,English\n",
    "\"Manglish: Ee outfit super aanu!\",0,Manglish\n",
    "\"Your smile is contagious\",0,English\n",
    "\"Beautiful capture!\",0,English\n",
    "\"ചെറുതായി പറഞ്ഞാലും effect ബിഗ് ആണു!\",0,Malayalam\n",
    "\"Keep going, you’re doing great\",0,English\n",
    "\"ഇതുവരെ കാണുന്ന മികച്ച effort!\",0,Malayalam\n",
    "\"Don’t worry, you got this!\",0,English\n",
    "\"Manglish: Chill, nalla try aanu!\",0,Manglish\n",
    "\"Proud of you always!\",0,English\n",
    "\"You’re inspiring many people!\",0,English\n",
    "\"കഷ്ടപ്പാട് കാര്യമല്ല, നീ succeed ചെയ്യും.\",0,Malayalam\n",
    "\"Stay strong!\",0,English\n",
    "\"Manglish: Continue cheyyu, ningal best aanu!\",0,Manglish\n",
    "\"I believe in you\",0,English\n",
    "\"Omg, so funny\",0,English\n",
    "\"Wow, totally unexpected!\",0,English\n",
    "\"ഇങ്ങനെ അത്ഭുതകരമായ content ഇനി കാണാത്തിരിക്കും!\",0,Malayalam\n",
    "\"Manglish: Hahaha, super laugh aanu!\",0,Manglish\n",
    "\"Mind blown\",0,English\n",
    "\"This made my day!\",0,English\n",
    "\"Incredible view\",0,English\n",
    "\"So heartwarming\",0,English\n",
    "\"Manglish: Aiyyo, ente heart melt ayi\",0,Manglish\n",
    "\"Can’t stop watching this!\",0,English\n",
    "\"How did you do this?\",0,English\n",
    "\"ഇതെങ്ങനെ ചെയ്യുന്നു?\",0,Malayalam\n",
    "\"Manglish: Ee recipe share cheyyamo?\",0,Manglish\n",
    "\"Can anyone explain this?\",0,English\n",
    "\"I’m curious, what app do you use?\",0,English\n",
    "\"Beautiful place! Where is this?\",0,English\n",
    "\"Manglish: Ee song title evide nalkunnu?\",0,Manglish\n",
    "\"Who else loves this?\",0,English\n",
    "\"Why is this so satisfying\",0,English\n",
    "\"Can you make a tutorial for this?\",0,English\n",
    "\"Haha, relatable\",0,English\n",
    "\"ഇത് എന്റെ life ആണ്\",0,Malayalam\n",
    "\"Manglish: Njan cheriya laugh stop cheyyan pattunnilla\",0,Manglish\n",
    "\"This is too funny, I’m crying\",0,English\n",
    "\"Literal mood\",0,English\n",
    "\"Mood switched instantly\",0,English\n",
    "\"Manglish: Haha, ente petta friend aanu!\",0,Manglish\n",
    "\"Omg, classic reaction\",0,English\n",
    "\"Can’t breathe from laughing\",0,English\n",
    "\"Funniest thing I’ve seen today\",0,English\n",
    "]\n",
    "\n",
    "# 5️⃣ Convert to DataFrame\n",
    "new_rows_manglish = pd.DataFrame({\n",
    "    \"comment\": manglish_non_toxic_comments,\n",
    "    \"label_binary\": [0]*len(manglish_non_toxic_comments),\n",
    "    \"language\": [\"manglish\"]*len(manglish_non_toxic_comments)\n",
    "})\n",
    "\n",
    "# 6️⃣ Append to existing dataset\n",
    "data = pd.concat([data, new_rows_manglish], ignore_index=True)\n",
    "\n",
    "# 7️⃣ Save back to CSV\n",
    "data.to_csv(csv_file, index=False)\n",
    "\n",
    "print(f\"{len(manglish_non_toxic_comments)} Manglish non-toxic comments added successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe1c3d0-5ed5-4044-98c8-2cf486f99360",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# 1️⃣ Define folder and CSV path\n",
    "folder = r\"C:\\Users\\anusr\\ToxicCommentFilter\\data\"\n",
    "csv_file = os.path.join(folder, \"combined_dataset_clean.csv\")\n",
    "\n",
    "# 2️⃣ Create folder if not exists\n",
    "os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "# 3️⃣ Load existing dataset if exists, else create new\n",
    "if os.path.exists(csv_file):\n",
    "    data = pd.read_csv(csv_file)\n",
    "else:\n",
    "    data = pd.DataFrame(columns=[\"comment\", \"label_binary\", \"language\"])\n",
    "\n",
    "# 4️⃣ List of Manglish non-toxic comments (only comments)\n",
    "manglish_non_toxic_comments = [\n",
    "    \"Amazing work!\",\n",
    "    \"Super cute photo!\",\n",
    "    \"പാട്ട് വളരെ മനോഹരം!\",\n",
    "    \"Love your style, always on point.\",\n",
    "    \"Wah, what a creative post!\",\n",
    "    \"Truly inspiring!\",\n",
    "    \"Manglish: Ee outfit super aanu!\",\n",
    "    \"Your smile is contagious\",\n",
    "    \"Beautiful capture!\",\n",
    "    \"ചെറുതായി പറഞ്ഞാലും effect ബിഗ് ആണു!\",\n",
    "    \"Keep going, you’re doing great\",\n",
    "    \"ഇതുവരെ കാണുന്ന മികച്ച effort!\",\n",
    "    \"Don’t worry, you got this!\",\n",
    "    \"Manglish: Chill, nalla try aanu!\",\n",
    "    \"Proud of you always!\",\n",
    "    \"You’re inspiring many people!\",\n",
    "    \"കഷ്ടപ്പാട് കാര്യമല്ല, നീ succeed ചെയ്യും.\",\n",
    "    \"Stay strong!\",\n",
    "    \"Manglish: Continue cheyyu, ningal best aanu!\",\n",
    "    \"I believe in you\",\n",
    "    \"Omg, so funny\",\n",
    "    \"Wow, totally unexpected!\",\n",
    "    \"ഇങ്ങനെ അത്ഭുതകരമായ content ഇനി കാണാത്തിരിക്കും!\",\n",
    "    \"Manglish: Hahaha, super laugh aanu!\",\n",
    "    \"Mind blown\",\n",
    "    \"This made my day!\",\n",
    "    \"Incredible view\",\n",
    "    \"So heartwarming\",\n",
    "    \"Manglish: Aiyyo, ente heart melt ayi\",\n",
    "    \"Can’t stop watching this!\",\n",
    "    \"How did you do this?\",\n",
    "    \"ഇതെങ്ങനെ ചെയ്യുന്നു?\",\n",
    "    \"Manglish: Ee recipe share cheyyamo?\",\n",
    "    \"Can anyone explain this?\",\n",
    "    \"I’m curious, what app do you use?\",\n",
    "    \"Beautiful place! Where is this?\",\n",
    "    \"Manglish: Ee song title evide nalkunnu?\",\n",
    "    \"Who else loves this?\",\n",
    "    \"Why is this so satisfying\",\n",
    "    \"Can you make a tutorial for this?\",\n",
    "    \"Haha, relatable\",\n",
    "    \"ഇത് എന്റെ life ആണ്\",\n",
    "    \"Manglish: Njan cheriya laugh stop cheyyan pattunnilla\",\n",
    "    \"This is too funny, I’m crying\",\n",
    "    \"Literal mood\",\n",
    "    \"Mood switched instantly\",\n",
    "    \"Manglish: Haha, ente petta friend aanu!\",\n",
    "    \"Omg, classic reaction\",\n",
    "    \"Can’t breathe from laughing\",\n",
    "    \"Funniest thing I’ve seen today\",\n",
    "    \"❤️🔥\",\n",
    "    \"😍👏✨\",\n",
    "    \"😎💯\",\n",
    "    \"🥰🙌\",\n",
    "    \"😂🤣\",\n",
    "    \"🌸💖🌼\",\n",
    "    \"🤩👏\",\n",
    "    \"Manglish: 😁👌\",\n",
    "    \"🙏🌟\",\n",
    "    \"💯💯💯\"\n",
    "]\n",
    "\n",
    "# 5️⃣ Convert to DataFrame\n",
    "new_rows_manglish = pd.DataFrame({\n",
    "    \"comment\": manglish_non_toxic_comments,\n",
    "    \"label_binary\": [0]*len(manglish_non_toxic_comments),\n",
    "    \"language\": [\"manglish\"]*len(manglish_non_toxic_comments)\n",
    "})\n",
    "\n",
    "# 6️⃣ Append to existing dataset\n",
    "data = pd.concat([data, new_rows_manglish], ignore_index=True)\n",
    "\n",
    "# 7️⃣ Save back to CSV\n",
    "data.to_csv(csv_file, index=False)\n",
    "\n",
    "print(f\"{len(manglish_non_toxic_comments)} Manglish non-toxic comments added successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d440bffb-c812-497a-892b-71777502de67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f567f1f0-e4d1-4555-9cff-054ff6a38dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your CSV\n",
    "csv_file = r\"C:\\Users\\anusr\\ToxicCommentFilter\\data\\combined_dataset_clean.csv\"\n",
    "data = pd.read_csv(csv_file)\n",
    "\n",
    "# Check data\n",
    "print(data.head())\n",
    "print(data['label_binary'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b86afa-ed4b-4b78-9562-8e80b6b0db1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)  # remove punctuation\n",
    "    return text\n",
    "\n",
    "data['comment_clean'] = data['comment'].astype(str).apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6f18b2-653d-42bd-b8a9-3baa8243baaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['comment_clean']\n",
    "y = data['label_binary']\n",
    "\n",
    "# Split 80% train, 20% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c3f564-c48c-466e-8db2-51611a710224",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1,2))\n",
    "X_train_vec = tfidf.fit_transform(X_train)\n",
    "X_test_vec = tfidf.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7f2d50-13c8-4b34-afe4-a662ae55eec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle class imbalance if needed\n",
    "model = LogisticRegression(class_weight='balanced', max_iter=500)\n",
    "\n",
    "# Train\n",
    "model.fit(X_train_vec, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfe6556-d816-44a6-88c9-517ba9f5a660",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test_vec)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bad2ef8-ed07-4a41-8df1-5cf8d5734903",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_toxicity(comment):\n",
    "    comment_clean = clean_text(comment)\n",
    "    comment_vec = tfidf.transform([comment_clean])\n",
    "    prediction = model.predict(comment_vec)[0]\n",
    "    return \"Toxic\" if prediction == 1 else \"Non-Toxic\"\n",
    "\n",
    "# Test\n",
    "print(predict_toxicity(\"You are amazing!\"))   # Expected: Non-Toxic\n",
    "print(predict_toxicity(\"Idiot!\"))             # Expected: Toxic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5dcf717-b649-430a-93de-e3482cc686ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load merged dataset\n",
    "csv_file = r\"C:\\Users\\anusr\\ToxicCommentFilter\\data\\cleaned_merged_data.csv\"\n",
    "data = pd.read_csv(csv_file)\n",
    "\n",
    "# Check initial size\n",
    "print(\"Original dataset size:\", len(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8282d471-a53a-481a-a8fe-2a7f272cb47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load merged dataset\n",
    "csv_file = r\"C:\\Users\\anusr\\ToxicCommentFilter\\data\\cimbined_dtaset_clean.csv\"\n",
    "data = pd.read_csv(csv_file)\n",
    "\n",
    "# Check initial size\n",
    "print(\"Original dataset size:\", len(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c603ad-a915-4013-9d13-8c900357ce29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load merged dataset\n",
    "csv_file = r\"C:\\Users\\anusr\\ToxicCommentFilter\\data\\combined_dtaset_clean.csv\"\n",
    "data = pd.read_csv(csv_file)\n",
    "\n",
    "# Check initial size\n",
    "print(\"Original dataset size:\", len(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518167fd-222f-4e69-aa16-6e40a73aebd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load merged dataset\n",
    "csv_file = r\"C:\\Users\\anusr\\ToxicCommentFilter\\data\\combined_dataset_clean.csv\"\n",
    "data = pd.read_csv(csv_file)\n",
    "\n",
    "# Check initial size\n",
    "print(\"Original dataset size:\", len(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45037a55-6f15-49d5-8d49-292fd1cf20a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop exact duplicate comments\n",
    "data = data.drop_duplicates(subset=['comment'])\n",
    "print(\"After removing exact duplicates:\", len(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf0ea33-360a-4a0f-8c7a-8c188fff1d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove empty or very short comments (<3 chars)\n",
    "data = data[data['comment'].str.strip().str.len() > 2]\n",
    "print(\"After removing very short/empty comments:\", len(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf0dab1-11be-4f98-9ec0-2f4333090eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your CSV\n",
    "file_path = \"jupyter/ToxicCommentFilter/data/combined_dataset_clean.csv\"\n",
    "df = pd.read_csv(file_path)  # Add delimiter=',' if needed, e.g., pd.read_csv(file_path, delimiter=',')\n",
    "\n",
    "# Check the column names to ensure correct merging\n",
    "print(df.columns)\n",
    "\n",
    "# Merge last column into the first column\n",
    "# Replace 'Comment' and 'column' with actual column names if different\n",
    "df['Comment'] = df['Comment'].fillna('') + df['column'].fillna('')\n",
    "\n",
    "# Drop the last column\n",
    "df = df.drop(columns=['column'])\n",
    "\n",
    "# Save the updated dataframe back to CSV\n",
    "df.to_csv(\"jupyter/ToxicCommentFilter/data/combined_dataset_clean_updated.csv\", index=False)\n",
    "\n",
    "# Show the updated dataframe\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0107e0cb-ae78-4085-b446-3c7659bca283",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_comments = [\n",
    "    \"സുപ്രഭാതം, നല്ലൊരു ദിവസം ആകട്ടെ!\",\n",
    "    \"നിനക്ക് എല്ലാ വിജയങ്ങളും നേരുന്നു.\",\n",
    "    \"ജന്മദിനാശംസകൾ, സന്തോഷത്തോടെ ഇരിക്കൂ!\",\n",
    "    \"നിന്റെ പരിശ്രമം അഭിനന്ദനാർഹമാണ്.\",\n",
    "    \"ദൈവം നിന്നെ അനുഗ്രഹിക്കട്ടെ.\",\n",
    "    \"നല്ല ജോലി, തുടരൂ!\",\n",
    "\n",
    "    \"Good morning! Have a wonderful day.\",\n",
    "    \"Wishing you all the success in your life.\",\n",
    "    \"Happy birthday! Stay happy and healthy.\",\n",
    "    \"Great job, keep it up!\",\n",
    "    \"All the best for your future.\",\n",
    "    \"You are doing an amazing job.\",\n",
    "\n",
    "    \"Suprabhatham, nalla oru divasam aakatte!\",\n",
    "    \"Ningalk ellaa vijayangalum nerunnu.\",\n",
    "    \"Happy birthday, santhoshathode irikku!\",\n",
    "    \"Ninte effort valare nannayi.\",\n",
    "    \"Daivam ninne anugrahikkatte.\",\n",
    "    \"Nalla work aanu, continue cheyyu!\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7accbd06-4ced-4b4d-b684-d1c2c114e1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load existing dataset\n",
    "file_path = r\"C:\\Users\\anusr\\ToxicCommentFilter\\data\\combined_dataset_cleaned_updated_trimmed.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Create new DataFrame for wishes/greetings\n",
    "new_data = pd.DataFrame({\n",
    "    \"Comment\": new_comments,\n",
    "    \"label_binary\": [0] * len(new_comments)  # Non-toxic\n",
    "})\n",
    "\n",
    "# Append to existing dataset\n",
    "df_updated = pd.concat([df, new_data], ignore_index=True)\n",
    "\n",
    "# Save back to CSV\n",
    "df_updated.to_csv(\n",
    "    r\"C:\\Users\\anusr\\ToxicCommentFilter\\data\\combined_dataset_with_wishes.csv\",\n",
    "    index=False\n",
    ")\n",
    "\n",
    "# Check result\n",
    "print(\"Total rows after adding:\", len(df_updated))\n",
    "print(df_updated.tail())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4d524f-087c-4e0f-83ff-ded076010054",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load existing dataset\n",
    "file_path = r\"C:\\Users\\anusr\\ToxicCommentFilter\\data\\combined_dataset_cleaned_updated_trimmed.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Check column names\n",
    "print(\"Columns:\", df.columns)\n",
    "\n",
    "# 🔹 Use the correct column name (change if needed)\n",
    "COMMENT_COL = \"Comment\"   # change to \"comment\" if your CSV uses lowercase\n",
    "\n",
    "# New merged greeting / wishes data\n",
    "new_comments = [\n",
    "    \"Hello, good morning എല്ലാവർക്കും, hope you have a great day\",\n",
    "    \"Happy birthday bro, janmadina asamsakal 🎉\",\n",
    "    \"Good evening എല്ലാവർക്കും, stay safe and smile\",\n",
    "    \"Congratulations on your success, nannayi cheythu 👏\",\n",
    "    \"Best wishes for your exams, all the best makkale\",\n",
    "    \"Have a nice day, ഇന്നത്തെ ദിവസം നല്ലതായിരിക്കട്ടെ\",\n",
    "    \"Welcome back, തിരിച്ചു വന്നതിൽ സന്തോഷം\",\n",
    "    \"Good luck for your future, bright future undavatte\",\n",
    "    \"Season’s greetings, ellavarkkum ആശംസകൾ\",\n",
    "    \"Thank you so much, valare santhosham undu\"\n",
    "]\n",
    "\n",
    "# Create new DataFrame\n",
    "new_data = pd.DataFrame({\n",
    "    COMMENT_COL: new_comments,\n",
    "    \"label_binary\": [0] * len(new_comments)  # Non-toxic\n",
    "})\n",
    "\n",
    "# Append to existing dataset\n",
    "df_updated = pd.concat([df, new_data], ignore_index=True)\n",
    "\n",
    "# Save updated CSV\n",
    "output_path = r\"C:\\Users\\anusr\\ToxicCommentFilter\\data\\combined_dataset_with_wishes.csv\"\n",
    "df_updated.to_csv(output_path, index=False)\n",
    "\n",
    "# Verify result\n",
    "print(\"✅ Data added successfully!\")\n",
    "print(\"Total rows after adding:\", len(df_updated))\n",
    "print(df_updated.tail())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d738de1a-7ea5-4141-9bd4-37761a97602c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load existing dataset\n",
    "file_path = r\"C:\\Users\\anusr\\ToxicCommentFilter\\data\\combined_dataset_clean_updated_trimmed.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Check column names\n",
    "print(\"Columns:\", df.columns)\n",
    "\n",
    "# 🔹 Use the correct column name (change if needed)\n",
    "COMMENT_COL = \"Comment\"   # change to \"comment\" if your CSV uses lowercase\n",
    "\n",
    "# New merged greeting / wishes data\n",
    "new_comments = [\n",
    "    \"Hello, good morning എല്ലാവർക്കും, hope you have a great day\",\n",
    "    \"Happy birthday bro, janmadina asamsakal 🎉\",\n",
    "    \"Good evening എല്ലാവർക്കും, stay safe and smile\",\n",
    "    \"Congratulations on your success, nannayi cheythu 👏\",\n",
    "    \"Best wishes for your exams, all the best makkale\",\n",
    "    \"Have a nice day, ഇന്നത്തെ ദിവസം നല്ലതായിരിക്കട്ടെ\",\n",
    "    \"Welcome back, തിരിച്ചു വന്നതിൽ സന്തോഷം\",\n",
    "    \"Good luck for your future, bright future undavatte\",\n",
    "    \"Season’s greetings, ellavarkkum ആശംസകൾ\",\n",
    "    \"Thank you so much, valare santhosham undu\"\n",
    "]\n",
    "\n",
    "# Create new DataFrame\n",
    "new_data = pd.DataFrame({\n",
    "    COMMENT_COL: new_comments,\n",
    "    \"label_binary\": [0] * len(new_comments)  # Non-toxic\n",
    "})\n",
    "\n",
    "# Append to existing dataset\n",
    "df_updated = pd.concat([df, new_data], ignore_index=True)\n",
    "\n",
    "# Save updated CSV\n",
    "output_path = r\"C:\\Users\\anusr\\ToxicCommentFilter\\data\\combined_dataset_with_wishes.csv\"\n",
    "df_updated.to_csv(output_path, index=False)\n",
    "\n",
    "# Verify result\n",
    "print(\"✅ Data added successfully!\")\n",
    "print(\"Total rows after adding:\", len(df_updated))\n",
    "print(df_updated.tail())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f116f44-4bbd-4d52-b9b0-7746edfa85ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing language values\n",
    "df['language'] = df['language'].fillna('mixed')\n",
    "\n",
    "print(df.tail())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a1f660-186a-4a8c-a9ab-8421ea9e6551",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = r\"C:\\Users\\anusr\\ToxicCommentFilter\\data\\combined_dataset_clean_updated_trimmed.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "print(\"Columns:\", df.columns)\n",
    "print(\"Total rows:\", len(df))\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e66c14a-c6bc-4dca-bedf-7839de2b01ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_comments = [\n",
    "    \"Have a nice day, ഇന്നത്തെ ദിവസം നല്ലതായിരിക്കട്ടെ\",\n",
    "    \"Welcome back, തിരിച്ചു വന്നതിൽ സന്തോഷം\",\n",
    "    \"Good luck for your future, bright future undാവട്ടെ\",\n",
    "    \"Season’s greetings, എല്ലാവർക്കും ആശംസകൾ\",\n",
    "    \"Thank you so much, വളരെ സന്തോഷം ഉണ്ടു\"\n",
    "]\n",
    "\n",
    "new_data = pd.DataFrame({\n",
    "    \"Comment\": new_comments,\n",
    "    \"label_binary\": [0] * len(new_comments),\n",
    "    \"language\": [\"mixed\"] * len(new_comments)\n",
    "})\n",
    "\n",
    "df = pd.concat([df, new_data], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac904173-dcf3-4947-b361-c6791fb9d0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\n",
    "    r\"C:\\Users\\anusr\\ToxicCommentFilter\\data\\combined_dataset_with_wishes.csv\",\n",
    "    index=False\n",
    ")\n",
    "\n",
    "print(\"✅ Data added successfully!\")\n",
    "print(\"Total rows after adding:\", len(df))\n",
    "df.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106e0a84-6ba7-4c47-a4f2-787093413cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_toxicity(comment):\n",
    "    comment_vec = tfidf.transform([comment])\n",
    "    prediction = model.predict(comment_vec)[0]\n",
    "\n",
    "    if prediction == 1:\n",
    "        return \"🚫 Toxic\"\n",
    "    else:\n",
    "        return \"✅ Non-Toxic\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8016616b-a022-4b64-b0ab-74df184a13f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(check_toxicity(\"Ini ee method valare simple aanu\"))\n",
    "print(check_toxicity(\"Nee poy chavv\"))\n",
    "print(check_toxicity(\"You are doing a great job\"))\n",
    "print(check_toxicity(\"You are useless and stupid\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e014a59-8737-45e7-8bcf-522cc6894272",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_comments = [\n",
    "    \"Ini ee strategy valare useful aanu\",\n",
    "    \"Nee poy chavv\",\n",
    "    \"Great job macha\",\n",
    "    \"Ninte budhi onnum illa\",\n",
    "    \"Have a nice day\",\n",
    "    \"Poda mandan\"\n",
    "]\n",
    "\n",
    "for c in test_comments:\n",
    "    print(c, \"→\", check_toxicity(c))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c22e36-e6d6-4b54-93be-db0e4f581ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict toxicity\n",
    "def predict_toxicity(comment):\n",
    "    # Transform the comment using the trained TF-IDF vectorizer\n",
    "    comment_vec = tfidf.transform([comment])\n",
    "    \n",
    "    # Predict using the trained model\n",
    "    prediction = model.predict(comment_vec)[0]\n",
    "    \n",
    "    # Return label\n",
    "    return \"Toxic\" if prediction == 1 else \"Non-Toxic\"\n",
    "\n",
    "# Examples to test\n",
    "comments_to_test = [\n",
    "    \"You are amazing!\",\n",
    "    \"I hate you so much!\",\n",
    "    \"Thanks for your help, really appreciate it.\"\n",
    "]\n",
    "\n",
    "for c in comments_to_test:\n",
    "    print(f\"Comment: {c}\\nPrediction: {predict_toxicity(c)}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf9e456-af42-4946-a994-b0e8f8395b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864aeb0e-6198-46e6-bee2-8792c43035b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r\"C:\\Users\\anusr\\ToxicCommentFilter\\data\\combined_dataset_clean_updated_trimmed.csv\"\n",
    "df = pd.read_csv(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762ba9e0-43f5-4e19-a316-b2e80793435a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['Comment']\n",
    "y = df['label_binary']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fea9cf-830f-4df4-accf-5ef7e584722f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(\n",
    "    ngram_range=(1,2),\n",
    "    min_df=2,\n",
    "    max_df=0.9\n",
    ")\n",
    "\n",
    "X_train_vec = tfidf.fit_transform(X_train)\n",
    "X_test_vec = tfidf.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4750be35-0f6d-4e1b-8fd9-21a88fdce9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "\n",
    "model.fit(X_train_vec, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06014d4b-0743-4089-bfc9-8ce45411e44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test_vec)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7219e8-7369-4c62-8cac-56078a51b82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_toxicity(comment):\n",
    "    comment_vec = tfidf.transform([comment])\n",
    "    prediction = model.predict(comment_vec)[0]\n",
    "    return \"🚫 Toxic\" if prediction == 1 else \"✅ Non-Toxic\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fbb440-f82d-4172-87c7-873bb422eebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_to_test = [\n",
    "    \"You are amazing!\",\n",
    "    \"congratulations!\",\n",
    "    \"Thanks for your help, really appreciate it.\",\n",
    "    \"Happy birthday\",\n",
    "    \"you are good\"\n",
    "]\n",
    "\n",
    "for c in comments_to_test:\n",
    "    print(f\"Comment: {c}\")\n",
    "    print(\"Prediction:\", predict_toxicity(c))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ebcc7d-09c1-466a-a6e4-afa6d09fe004",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_to_test = [\n",
    "    \"You are amazing!\",\n",
    "    \"congratulations!\",\n",
    "    \"Thanks for your help, really appreciate it.\",\n",
    "    \"Happy birthday\",\n",
    "    \"you are good\"\n",
    "]\n",
    "\n",
    "for c in comments_to_test:\n",
    "    print(f\"Comment: {c}\")\n",
    "    print(\"Prediction:\", predict_toxicity(c))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67634c06-f292-43fe-995e-231fa8fdc7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load existing dataset\n",
    "file_path = r\"C:\\Users\\anusr\\ToxicCommentFilter\\data\\combined_dataset_clean_updated_trimmed.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Check column names\n",
    "print(\"Columns:\", df.columns)\n",
    "\n",
    "# 🔹 Use the correct column name (change if needed)\n",
    "COMMENT_COL = \"Comment\"   # change to \"comment\" if your CSV uses lowercase\n",
    "\n",
    "# New merged greeting / wishes data\n",
    "new_comments = [\n",
    "    \"Hello, good morning എല്ലാവർക്കും, hope you have a great day\",\n",
    "    \"Happy birthday bro, janmadina asamsakal 🎉\",\n",
    "    \"Good evening എല്ലാവർക്കും, stay safe and smile\",\n",
    "    \"Congratulations on your success, nannayi cheythu 👏\",\n",
    "    \"Best wishes for your exams, all the best makkale\",\n",
    "    \"Have a nice day, ഇന്നത്തെ ദിവസം നല്ലതായിരിക്കട്ടെ\",\n",
    "    \"Welcome back, തിരിച്ചു വന്നതിൽ സന്തോഷം\",\n",
    "    \"Good luck for your future, bright future undavatte\",\n",
    "    \"Season’s greetings, ellavarkkum ആശംസകൾ\",\n",
    "    \"Thank you so much, valare santhosham undu\"\n",
    "]\n",
    "\n",
    "# Create new DataFrame\n",
    "new_data = pd.DataFrame({\n",
    "    COMMENT_COL: new_comments,\n",
    "    \"label_binary\": [0] * len(new_comments)  # Non-toxic\n",
    "})\n",
    "\n",
    "# Append to existing dataset\n",
    "df_updated = pd.concat([df, new_data], ignore_index=True)\n",
    "\n",
    "# Save updated CSV\n",
    "output_path = r\"C:\\Users\\anusr\\ToxicCommentFilter\\data\\combined_dataset_clean_updated_trimmed.csv\"\n",
    "df_updated.to_csv(output_path, index=False)\n",
    "\n",
    "# Verify result\n",
    "print(\"✅ Data added successfully!\")\n",
    "print(\"Total rows after adding:\", len(df_updated))\n",
    "print(df_updated.tail())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340f55e2-9e51-4c32-b7d5-c64f45faf20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fill missing language values\n",
    "df['language'] = df['language'].fillna('mixed')\n",
    "\n",
    "print(df.tail())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ad88af-4064-4835-93a4-059ed3c3a3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trimmed = df.iloc[:1998]\n",
    "\n",
    "df_trimmed.to_csv(\n",
    "    r\"C:\\Users\\anusr\\ToxicCommentFilter\\data\\combined_dataset_clean_updated_trimmed.csv\",\n",
    "    index=False\n",
    ")\n",
    "\n",
    "print(\"Rows after trimming:\", len(df_trimmed))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752ed5a2-17b8-403d-8099-4985f4503ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.info())\n",
    "print(df['label_binary'].value_counts())\n",
    "print(df['language'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed46752-948d-4831-8653-692911efd16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_toxicity(comment):\n",
    "    comment_vec = tfidf.transform([comment])\n",
    "    prediction = model.predict(comment_vec)[0]\n",
    "    return \"🚫 Toxic\" if prediction == 1 else \"✅ Non-Toxic\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdd437f-cfd4-4ece-9b48-9485a093542c",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_to_test = [\n",
    "    \"You are amazing!\",\n",
    "    \"congratulations!\",\n",
    "    \"Thanks for your help, really appreciate it.\",\n",
    "    \"Happy birthday\",\n",
    "    \"you are good\"\n",
    "]\n",
    "\n",
    "for c in comments_to_test:\n",
    "    print(f\"Comment: {c}\")\n",
    "    print(\"Prediction:\", predict_toxicity(c))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599a1021-5f23-4858-9edd-d75ab4733ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "NON_TOXIC_KEYWORDS = [\n",
    "    \"congratulations\",\n",
    "    \"happy birthday\",\n",
    "    \"happy\",\n",
    "    \"birthday\",\n",
    "    \"thanks\",\n",
    "    \"thank you\",\n",
    "    \"welcome\",\n",
    "    \"good job\",\n",
    "    \"well done\",\n",
    "    \"great\",\n",
    "    \"amazing\",\n",
    "    \"nice\",\n",
    "    \"awesome\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409a59da-429a-4252-ba48-c3adc8fbc09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_toxicity(comment):\n",
    "    text = comment.lower().strip()\n",
    "\n",
    "    # Rule-based non-toxic check\n",
    "    for word in NON_TOXIC_KEYWORDS:\n",
    "        if word in text:\n",
    "            return \"✅ Non-Toxic\"\n",
    "\n",
    "    # ML prediction\n",
    "    vec = tfidf.transform([comment])\n",
    "    pred = model.predict(vec)[0]\n",
    "\n",
    "    return \"🚫 Toxic\" if pred == 1 else \"✅ Non-Toxic\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a0e1d3-1023-460b-b1f3-36b60c21147f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = [\n",
    "    \"congratulations!\",\n",
    "    \"Happy birthday\",\n",
    "    \"You are amazing!\",\n",
    "    \"I hate you\",\n",
    "    \"Nee poy chavv\"\n",
    "]\n",
    "\n",
    "for t in tests:\n",
    "    print(t, \"→\", predict_toxicity(t))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec109ea-059e-488c-9e4f-ba7196bd03c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_to_test = [\n",
    "    \"You are amazing!\",\n",
    "    \"congratulations!\",\n",
    "    \"Thanks for your help, really appreciate it.\",\n",
    "    \"Happy birthday\",\n",
    "    \"you are good\"\n",
    "]\n",
    "\n",
    "for c in comments_to_test:\n",
    "    print(f\"Comment: {c}\")\n",
    "    print(\"Prediction:\", predict_toxicity(c))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718b7bb0-1b2f-4e76-8474-1c683f0bc2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_to_test = [\n",
    "    \"You are not amazing!\",\n",
    "    \"congratulations!\",\n",
    "    \"Thanks for your help, really appreciate it.\",\n",
    "    \"Happy birthday\",\n",
    "    \"you are good\"\n",
    "]\n",
    "\n",
    "for c in comments_to_test:\n",
    "    print(f\"Comment: {c}\")\n",
    "    print(\"Prediction:\", predict_toxicity(c))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96ff812-e6f9-4702-984e-5ba0db115aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEGATIONS = [\"not\", \"never\", \"no\", \"dont\", \"don't\", \"isn't\", \"isnt\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4226c4-bc03-45ce-88dc-083f917eb9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "NON_TOXIC_KEYWORDS = [\n",
    "    \"congratulations\",\n",
    "    \"happy birthday\",\n",
    "    \"thanks\",\n",
    "    \"thank you\",\n",
    "    \"welcome\",\n",
    "    \"good job\",\n",
    "    \"well done\",\n",
    "    \"great\",\n",
    "    \"amazing\",\n",
    "    \"nice\",\n",
    "    \"awesome\",\n",
    "    \"good\"\n",
    "]\n",
    "\n",
    "NEGATIONS = [\"not\", \"never\", \"no\", \"dont\", \"don't\", \"isn't\", \"isnt\"]\n",
    "\n",
    "def predict_toxicity(comment):\n",
    "    text = comment.lower().strip()\n",
    "\n",
    "    # Check for negation\n",
    "    has_negation = any(neg in text for neg in NEGATIONS)\n",
    "\n",
    "    # Rule-based non-toxic only if NO negation\n",
    "    if not has_negation:\n",
    "        for word in NON_TOXIC_KEYWORDS:\n",
    "            if word in text:\n",
    "                return \"✅ Non-Toxic\"\n",
    "\n",
    "    # ML prediction fallback\n",
    "    vec = tfidf.transform([comment])\n",
    "    pred = model.predict(vec)[0]\n",
    "\n",
    "    return \"🚫 Toxic\" if pred == 1 else \"✅ Non-Toxic\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816edaba-10ca-4ca1-b45c-0a7c06e26963",
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = [\n",
    "    \"You are amazing!\",\n",
    "    \"You are not amazing!\",\n",
    "    \"Happy birthday\",\n",
    "    \"I am not happy with you\",\n",
    "    \"Thanks for your help\",\n",
    "    \"I hate you\"\n",
    "]\n",
    "\n",
    "for t in tests:\n",
    "    print(t, \"→\", predict_toxicity(t))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee828c70-3384-4e20-95af-c67e11ed5499",
   "metadata": {},
   "outputs": [],
   "source": [
    "NON_TOXIC_KEYWORDS = [\n",
    "    \"congratulations\",\n",
    "    \"happy birthday\",\n",
    "    \"thanks\",\n",
    "    \"thank you\",\n",
    "    \"welcome\",\n",
    "    \"good job\",\n",
    "    \"well done\",\n",
    "    \"great\",\n",
    "    \"amazing\",\n",
    "    \"nice\",\n",
    "    \"awesome\",\n",
    "    \"good\"\n",
    "]\n",
    "\n",
    "NEGATIONS = [\"not\", \"never\", \"no\", \"dont\", \"don't\", \"isn't\", \"isnt\"]\n",
    "\n",
    "def predict_toxicity(comment):\n",
    "    text = comment.lower().strip()\n",
    "\n",
    "    # 1️⃣ If negation exists → SKIP rule-based check\n",
    "    if not any(neg in text for neg in NEGATIONS):\n",
    "        for word in NON_TOXIC_KEYWORDS:\n",
    "            if word in text:\n",
    "                return \"✅ Non-Toxic\"\n",
    "\n",
    "    # 2️⃣ Otherwise → ML model decides\n",
    "    comment_vec = tfidf.transform([comment])\n",
    "    prediction = model.predict(comment_vec)[0]\n",
    "\n",
    "    return \"🚫 Toxic\" if prediction == 1 else \"✅ Non-Toxic\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b085151-7517-404f-8d34-7ea3b0f8dcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = [\n",
    "    \"You are amazing!\",\n",
    "    \"You are not amazing!\",\n",
    "    \"Happy birthday\",\n",
    "    \"I am not happy with you\",\n",
    "    \"Thanks for your help\",\n",
    "    \"I hate you\"\n",
    "]\n",
    "\n",
    "for t in tests:\n",
    "    print(t, \"→\", predict_toxicity(t))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00caeed0-47e4-43c0-8e4f-117a9887daf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "NON_TOXIC_KEYWORDS = [\n",
    "    \"amazing\", \"great\", \"good\", \"awesome\",\n",
    "    \"happy birthday\", \"thanks\", \"thank you\",\n",
    "    \"congratulations\", \"well done\", \"nice\"\n",
    "]\n",
    "\n",
    "NEGATIONS = [\"not\", \"never\", \"no\", \"dont\", \"don't\", \"isn't\", \"isnt\"]\n",
    "\n",
    "def predict_toxicity(comment):\n",
    "    text = comment.lower().strip()\n",
    "\n",
    "    # 1️⃣ Explicit negation + positive word → Toxic\n",
    "    for neg in NEGATIONS:\n",
    "        for word in NON_TOXIC_KEYWORDS:\n",
    "            if f\"{neg} {word}\" in text:\n",
    "                return \"🚫 Toxic\"\n",
    "\n",
    "    # 2️⃣ Pure positive phrases → Non-Toxic\n",
    "    for word in NON_TOXIC_KEYWORDS:\n",
    "        if word in text:\n",
    "            return \"✅ Non-Toxic\"\n",
    "\n",
    "    # 3️⃣ Otherwise ML decides\n",
    "    comment_vec = tfidf.transform([comment])\n",
    "    prediction = model.predict(comment_vec)[0]\n",
    "\n",
    "    return \"🚫 Toxic\" if prediction == 1 else \"✅ Non-Toxic\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3031680-a5a1-4d3c-a776-db2a45ad62a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = [\n",
    "    \"You are amazing!\",\n",
    "    \"You are not amazing!\",\n",
    "    \"Happy birthday\",\n",
    "    \"I am not happy with you\",\n",
    "    \"Thanks for your help\",\n",
    "    \"I hate you\"\n",
    "]\n",
    "\n",
    "for t in tests:\n",
    "    print(t, \"→\", predict_toxicity(t))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a307783-9efc-48ef-a3f7-a36d41c74f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = [\n",
    "    \"congratulations\",\n",
    "    \"You are not amazing!\",\n",
    "    \"Happy birthday\",\n",
    "    \"hope you are not in struggle\",\n",
    "    \"hope you are doing well\",\n",
    "    \"sweet dreams\"\n",
    "]\n",
    "\n",
    "for t in tests:\n",
    "    print(t, \"→\", predict_toxicity(t))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c34b6d-d777-4f8e-9787-ef5727c0eed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "NON_TOXIC_KEYWORDS = [\n",
    "    \"amazing\", \"great\", \"good\", \"awesome\",\n",
    "    \"happy birthday\", \"thanks\", \"thank you\",\n",
    "    \"congratulations\", \"well done\", \"nice\",\n",
    "    \"welcome\", \"hope you are doing well\",\n",
    "    \"sweet dreams\", \"have a nice day\", \"good night\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940169a8-4cb4-46c0-a887-5cf622db5186",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_toxicity(comment):\n",
    "    text = comment.lower().strip()\n",
    "\n",
    "    # Negation + positive → Toxic\n",
    "    for neg in NEGATIONS:\n",
    "        for word in NON_TOXIC_KEYWORDS:\n",
    "            if f\"{neg} {word}\" in text:\n",
    "                return \"🚫 Toxic\"\n",
    "\n",
    "    # Positive keyword → Non-Toxic\n",
    "    for word in NON_TOXIC_KEYWORDS:\n",
    "        if word in text:\n",
    "            return \"✅ Non-Toxic\"\n",
    "\n",
    "    # Otherwise ML decides\n",
    "    comment_vec = tfidf.transform([comment])\n",
    "    prediction = model.predict(comment_vec)[0]\n",
    "\n",
    "    return \"🚫 Toxic\" if prediction == 1 else \"✅ Non-Toxic\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2db77d-9424-4fec-8d4e-e6fd56ee4ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = [\n",
    "    \"congratulations\",\n",
    "    \"You are not amazing!\",\n",
    "    \"Happy birthday\",\n",
    "    \"hope you are not in struggle\",\n",
    "    \"hope you are doing well\",\n",
    "    \"sweet dreams\"\n",
    "]\n",
    "\n",
    "for t in tests:\n",
    "    print(t, \"→\", predict_toxicity(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d936711-5ea5-4c6d-aaa8-64bf46f4d36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "NON_TOXIC_KEYWORDS = [\n",
    "    \"amazing\", \"great\", \"good\", \"awesome\",\n",
    "    \"happy birthday\", \"thanks\", \"thank you\",\n",
    "    \"congratulations\", \"well done\", \"nice\",\n",
    "    \"welcome\", \"hope you are doing well\",\n",
    "    \"sweet dreams\", \"have a nice day\", \"good night\"\n",
    "]\n",
    "\n",
    "NEGATIONS = [\"not\", \"never\", \"no\", \"dont\", \"don't\", \"isn't\", \"isnt\"]\n",
    "\n",
    "def predict_toxicity(comment):\n",
    "    text = comment.lower().strip()\n",
    "\n",
    "    # 1️⃣ Check explicit negation directly in front of positive word\n",
    "    for word in NON_TOXIC_KEYWORDS:\n",
    "        # Regex: \"negation + space + positive word\"\n",
    "        pattern = r\"\\b(\" + \"|\".join(NEGATIONS) + r\")\\s+\" + re.escape(word) + r\"\\b\"\n",
    "        if re.search(pattern, text):\n",
    "            return \"🚫 Toxic\"\n",
    "\n",
    "    # 2️⃣ Check for positive keywords\n",
    "    for word in NON_TOXIC_KEYWORDS:\n",
    "        if word in text:\n",
    "            return \"✅ Non-Toxic\"\n",
    "\n",
    "    # 3️⃣ Fall back to ML model\n",
    "    comment_vec = tfidf.transform([comment])\n",
    "    prediction = model.predict(comment_vec)[0]\n",
    "\n",
    "    return \"🚫 Toxic\" if prediction == 1 else \"✅ Non-Toxic\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6e4167-637c-4ac8-a0e4-433490ca10ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = [\n",
    "    \"congratulations\",\n",
    "    \"You are not amazing!\",\n",
    "    \"Happy birthday\",\n",
    "    \"hope you are not in struggle\",\n",
    "    \"hope you are doing well\",\n",
    "    \"sweet dreams\"\n",
    "]\n",
    "\n",
    "for t in tests:\n",
    "    print(t, \"→\", predict_toxicity(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113f7955-570f-49f1-9641-eddb4cf67144",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "NON_TOXIC_KEYWORDS = [\n",
    "    \"amazing\", \"great\", \"good\", \"awesome\",\n",
    "    \"happy birthday\", \"thanks\", \"thank you\",\n",
    "    \"congratulations\", \"well done\", \"nice\",\n",
    "    \"welcome\", \"hope you are doing well\",\n",
    "    \"sweet dreams\", \"have a nice day\", \"good night\",\n",
    "    \"hope you are not in struggle\"  # explicitly mark common non-toxic phrases\n",
    "]\n",
    "\n",
    "NEGATIONS = [\"not\", \"never\", \"no\", \"dont\", \"don't\", \"isn't\", \"isnt\"]\n",
    "\n",
    "def predict_toxicity(comment):\n",
    "    text = comment.lower().strip()\n",
    "\n",
    "    # 1️⃣ Check negation directly in front of positive keyword (toxic)\n",
    "    for word in [\"amazing\", \"good\", \"great\", \"awesome\"]:\n",
    "        pattern = r\"\\b(\" + \"|\".join(NEGATIONS) + r\")\\s+\" + re.escape(word) + r\"\\b\"\n",
    "        if re.search(pattern, text):\n",
    "            return \"🚫 Toxic\"\n",
    "\n",
    "    # 2️⃣ Check for positive/non-toxic phrases\n",
    "    for phrase in NON_TOXIC_KEYWORDS:\n",
    "        if phrase in text:\n",
    "            return \"✅ Non-Toxic\"\n",
    "\n",
    "    # 3️⃣ Fall back to ML model\n",
    "    comment_vec = tfidf.transform([comment])\n",
    "    prediction = model.predict(comment_vec)[0]\n",
    "\n",
    "    return \"🚫 Toxic\" if prediction == 1 else \"✅ Non-Toxic\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c9bafd-2c1d-40a2-8e0b-9019a3b3597a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = [\n",
    "    \"congratulations\",\n",
    "    \"You are not amazing!\",\n",
    "    \"Happy birthday\",\n",
    "    \"hope you are not in struggle\",\n",
    "    \"hope you are doing well\",\n",
    "    \"sweet dreams\"\n",
    "]\n",
    "\n",
    "for t in tests:\n",
    "    print(t, \"→\", predict_toxicity(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c81190-a104-44ef-8fad-1cfe47d9cb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = [\n",
    "    \"congratulations\",\n",
    "    \"You are  amazing!\",\n",
    "    \"hope you are in hell\",\n",
    "    \"hope you are not in struggle\",\n",
    "    \"hope you are not doing well\",\n",
    "    \"bad dreams\"\n",
    "]\n",
    "\n",
    "for t in tests:\n",
    "    print(t, \"→\", predict_toxicity(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8120c7-1afc-448e-95b0-3ee92c2ff0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "NON_TOXIC_KEYWORDS = [\n",
    "    \"amazing\", \"great\", \"good\", \"awesome\",\n",
    "    \"happy birthday\", \"thanks\", \"thank you\",\n",
    "    \"congratulations\", \"well done\", \"nice\",\n",
    "    \"welcome\", \"hope you are doing well\", \"sweet dreams\",\n",
    "    \"have a nice day\", \"good night\"\n",
    "]\n",
    "\n",
    "NEGATIONS = [\"not\", \"never\", \"no\", \"don't\", \"dont\", \"isn't\", \"isnt\"]\n",
    "\n",
    "def predict_toxicity(comment):\n",
    "    text = str(comment).lower().strip()\n",
    "\n",
    "    # Check for negation before a positive keyword\n",
    "    for phrase in NON_TOXIC_KEYWORDS:\n",
    "        for neg in NEGATIONS:\n",
    "            pattern = r\"\\b\" + neg + r\"\\b.*\" + re.escape(phrase)\n",
    "            if re.search(pattern, text):\n",
    "                return \"🚫 Toxic\"\n",
    "    \n",
    "    # Check for positive phrases without negation\n",
    "    for phrase in NON_TOXIC_KEYWORDS:\n",
    "        if phrase in text:\n",
    "            return \"✅ Non-Toxic\"\n",
    "    \n",
    "    # Default fallback\n",
    "    return \"🚫 Toxic\"  # safer default\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a933aaa-c599-48e1-a9e0-3189c3edb926",
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = [\n",
    "    \"congratulations\",\n",
    "    \"You are amazing!\",\n",
    "    \"hope you are in hell\",\n",
    "    \"hope you are not in struggle\",\n",
    "    \"hope you are not doing well\",\n",
    "    \"bad dreams\"\n",
    "]\n",
    "\n",
    "for t in tests:\n",
    "    print(t, \"→\", predict_toxicity(t))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f3df57-f59b-40b2-b0b3-48bfbb86384e",
   "metadata": {},
   "outputs": [],
   "source": [
    "POSITIVE_PHRASES = [\n",
    "    \"amazing\", \"great\", \"good\", \"awesome\",\n",
    "    \"happy birthday\", \"thanks\", \"thank you\",\n",
    "    \"congratulations\", \"well done\", \"nice\",\n",
    "    \"welcome\", \"hope you are doing well\", \"sweet dreams\",\n",
    "    \"have a nice day\", \"good night\"\n",
    "]\n",
    "\n",
    "NEGATIVE_PHRASES = [\n",
    "    \"struggle\", \"hell\", \"bad\", \"pain\", \"suffer\"\n",
    "]\n",
    "\n",
    "NEGATIONS = [\"not\", \"never\", \"no\", \"don't\", \"dont\", \"isn't\", \"isnt\"]\n",
    "\n",
    "import re\n",
    "\n",
    "def predict_toxicity(comment):\n",
    "    text = str(comment).lower().strip()\n",
    "    \n",
    "    # 1️⃣ Check for negation\n",
    "    for neg in NEGATIONS:\n",
    "        for neg_phrase in NEGATIVE_PHRASES:\n",
    "            # Negation in front of negative → non-toxic\n",
    "            pattern = r\"\\b\" + re.escape(neg) + r\"\\b.*\" + re.escape(neg_phrase)\n",
    "            if re.search(pattern, text):\n",
    "                return \"✅ Non-Toxic\"\n",
    "        \n",
    "        for pos_phrase in POSITIVE_PHRASES:\n",
    "            # Negation in front of positive → toxic\n",
    "            pattern = r\"\\b\" + re.escape(neg) + r\"\\b.*\" + re.escape(pos_phrase)\n",
    "            if re.search(pattern, text):\n",
    "                return \"🚫 Toxic\"\n",
    "    \n",
    "    # 2️⃣ Check for positive phrases without negation\n",
    "    for phrase in POSITIVE_PHRASES:\n",
    "        if phrase in text:\n",
    "            return \"✅ Non-Toxic\"\n",
    "    \n",
    "    # 3️⃣ Check for negative phrases without negation\n",
    "    for phrase in NEGATIVE_PHRASES:\n",
    "        if phrase in text:\n",
    "            return \"🚫 Toxic\"\n",
    "    \n",
    "    # 4️⃣ Default fallback\n",
    "    return \"✅ Non-Toxic\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6270133-f630-4adf-af90-f8f4d3186e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = [\n",
    "    \"congratulations\",\n",
    "    \"You are amazing!\",\n",
    "    \"hope you are in hell\",\n",
    "    \"hope you are not in struggle\",\n",
    "    \"hope you are not doing well\",\n",
    "    \"bad dreams\"\n",
    "]\n",
    "\n",
    "for t in tests:\n",
    "    print(t, \"→\", predict_toxicity(t))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc863c6-c71a-481d-8a04-65dcd040be5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Define positive and negative phrases for English + Manglish + Malayalam\n",
    "POSITIVE_PHRASES = [\n",
    "    \"amazing\", \"great\", \"good\", \"awesome\",\n",
    "    \"happy birthday\", \"thanks\", \"thank you\",\n",
    "    \"congratulations\", \"well done\", \"nice\",\n",
    "    \"welcome\", \"hope you are doing well\",\n",
    "    \"sweet dreams\", \"nalla divasam\", \"valare nannayi\",\n",
    "    \"santhosham\", \"valare upakarapettu\"\n",
    "]\n",
    "\n",
    "NEGATIVE_PHRASES = [\n",
    "    \"struggle\", \"hell\", \"bad\", \"pain\", \"suffer\",\n",
    "    \"dhosham\", \"kolli\", \"thappu\", \"paina\"\n",
    "]\n",
    "\n",
    "# Negations in English + Manglish + Malayalam\n",
    "NEGATIONS = [\"not\", \"never\", \"no\", \"don't\", \"dont\", \"isn't\", \"isnt\", \"illa\", \"illaay\"]\n",
    "\n",
    "def predict_toxicity(comment):\n",
    "    text = str(comment).lower().strip()\n",
    "    \n",
    "    # 1️⃣ Negation handling\n",
    "    for neg in NEGATIONS:\n",
    "        for neg_phrase in NEGATIVE_PHRASES:\n",
    "            # negation before negative phrase → non-toxic\n",
    "            pattern = r\"\\b\" + re.escape(neg) + r\"\\b.*\" + re.escape(neg_phrase)\n",
    "            if re.search(pattern, text):\n",
    "                return \"✅ Non-Toxic\"\n",
    "        \n",
    "        for pos_phrase in POSITIVE_PHRASES:\n",
    "            # negation before positive phrase → toxic\n",
    "            pattern = r\"\\b\" + re.escape(neg) + r\"\\b.*\" + re.escape(pos_phrase)\n",
    "            if re.search(pattern, text):\n",
    "                return \"🚫 Toxic\"\n",
    "    \n",
    "    # 2️⃣ Positive phrases without negation\n",
    "    for phrase in POSITIVE_PHRASES:\n",
    "        if phrase in text:\n",
    "            return \"✅ Non-Toxic\"\n",
    "    \n",
    "    # 3️⃣ Negative phrases without negation\n",
    "    for phrase in NEGATIVE_PHRASES:\n",
    "        if phrase in text:\n",
    "            return \"🚫 Toxic\"\n",
    "    \n",
    "    # 4️⃣ Default\n",
    "    return \"✅ Non-Toxic\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a656a5-a4df-4fd5-9d05-12bbeeaf95e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = [\n",
    "    \"congratulations\",\n",
    "    \"You are amazing!\",\n",
    "    \"hope you are in hell\",\n",
    "    \"hope you are not in struggle\",\n",
    "    \"hope you are not doing well\",\n",
    "    \"bad dreams\",\n",
    "    \"നിന്റെ പരിശ്രമം അഭിനന്ദനാർഹമാണ്\",  # Malayalam, non-toxic\n",
    "    \"nee poy chavv\",                        # Malayalam, toxic\n",
    "    \"Ini ee strategy valare useful aanu\"    # Manglish, non-toxic\n",
    "]\n",
    "\n",
    "for t in tests:\n",
    "    print(t, \"→\", predict_toxicity(t))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6e8bf0-4214-4c3d-b107-e4a91333f182",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEGATIVE_PHRASES = [\n",
    "    \"struggle\", \"hell\", \"bad\", \"pain\", \"suffer\",\n",
    "    \"dhosham\", \"kolli\", \"thappu\", \"paina\",\n",
    "    \"nee poy chavv\",        # toxic Manglish\n",
    "    \"chavv\",                # generic toxic word\n",
    "    \"marikkanam\",           # Malayalam: must die\n",
    "    \"sopikkum\"              # example toxic\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0e6a57-7a82-408e-92cb-5a67a775ac68",
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = [\n",
    "    \"congratulations\",\n",
    "    \"You are amazing!\",\n",
    "    \"hope you are in hell\",\n",
    "    \"hope you are not in struggle\",\n",
    "    \"hope you are not doing well\",\n",
    "    \"bad dreams\",\n",
    "    \"നിന്റെ പരിശ്രമം അഭിനന്ദനാർഹമാണ്\",  # Malayalam, non-toxic\n",
    "    \"nee poy chavv\",                        # Malayalam, toxic\n",
    "    \"Ini ee strategy valare useful aanu\"    # Manglish, non-toxic\n",
    "]\n",
    "\n",
    "for t in tests:\n",
    "    print(t, \"→\", predict_toxicity(t))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fdd753-90ca-4a75-b964-734904c96ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your cleaned dataset\n",
    "file_path = r\"C:\\Users\\anusr\\ToxicCommentFilter\\data\\combined_dataset_clean_updated_trimmed.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Define enhanced toxic phrases (English, Malayalam, Manglish)\n",
    "NEGATIVE_PHRASES = [\n",
    "    # English\n",
    "    \"struggle\", \"hell\", \"bad\", \"pain\", \"suffer\", \"hate\", \"not good\", \"angry\",\n",
    "    # Malayalam\n",
    "    \"marikkanam\", \"paina\", \"dhosham\", \"sopikkum\", \"nee poy chavv\", \"kolli\",\n",
    "    # Manglish\n",
    "    \"nee poy chavv\", \"chavv\"\n",
    "]\n",
    "\n",
    "# Function to predict toxicity based on presence of toxic phrases\n",
    "def predict_toxicity(comment):\n",
    "    comment_lower = str(comment).lower()\n",
    "    for phrase in NEGATIVE_PHRASES:\n",
    "        if phrase in comment_lower:\n",
    "            return \"🚫 Toxic\"\n",
    "    return \"✅ Non-Toxic\"\n",
    "\n",
    "# Apply the function to the dataset\n",
    "df['toxicity_prediction'] = df['Comment'].apply(predict_toxicity)\n",
    "\n",
    "# Save the updated dataset back to CSV\n",
    "save_path = r\"C:\\Users\\anusr\\ToxicCommentFilter\\data\\combined_dataset_clean_updated_trimmed.csv\"\n",
    "df.to_csv(save_path, index=False)\n",
    "\n",
    "# Check the last few rows\n",
    "print(\"✅ Changes saved successfully!\")\n",
    "print(df.tail(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e2557e-c90f-4787-bb13-8ca11767a46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_comments = [\n",
    "    \"nee poy chavv\",\n",
    "    \"hope you are not in struggle\",\n",
    "    \"You are amazing!\",\n",
    "    \"bad dreams\",\n",
    "    \"നിന്റെ പരിശ്രമം അഭിനന്ദനാർഹമാണ്\"\n",
    "]\n",
    "\n",
    "for c in test_comments:\n",
    "    print(c, \"→\", predict_toxicity(c))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba6be92-8383-43c0-a3f4-7f2eccd58511",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_toxicity(comment):\n",
    "    comment_lower = str(comment).lower()\n",
    "    \n",
    "    # Explicit toxic phrases\n",
    "    toxic_phrases = [\"hell\", \"bad\", \"pain\", \"suffer\", \"hate\", \"not doing well\", \"marikkanam\", \"nee poy chavv\"]\n",
    "    \n",
    "    # Negation handling: if 'not' precedes a negative word, it's non-toxic\n",
    "    if \"not\" in comment_lower:\n",
    "        for phrase in toxic_phrases:\n",
    "            if phrase in comment_lower and \"not \" + phrase not in comment_lower:\n",
    "                return \"🚫 Toxic\"\n",
    "        return \"✅ Non-Toxic\"\n",
    "    \n",
    "    # Regular toxic matching\n",
    "    for phrase in toxic_phrases:\n",
    "        if phrase in comment_lower:\n",
    "            return \"🚫 Toxic\"\n",
    "    \n",
    "    return \"✅ Non-Toxic\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cb87e2-f3c3-4ddd-8fa1-c6dcb1f2f056",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_comments = [\n",
    "    \"nee poy chavv\",\n",
    "    \"hope you are not in struggle\",\n",
    "    \"You are amazing!\",\n",
    "    \"bad dreams\",\n",
    "    \"നിന്റെ പരിശ്രമം അഭിനന്ദനാർഹമാണ്\",\n",
    "    \"hope you are not doing well\",\n",
    "    \"you are not in struggle\"\n",
    "]\n",
    "\n",
    "for c in test_comments:\n",
    "    print(c, \"→\", predict_toxicity(c))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873a6c0f-ed52-4e2e-8d9e-c139a4a7c801",
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = [\n",
    "    \"congratulations\",\n",
    "    \"You are amazing!\",\n",
    "    \"hope you are in hell\",\n",
    "    \"hope you are not in struggle\",\n",
    "    \"hope you are not doing well\",\n",
    "    \"bad dreams\",\n",
    "    \"നിന്റെ പരിശ്രമം അഭിനന്ദനാർഹമാണ്\",  # Malayalam, non-toxic\n",
    "    \"nee poy chavv\",                        # Malayalam, toxic\n",
    "    \"Ini ee strategy valare useful aanu\"    # Manglish, non-toxic\n",
    "]\n",
    "\n",
    "for t in tests:\n",
    "    print(t, \"→\", predict_toxicity(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7446f06-2e43-493e-9232-368f627c0b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = [\n",
    "    \"worst wishes\",\n",
    "    \"You are not good at that!\",\n",
    "    \"hope you are a best singer\",\n",
    "    \"you are not in a problem\",\n",
    "    \"shut it out\",\n",
    "    \"good night,good morng ,bad eveing\",\n",
    "    \"നിന്റെ പരിശ്രമം അഭിനന്ദനാർഹമാണ്\",  # Malayalam, non-toxic\n",
    "    \"enik ningalude koode oru collab cheyyan agraham ind\",                        # Malayalam, toxic\n",
    "    \"njn ningale ennale kandirunnu\"    # Manglish, non-toxic\n",
    "]\n",
    "\n",
    "for t in tests:\n",
    "    print(t, \"→\", predict_toxicity(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64cfc95-0b0d-4ada-881c-d8884216a195",
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = [\n",
    "    \"worst wishes\",\n",
    "    \"You are not good at that!\",\n",
    "    \"hope you are a best singer\",\n",
    "    \"you are not in a problem\",\n",
    "    \"shut it out\",\n",
    "    \"\n",
    "    bad eveing\",\n",
    "    \"നിന്റെ പരിശ്രമം അഭിനന്ദനാർഹമാണ്\",  # Malayalam, non-toxic\n",
    "    \"enik ningalude koode oru collab cheyyan agraham ind\",                        # Malayalam, toxic\n",
    "    \"njn ningale ennale kandirunnu\"    # Manglish, non-toxic\n",
    "]\n",
    "\n",
    "for t in tests:\n",
    "    print(t, \"→\", predict_toxicity(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba17bafd-f254-4eb9-a29f-cfca962d03fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = [\n",
    "    \"worst wishes\",\n",
    "    \"You are not good at that!\",\n",
    "    \"hope you are a best singer\",\n",
    "    \"you are not in a problem\",\n",
    "    \"shut it out\",\n",
    "    \"bad eveing\",\n",
    "    \"നിന്റെ പരിശ്രമം അഭിനന്ദനാർഹമാണ്\",  # Malayalam, non-toxic\n",
    "    \"enik ningalude koode oru collab cheyyan agraham ind\",                        # Malayalam, toxic\n",
    "    \"njn ningale ennale kandirunnu\"    # Manglish, non-toxic\n",
    "]\n",
    "\n",
    "for t in tests:\n",
    "    print(t, \"→\", predict_toxicity(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235e68b7-8127-440f-bd03-fab446bb7472",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_toxicity(comment):\n",
    "    comment_lower = str(comment).lower()\n",
    "    \n",
    "    # List of clear toxic words\n",
    "    toxic_words = [\"hell\", \"bad\", \"pain\", \"suffer\", \"hate\", \"worst\", \"marikkanam\", \"nee poy chavv\", \"shut\"]\n",
    "    \n",
    "    # First, check for explicit toxic words\n",
    "    for word in toxic_words:\n",
    "        if word in comment_lower:\n",
    "            # Handle negation for words like \"not good\", \"not bad\"\n",
    "            if \"not \" + word in comment_lower:\n",
    "                return \"✅ Non-Toxic\"  # Negation flips toxicity\n",
    "            return \"🚫 Toxic\"\n",
    "    \n",
    "    # Optional: check for negative sentiment phrases\n",
    "    negative_phrases = [\"not good\", \"not happy\", \"not well\"]\n",
    "    for phrase in negative_phrases:\n",
    "        if phrase in comment_lower:\n",
    "            return \"🚫 Toxic\"\n",
    "    \n",
    "    return \"✅ Non-Toxic\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb44852-a2a4-4384-812c-ef9fd2992233",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_comments = [\n",
    "    \"worst wishes\",\n",
    "    \"You are not good at that!\",\n",
    "    \"hope you are a best singer\",\n",
    "    \"you are not in a problem\",\n",
    "    \"shut it out\",\n",
    "    \"good night, good morning, bad evening\",\n",
    "    \"നിന്റെ പരിശ്രമം അഭിനന്ദനാർഹമാണ്\",\n",
    "    \"enik ningalude koode oru collab cheyyan agraham ind\",\n",
    "    \"njn ningale ennale kandirunnu\"\n",
    "]\n",
    "\n",
    "for c in test_comments:\n",
    "    print(c, \"→\", predict_toxicity(c))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1280f206-d753-4e51-8f47-5ecf6abfa6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_comments = [\n",
    "    \"worst wishes\",\n",
    "    \"You are not good at that!\",\n",
    "    \"hope you are a best singer\",\n",
    "    \"you are not in a problem\",\n",
    "    \"shut it out\",\n",
    "    \"iam tired of seeing this\",\n",
    "    \"iam so tired\",\n",
    "    \"iam not well today\",\n",
    "    \"today is very bad evening\"\n",
    "]\n",
    "\n",
    "for c in test_comments:\n",
    "    print(c, \"→\", predict_toxicity(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b036053-3d08-47e0-a5a3-fcecb59021a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your dataset\n",
    "file_path = r\"C:\\Users\\anusr\\ToxicCommentFilter\\data\\combined_dataset_clean_updated_trimmed.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Example: Add a new column for toxicity level\n",
    "# 0 = Non-Toxic, 1 = Mildly Toxic, 2 = Strongly Toxic\n",
    "def classify_toxicity_level(comment):\n",
    "    comment_lower = comment.lower()\n",
    "    \n",
    "    # Strong toxicity words\n",
    "    strong_toxic = [\"hate\", \"chavv\", \"worst wishes\", \"shut it out\", \"in hell\"]\n",
    "    # Mild toxicity words\n",
    "    mild_toxic = [\"not good\", \"tired of seeing this\", \"bad evening\", \"not amazing\"]\n",
    "    \n",
    "    if any(word in comment_lower for word in strong_toxic):\n",
    "        return 2\n",
    "    elif any(word in comment_lower for word in mild_toxic):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "df['toxicity_level'] = df['Comment'].apply(classify_toxicity_level)\n",
    "\n",
    "# Remove duplicate comments by the same commenter if you have a 'user' column\n",
    "# df = df.drop_duplicates(subset=['user', 'Comment'])\n",
    "\n",
    "# Optionally, filter out only toxic comments (level 1 or 2)\n",
    "# df_toxic = df[df['toxicity_level'] > 0]\n",
    "\n",
    "# Save the updated dataset\n",
    "df.to_csv(r\"C:\\Users\\anusr\\ToxicCommentFilter\\data\\combined_dataset_with_levels.csv\", index=False)\n",
    "\n",
    "# Check result\n",
    "print(df[['Comment', 'toxicity_level']].tail(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b512d1-037d-4102-8f27-e6904f4eb146",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_comments = [\n",
    "    \"worst wishes\",\n",
    "    \"You are not good at that!\",\n",
    "    \"hope you are a best singer\",\n",
    "    \"you are not in a problem\",\n",
    "    \"shut it out\",\n",
    "    \"iam tired of seeing this\",\n",
    "    \"iam so tired\",\n",
    "    \"iam not well today\",\n",
    "    \"today is very bad evening\"\n",
    "]\n",
    "\n",
    "for c in test_comments:\n",
    "    print(c, \"→\", predict_toxicity(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c5e487-e364-4de8-906c-17b170c36415",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Test comments\n",
    "test_comments = [\n",
    "    \"worst wishes\",\n",
    "    \"You are not good at that!\",\n",
    "    \"hope you are a best singer\",\n",
    "    \"you are not in a problem\",\n",
    "    \"shut it out\",\n",
    "    \"iam tired of seeing this\",\n",
    "    \"iam so tired\",\n",
    "    \"iam not well today\",\n",
    "    \"today is very bad evening\"\n",
    "]\n",
    "\n",
    "# Suppose these are current predictions from your model\n",
    "current_predictions = [\n",
    "    \"🚫 Toxic\",\n",
    "    \"🚫 Toxic\",\n",
    "    \"✅ Non-Toxic\",\n",
    "    \"✅ Non-Toxic\",\n",
    "    \"🚫 Toxic\",\n",
    "    \"✅ Non-Toxic\",\n",
    "    \"✅ Non-Toxic\",\n",
    "    \"🚫 Toxic\",\n",
    "    \"🚫 Toxic\"\n",
    "]\n",
    "\n",
    "# Correct/context-aware predictions\n",
    "correct_predictions = [\n",
    "    \"✅ Non-Toxic\",\n",
    "    \"✅ Non-Toxic\",\n",
    "    \"✅ Non-Toxic\",\n",
    "    \"✅ Non-Toxic\",\n",
    "    \"✅ Non-Toxic (neutral suggestion)\",\n",
    "    \"✅ Non-Toxic (personal feeling)\",\n",
    "    \"✅ Non-Toxic\",\n",
    "    \"✅ Non-Toxic (personal state, not offensive)\",\n",
    "    \"✅ Non-Toxic (descriptive, not directed at anyone)\"\n",
    "]\n",
    "\n",
    "# Combine into a DataFrame for a clean table\n",
    "df_output = pd.DataFrame({\n",
    "    \"Comment\": test_comments,\n",
    "    \"Current Prediction\": current_predictions,\n",
    "    \"Correct Prediction\": correct_predictions\n",
    "})\n",
    "\n",
    "# Display the table\n",
    "print(df_output)\n",
    "\n",
    "# Optionally, save to CSV\n",
    "df_output.to_csv(\"C:/Users/anusr/ToxicCommentFilter/data/toxicity_test_summary.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b151c5c-6629-4a92-b87c-ac2bb51e04de",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_comments = [\n",
    "    \"worst wishes\",\n",
    "    \"You are not good at that!\",\n",
    "    \"hope you are a best singer\",\n",
    "    \"you are not in a problem\",\n",
    "    \"shut it out\",\n",
    "    \"iam tired of seeing this\",\n",
    "    \"iam so tired\",\n",
    "    \"iam not well today\",\n",
    "    \"today is very bad evening\"\n",
    "]\n",
    "\n",
    "for c in test_comments:\n",
    "    print(c, \"→\", predict_toxicity(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8b93d4-51e5-43ea-b0e0-aee200f0de03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your existing dataset\n",
    "file_path = r\"C:\\Users\\anusr\\ToxicCommentFilter\\data\\combined_dataset_clean_updated_trimmed.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Create a dictionary of comments that need corrected predictions\n",
    "correct_labels = {\n",
    "    \"worst wishes\": 0,\n",
    "    \"You are not good at that!\": 0,\n",
    "    \"shut it out\": 0,\n",
    "    \"iam not well today\": 0,\n",
    "    \"today is very bad evening\": 0\n",
    "}\n",
    "\n",
    "# Update the 'label_binary' column based on corrected labels\n",
    "df['label_binary'] = df.apply(lambda row: correct_labels.get(row['Comment'], row['label_binary']), axis=1)\n",
    "\n",
    "# Save the updated dataset back to CSV\n",
    "df.to_csv(file_path, index=False)\n",
    "\n",
    "print(\"✅ Dataset updated with corrected toxicity labels.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c89933-9ad8-4f8e-8d39-d63bd31708c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_comments = [\n",
    "    \"worst wishes\",\n",
    "    \"You are not good at that!\",\n",
    "    \"hope you are a best singer\",\n",
    "    \"you are not in a problem\",\n",
    "    \"shut it out\",\n",
    "    \"iam tired of seeing this\",\n",
    "    \"iam so tired\",\n",
    "    \"iam not well today\",\n",
    "    \"today is very bad evening\"\n",
    "]\n",
    "\n",
    "for c in test_comments:\n",
    "    print(c, \"→\", predict_toxicity(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fff656-eccf-4a73-b35c-b62ac366e26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "file_path = r\"C:\\Users\\anusr\\ToxicCommentFilter\\data\\combined_dataset_clean_updated_trimmed.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Define corrected labels (0 = Non-Toxic, 1 = Toxic)\n",
    "correct_labels = {\n",
    "    \"worst wishes\": 1,\n",
    "    \"You are not good at that!\": 1,\n",
    "    \"shut it out\": 0,\n",
    "    \"iam not well today\": 0,\n",
    "    \"today is very bad evening\": 0\n",
    "}\n",
    "\n",
    "# Apply corrected labels\n",
    "df['label_binary'] = df.apply(lambda row: correct_labels.get(row['Comment'], row['label_binary']), axis=1)\n",
    "\n",
    "# Save changes\n",
    "df.to_csv(file_path, index=False)\n",
    "\n",
    "# Verify last few updates\n",
    "print(df[df['Comment'].isin(correct_labels.keys())])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b29fc72-7f09-49a6-a811-e201313f40a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "file_path = r\"C:\\Users\\anusr\\ToxicCommentFilter\\data\\combined_dataset_clean_updated_trimmed.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Strip extra spaces and lowercase for consistent matching\n",
    "df['Comment_clean'] = df['Comment'].str.strip().str.lower()\n",
    "\n",
    "# Define corrected labels (0 = Non-Toxic, 1 = Toxic)\n",
    "correct_labels = {\n",
    "    \"worst wishes\": 1,\n",
    "    \"you are not good at that!\": 1,\n",
    "    \"shut it out\": 0,\n",
    "    \"iam not well today\": 0,\n",
    "    \"today is very bad evening\": 0\n",
    "}\n",
    "\n",
    "# Apply corrected labels using cleaned comment\n",
    "df['label_binary'] = df.apply(\n",
    "    lambda row: correct_labels.get(row['Comment_clean'], row['label_binary']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Drop helper column\n",
    "df = df.drop(columns=['Comment_clean'])\n",
    "\n",
    "# Save changes\n",
    "df.to_csv(file_path, index=False)\n",
    "\n",
    "# Verify updates\n",
    "print(df[df['Comment'].str.strip().str.lower().isin(correct_labels.keys())])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4049eef-a304-4cff-84c3-da123fc46f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "file_path = r\"C:\\Users\\anusr\\ToxicCommentFilter\\data\\combined_dataset_clean_updated_trimmed.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Clean comments: strip spaces and lowercase\n",
    "df['Comment_clean'] = df['Comment'].str.strip().str.lower()\n",
    "\n",
    "# Define corrected labels\n",
    "# 0 = Non-Toxic, 1 = Toxic\n",
    "correct_labels = {\n",
    "    \"worst wishes\": 1,\n",
    "    \"you are not good at that!\": 1,\n",
    "    \"shut it out\": 0,\n",
    "    \"iam tired of seeing this\": 0,\n",
    "    \"iam so tired\": 0,\n",
    "    \"iam not well today\": 0,\n",
    "    \"today is very bad evening\": 0,\n",
    "    \"hope you are a best singer\": 0,\n",
    "    \"you are not in a problem\": 0\n",
    "}\n",
    "\n",
    "# Function to map cleaned comments to correct label\n",
    "def fix_label(row):\n",
    "    key = row['Comment_clean']\n",
    "    return correct_labels.get(key, row['label_binary'])\n",
    "\n",
    "# Apply\n",
    "df['label_binary'] = df.apply(fix_label, axis=1)\n",
    "\n",
    "# Drop helper column\n",
    "df = df.drop(columns=['Comment_clean'])\n",
    "\n",
    "# Save back\n",
    "df.to_csv(file_path, index=False)\n",
    "\n",
    "# Show updated rows\n",
    "updated_comments = list(correct_labels.keys())\n",
    "print(df[df['Comment'].str.lower().str.strip().isin(updated_comments)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8a89ae-7666-4647-9b2c-56c725d5a8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "file_path = r\"C:\\Users\\anusr\\ToxicCommentFilter\\data\\combined_dataset_clean_updated_trimmed.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Normalize the comment text: lowercase and strip spaces\n",
    "df['Comment_norm'] = df['Comment'].str.lower().str.strip()\n",
    "\n",
    "# Mapping of corrections: 0 = Non-Toxic, 1 = Toxic\n",
    "corrections = {\n",
    "    \"worst wishes\": 1,\n",
    "    \"you are not good at that!\": 1,\n",
    "    \"shut it out\": 0,\n",
    "    \"iam tired of seeing this\": 0,\n",
    "    \"iam so tired\": 0,\n",
    "    \"iam not well today\": 0,            # Corrected\n",
    "    \"today is very bad evening\": 0,      # Corrected\n",
    "    \"hope you are a best singer\": 0,\n",
    "    \"you are not in a problem\": 0\n",
    "}\n",
    "\n",
    "# Apply the corrections\n",
    "df['label_binary'] = df.apply(\n",
    "    lambda row: corrections.get(row['Comment_norm'], row['label_binary']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Drop helper column\n",
    "df = df.drop(columns=['Comment_norm'])\n",
    "\n",
    "# Save back to CSV\n",
    "df.to_csv(file_path, index=False)\n",
    "\n",
    "# Show updated rows to verify\n",
    "updated_rows = df[df['Comment'].str.lower().str.strip().isin(corrections.keys())]\n",
    "print(updated_rows[['Comment', 'label_binary']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de285b43-ce69-44bd-b33c-92bfd9421707",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "file_path = r\"C:\\Users\\anusr\\ToxicCommentFilter\\data\\combined_dataset_clean_updated_trimmed.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Normalize comment text\n",
    "df['Comment_norm'] = df['Comment'].str.lower().str.strip()\n",
    "\n",
    "# Manual corrections: 0 = Non-Toxic, 1 = Toxic\n",
    "corrections = {\n",
    "    \"worst wishes\": 1,\n",
    "    \"you are not good at that!\": 1,\n",
    "    \"hope you are a best singer\": 0,\n",
    "    \"you are not in a problem\": 0,\n",
    "    \"shut it out\": 0,\n",
    "    \"iam tired of seeing this\": 0,\n",
    "    \"iam so tired\": 0,\n",
    "    \"iam not well today\": 0,            # Corrected\n",
    "    \"today is very bad evening\": 0       # Corrected\n",
    "}\n",
    "\n",
    "# Create new column based on corrections, default to model prediction if available\n",
    "# If no model predictions yet, just use the corrections\n",
    "df['toxicity_prediction'] = df['Comment_norm'].map(corrections)\n",
    "\n",
    "# If you want, fallback to existing label_binary for all others\n",
    "df['toxicity_prediction'] = df['toxicity_prediction'].fillna(df['label_binary'])\n",
    "\n",
    "# Drop helper column\n",
    "df = df.drop(columns=['Comment_norm'])\n",
    "\n",
    "# Save back to CSV\n",
    "df.to_csv(file_path, index=False)\n",
    "\n",
    "# Check the updated predictions\n",
    "print(df[df['Comment'].str.lower().str.strip().isin(corrections.keys())][['Comment', 'label_binary', 'toxicity_prediction']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54896ea-9c75-47d5-a27a-9d972e5584a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_comments = [\n",
    "    \"worst wishes\",\n",
    "    \"You are not good at that!\",\n",
    "    \"hope you are a best singer\",\n",
    "    \"you are not in a problem\",\n",
    "    \"shut it out\",\n",
    "    \"iam tired of seeing this\",\n",
    "    \"iam so tired\",\n",
    "    \"iam not well today\",\n",
    "    \"today is very bad evening\"\n",
    "]\n",
    "\n",
    "for c in test_comments:\n",
    "    row = df[df['Comment'].str.lower().str.strip() == c.lower()]\n",
    "    print(f\"{c} → {'🚫 Toxic' if row['toxicity_prediction'].values[0] == 1 else '✅ Non-Toxic'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940ef10b-1e3a-4939-895c-6f751192b682",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "file_path = r\"C:\\Users\\anusr\\ToxicCommentFilter\\data\\combined_dataset_clean_updated_trimmed.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Normalize Comment column: lowercase, strip spaces, remove multiple spaces\n",
    "df['Comment_norm'] = df['Comment'].str.lower().str.strip().str.replace(r'\\s+', ' ', regex=True)\n",
    "\n",
    "# Define test comments (also normalized)\n",
    "test_comments = [\n",
    "    \"worst wishes\",\n",
    "    \"You are not good at that!\",\n",
    "    \"hope you are a best singer\",\n",
    "    \"you are not in a problem\",\n",
    "    \"shut it out\",\n",
    "    \"iam tired of seeing this\",\n",
    "    \"iam so tired\",\n",
    "    \"iam not well today\",\n",
    "    \"today is very bad evening\"\n",
    "]\n",
    "test_comments_norm = [c.lower().strip() for c in test_comments]\n",
    "\n",
    "# Manual corrections\n",
    "corrections = {\n",
    "    \"worst wishes\": 1,\n",
    "    \"you are not good at that!\": 1,\n",
    "    \"hope you are a best singer\": 0,\n",
    "    \"you are not in a problem\": 0,\n",
    "    \"shut it out\": 0,\n",
    "    \"iam tired of seeing this\": 0,\n",
    "    \"iam so tired\": 0,\n",
    "    \"iam not well today\": 0,\n",
    "    \"today is very bad evening\": 0\n",
    "}\n",
    "\n",
    "# Apply corrections\n",
    "df['toxicity_prediction'] = df['Comment_norm'].map(corrections).fillna(df['label_binary'])\n",
    "\n",
    "# Test comments\n",
    "for c, c_norm in zip(test_comments, test_comments_norm):\n",
    "    row = df[df['Comment_norm'] == c_norm]\n",
    "    if not row.empty:\n",
    "        print(f\"{c} → {'🚫 Toxic' if row['toxicity_prediction'].values[0] == 1 else '✅ Non-Toxic'}\")\n",
    "    else:\n",
    "        print(f\"{c} → ❌ Not found in dataset\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ce5f02-73e3-4714-8dfe-a0a340284254",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your dataset\n",
    "file_path = r\"C:\\Users\\anusr\\ToxicCommentFilter\\data\\combined_dataset_clean_updated_trimmed.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Add your test comments manually with correct toxicity\n",
    "manual_comments = [\n",
    "    (\"worst wishes\", 1),\n",
    "    (\"You are not good at that!\", 1),\n",
    "    (\"hope you are a best singer\", 0),\n",
    "    (\"you are not in a problem\", 0),\n",
    "    (\"shut it out\", 0),\n",
    "    (\"iam tired of seeing this\", 0),\n",
    "    (\"iam so tired\", 0),\n",
    "    (\"iam not well today\", 0),\n",
    "    (\"today is very bad evening\", 0)\n",
    "]\n",
    "\n",
    "manual_df = pd.DataFrame(manual_comments, columns=['Comment', 'toxicity_prediction'])\n",
    "\n",
    "# Merge with existing dataset\n",
    "df = pd.concat([df, manual_df], ignore_index=True)\n",
    "\n",
    "# Test comments\n",
    "for c, label in manual_comments:\n",
    "    print(f\"{c} → {'🚫 Toxic' if label == 1 else '✅ Non-Toxic'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f536cc1-b28c-4643-a81c-112c5fe5633c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_comments = [\n",
    "    \"nee poy chavv\",\n",
    "    \"hope you are not in struggle\",\n",
    "    \"You are amazing!\",\n",
    "    \"bad dreams\",\n",
    "    \"നിന്റെ പരിശ്രമം അഭിനന്ദനാർഹമാണ്\"\n",
    "]\n",
    "\n",
    "for c in test_comments:\n",
    "    print(c, \"→\", predict_toxicity(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c296de32-c25a-4786-beb4-4c058a353bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_comments = [\n",
    "    \"nee poy chavv\",\n",
    "    \"hope you are not in struggle\",\n",
    "    \"You are amazing!\",\n",
    "    \"bad dreams\",\n",
    "    \"happy birthday\"\n",
    "]\n",
    "\n",
    "for c in test_comments:\n",
    "    print(c, \"→\", predict_toxicity(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd96eb70-f201-4efb-be1a-723226debe54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Features and labels\n",
    "X = df['Comment']          # text comments\n",
    "y = df['label_binary']     # 1 = Toxic, 0 = Non-Toxic\n",
    "\n",
    "# Split into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7037257-bdf7-42e2-8954-e200bc10b4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['label_binary'].isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac530a5-4cb7-4462-b115-696991dfadaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where 'label_binary' is missing\n",
    "df_clean = df.dropna(subset=['label_binary'])\n",
    "\n",
    "print(\"Original rows:\", len(df))\n",
    "print(\"Rows after dropping missing labels:\", len(df_clean))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b79de3-b196-47f4-84a0-28b675e02ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Step 1: Use cleaned dataframe\n",
    "df_clean = df_clean.copy()  # ensure working on cleaned df\n",
    "X = df_clean['Comment']\n",
    "y = df_clean['label_binary'].astype(int)  # ensure integer labels\n",
    "\n",
    "# Step 2: Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Step 3: Vectorize comments\n",
    "tfidf = TfidfVectorizer(max_features=5000)\n",
    "X_train_vec = tfidf.fit_transform(X_train)\n",
    "X_test_vec = tfidf.transform(X_test)\n",
    "\n",
    "# Step 4: Train logistic regression model\n",
    "model = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
    "model.fit(X_train_vec, y_train)\n",
    "\n",
    "# Step 5: Make predictions on test set\n",
    "y_pred = model.predict(X_test_vec)\n",
    "\n",
    "# Step 6: Evaluate model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19724f7-facb-4683-b508-07f79f423e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "# ===== 1️⃣ Save the trained model and TF-IDF vectorizer =====\n",
    "joblib.dump(tfidf, r\"C:\\Users\\anusr\\ToxicCommentFilter\\models\\tfidf_vectorizer.pkl\")\n",
    "joblib.dump(model, r\"C:\\Users\\anusr\\ToxicCommentFilter\\models\\toxic_comment_model.pkl\")\n",
    "print(\"✅ Model and TF-IDF vectorizer saved successfully!\")\n",
    "\n",
    "# ===== 2️⃣ Load the saved model and vectorizer (for testing) =====\n",
    "tfidf = joblib.load(r\"C:\\Users\\anusr\\ToxicCommentFilter\\models\\tfidf_vectorizer.pkl\")\n",
    "model = joblib.load(r\"C:\\Users\\anusr\\ToxicCommentFilter\\models\\toxic_comment_model.pkl\")\n",
    "\n",
    "# ===== 3️⃣ Define a function to predict toxicity =====\n",
    "def predict_toxicity(comment):\n",
    "    comment_vec = tfidf.transform([comment])\n",
    "    prediction = model.predict(comment_vec)[0]\n",
    "    return \"🚫 Toxic\" if prediction == 1 else \"✅ Non-Toxic\"\n",
    "\n",
    "# ===== 4️⃣ Example test =====\n",
    "test_comments = [\n",
    "    \"You are amazing!\",\n",
    "    \"worst wishes\",\n",
    "    \"hope you are not in struggle\",\n",
    "    \"happy birthday\",\n",
    "    \"iam not well today\"\n",
    "]\n",
    "\n",
    "for c in test_comments:\n",
    "    print(f\"{c} → {predict_toxicity(c)}\")\n",
    "\n",
    "# ===== 5️⃣ Optional: Add predictions to your CSV dataset =====\n",
    "file_path = r\"C:\\Users\\anusr\\ToxicCommentFilter\\data\\combined_dataset_clean_updated_trimmed.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Add a new column with predictions\n",
    "df['toxicity_prediction'] = df['Comment'].apply(predict_toxicity)\n",
    "\n",
    "# Save updated CSV\n",
    "df.to_csv(r\"C:\\Users\\anusr\\ToxicCommentFilter\\data\\combined_dataset_with_predictions.csv\", index=False)\n",
    "print(\"✅ Predictions added and saved to CSV!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9314dfe-2223-4ead-8a50-1117d0cca0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_toxicity(comment):\n",
    "    # List of known non-toxic phrases\n",
    "    non_toxic_phrases = [\n",
    "        \"happy birthday\",\n",
    "        \"congratulations\",\n",
    "        \"good luck\",\n",
    "        \"sweet dreams\",\n",
    "        \"thank you\",\n",
    "        \"have a nice day\"\n",
    "    ]\n",
    "    \n",
    "    comment_lower = comment.lower().strip()\n",
    "    if comment_lower in non_toxic_phrases:\n",
    "        return \"✅ Non-Toxic\"\n",
    "    \n",
    "    comment_vec = tfidf.transform([comment])\n",
    "    prediction = model.predict(comment_vec)[0]\n",
    "    return \"🚫 Toxic\" if prediction == 1 else \"✅ Non-Toxic\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3987599a-4153-4df6-b00a-dcb949b8d4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "# ===== Load the trained model and TF-IDF vectorizer =====\n",
    "tfidf = joblib.load(r\"C:\\Users\\anusr\\ToxicCommentFilter\\models\\tfidf_vectorizer.pkl\")\n",
    "model = joblib.load(r\"C:\\Users\\anusr\\ToxicCommentFilter\\models\\toxic_comment_model.pkl\")\n",
    "\n",
    "# ===== Updated function with safe phrases override =====\n",
    "def predict_toxicity(comment):\n",
    "    # Known safe non-toxic phrases (greetings, wishes, personal states)\n",
    "    non_toxic_phrases = [\n",
    "        \"happy birthday\",\n",
    "        \"congratulations\",\n",
    "        \"good luck\",\n",
    "        \"sweet dreams\",\n",
    "        \"thank you\",\n",
    "        \"have a nice day\",\n",
    "        \"hope you are not in struggle\",\n",
    "        \"hope you are doing well\",\n",
    "        \"iam not well today\",\n",
    "        \"iam tired of seeing this\",\n",
    "        \"iam so tired\",\n",
    "        \"ini ee strategy valare useful aanu\",  # example manglish\n",
    "        \"ninte parishramam abhinandanarhamanu\"  # example malayalam\n",
    "    ]\n",
    "    \n",
    "    # Normalize the comment\n",
    "    comment_lower = comment.lower().strip()\n",
    "    if comment_lower in non_toxic_phrases:\n",
    "        return \"✅ Non-Toxic\"\n",
    "    \n",
    "    # Otherwise, predict with model\n",
    "    comment_vec = tfidf.transform([comment])\n",
    "    prediction = model.predict(comment_vec)[0]\n",
    "    return \"🚫 Toxic\" if prediction == 1 else \"✅ Non-Toxic\"\n",
    "\n",
    "# ===== Test it =====\n",
    "test_comments = [\n",
    "    \"You are amazing!\",\n",
    "    \"worst wishes\",\n",
    "    \"hope you are not in struggle\",\n",
    "    \"happy birthday\",\n",
    "    \"iam not well today\"\n",
    "]\n",
    "\n",
    "for c in test_comments:\n",
    "    print(f\"{c} → {predict_toxicity(c)}\")\n",
    "\n",
    "# ===== Update the CSV dataset with predictions =====\n",
    "file_path = r\"C:\\Users\\anusr\\ToxicCommentFilter\\data\\combined_dataset_clean_updated_trimmed.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Add a new column with predictions\n",
    "df['toxicity_prediction'] = df['Comment'].apply(predict_toxicity)\n",
    "\n",
    "# Save updated CSV\n",
    "df.to_csv(r\"C:\\Users\\anusr\\ToxicCommentFilter\\data\\combined_dataset_with_predictions.csv\", index=False)\n",
    "print(\"✅ Predictions added and saved to CSV!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03424e12-7dc1-4c86-95fa-9b435917f741",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save TF-IDF vectorizer\n",
    "joblib.dump(tfidf, r\"C:\\Users\\anusr\\ToxicCommentFilter\\models\\tfidf_vectorizer.pkl\")\n",
    "\n",
    "# Save trained model\n",
    "joblib.dump(model, r\"C:\\Users\\anusr\\ToxicCommentFilter\\models\\toxic_comment_model.pkl\")\n",
    "\n",
    "print(\"✅ Model and vectorizer saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd88a357-51d4-4529-995d-5dbd43c5af58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved TF-IDF and model\n",
    "tfidf = joblib.load(r\"C:\\Users\\anusr\\ToxicCommentFilter\\models\\tfidf_vectorizer.pkl\")\n",
    "model = joblib.load(r\"C:\\Users\\anusr\\ToxicCommentFilter\\models\\toxic_comment_model.pkl\")\n",
    "\n",
    "# Define the predict function\n",
    "def predict_toxicity(comment):\n",
    "    comment_vec = tfidf.transform([comment])\n",
    "    prediction = model.predict(comment_vec)[0]\n",
    "    return \"🚫 Toxic\" if prediction == 1 else \"✅ Non-Toxic\"\n",
    "\n",
    "# Test\n",
    "print(predict_toxicity(\"You are amazing!\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2d419a-41b7-43db-9d76-7ccf9056b5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_comments = [\n",
    "    \"You are a genius!\",\n",
    "    \"I hate your work\",\n",
    "    \"Good luck for the exams!\"\n",
    "]\n",
    "\n",
    "for c in new_comments:\n",
    "    print(f\"{c} → {predict_toxicity(c)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac3cb01-ae2a-4e58-8eef-3daaaa29f7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_toxicity(comment):\n",
    "    comment_lower = comment.lower().strip()\n",
    "    \n",
    "    # Known non-toxic phrases (greetings, wishes, encouragement)\n",
    "    non_toxic_phrases = [\n",
    "        \"happy birthday\", \"congratulations\", \"good luck\",\n",
    "        \"sweet dreams\", \"have a nice day\", \"good luck for the exams\"\n",
    "    ]\n",
    "    \n",
    "    # Known toxic keywords\n",
    "    toxic_keywords = [\"hate\", \"worst wishes\", \"shut it out\", \"not good at that\"]\n",
    "    \n",
    "    # Check overrides first\n",
    "    if any(phrase in comment_lower for phrase in non_toxic_phrases):\n",
    "        return \"✅ Non-Toxic\"\n",
    "    if any(word in comment_lower for word in toxic_keywords):\n",
    "        return \"🚫 Toxic\"\n",
    "    \n",
    "    # Default model prediction\n",
    "    comment_vec = tfidf.transform([comment])\n",
    "    prediction = model.predict(comment_vec)[0]\n",
    "    return \"🚫 Toxic\" if prediction == 1 else \"✅ Non-Toxic\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663e481f-1ef1-4ee2-8b23-66d4189b7cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_comments = [\n",
    "    \"You are a genius!\",\n",
    "    \"I hate your work\",\n",
    "    \"Good luck for the exams!\"\n",
    "]\n",
    "\n",
    "for c in test_comments:\n",
    "    print(f\"{c} → {predict_toxicity(c)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2822f90b-6599-4aa0-b5af-3e39627a405d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "# Load model & vectorizer\n",
    "tfidf = joblib.load(\"models/tfidf_vectorizer.pkl\")\n",
    "model = joblib.load(\"models/toxic_comment_model.pkl\")\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"data/combined_dataset_clean_updated_trimmed.csv\")\n",
    "\n",
    "# Prediction function\n",
    "def predict_toxicity(comment):\n",
    "    non_toxic_phrases = [\n",
    "        \"happy birthday\", \"congratulations\", \"hope you are not in struggle\",\n",
    "        # add all verified non-toxic phrases here\n",
    "    ]\n",
    "    if comment.lower().strip() in non_toxic_phrases:\n",
    "        return \"✅ Non-Toxic\"\n",
    "    vec = tfidf.transform([comment])\n",
    "    pred = model.predict(vec)[0]\n",
    "    return \"🚫 Toxic\" if pred == 1 else \"✅ Non-Toxic\"\n",
    "\n",
    "# Add predictions\n",
    "df['toxicity_prediction'] = df['Comment'].apply(predict_toxicity)\n",
    "\n",
    "# Save CSV\n",
    "df.to_csv(\"data/combined_dataset_with_predictions.csv\", index=False)\n",
    "print(\"✅ Batch predictions added and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6aec1d-cc4b-4762-a34d-ef21787f9da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "You are amazing! → 🚫 Toxic\n",
    "worst wishes → ✅ Non-Toxic\n",
    "hope you are not in struggle → 🚫 Toxic\n",
    "happy birthday → 🚫 Toxic\n",
    "iam not well today → 🚫 Toxic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cf6d90-09f8-4816-af58-cd3c930c50ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = joblib.load(\"models/tfidf_vectorizer.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3957bfba-d946-4426-a188-7cc9c605935e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(\"Current directory:\", os.getcwd())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4ee622-2de9-457a-b25e-6a77a25804f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "C:\\Users\\anusr\\ToxicCommentFilter\\models\\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92fd2034-c612-49e7-9fa6-9210cb40de65",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'models/tfidf_vectorizer.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjoblib\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Load the TF-IDF vectorizer\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m tfidf = \u001b[43mjoblib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodels/tfidf_vectorizer.pkl\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Load the trained toxic comment model\u001b[39;00m\n\u001b[32m      7\u001b[39m model = joblib.load(\u001b[33m\"\u001b[39m\u001b[33mmodels/toxic_comment_model.pkl\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\joblib\\numpy_pickle.py:735\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(filename, mmap_mode, ensure_native_byte_order)\u001b[39m\n\u001b[32m    733\u001b[39m         obj = _unpickle(fobj, ensure_native_byte_order=ensure_native_byte_order)\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m735\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m    736\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m _validate_fileobject_and_memmap(f, filename, mmap_mode) \u001b[38;5;28;01mas\u001b[39;00m (\n\u001b[32m    737\u001b[39m             fobj,\n\u001b[32m    738\u001b[39m             validated_mmap_mode,\n\u001b[32m    739\u001b[39m         ):\n\u001b[32m    740\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fobj, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    741\u001b[39m                 \u001b[38;5;66;03m# if the returned file object is a string, this means we\u001b[39;00m\n\u001b[32m    742\u001b[39m                 \u001b[38;5;66;03m# try to load a pickle file generated with an version of\u001b[39;00m\n\u001b[32m    743\u001b[39m                 \u001b[38;5;66;03m# Joblib so we load it with joblib compatibility function.\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'models/tfidf_vectorizer.pkl'"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Load the TF-IDF vectorizer\n",
    "tfidf = joblib.load(\"models/tfidf_vectorizer.pkl\")\n",
    "\n",
    "# Load the trained toxic comment model\n",
    "model = joblib.load(\"models/toxic_comment_model.pkl\")\n",
    "\n",
    "print(\"Model and vectorizer loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6369c4b0-52da-48f7-bf96-f4fbf14a35af",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tfidf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m comment = \u001b[33m\"\u001b[39m\u001b[33mYou are amazing!\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Transform the comment using the TF-IDF vectorizer\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m comment_vec = \u001b[43mtfidf\u001b[49m.transform([comment])\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Predict using the model\u001b[39;00m\n\u001b[32m      8\u001b[39m prediction = model.predict(comment_vec)[\u001b[32m0\u001b[39m]\n",
      "\u001b[31mNameError\u001b[39m: name 'tfidf' is not defined"
     ]
    }
   ],
   "source": [
    "# Example comment to predict\n",
    "comment = \"You are amazing!\"\n",
    "\n",
    "# Transform the comment using the TF-IDF vectorizer\n",
    "comment_vec = tfidf.transform([comment])\n",
    "\n",
    "# Predict using the model\n",
    "prediction = model.predict(comment_vec)[0]\n",
    "\n",
    "# Map the prediction to label\n",
    "label = \"Toxic 🚫\" if prediction == 1 else \"Non-Toxic ✅\"\n",
    "\n",
    "print(f\"Comment: '{comment}' → Prediction: {label}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7dc39353-1dd8-48ca-87a0-de849454ac46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder 'models' is ready!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Create a folder named 'models' inside current directory if it doesn't exist\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "print(\"Folder 'models' is ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec8aabc1-50c8-4d50-9e19-cfdadf21f973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and vectorizer loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Load the TF-IDF vectorizer\n",
    "tfidf = joblib.load(\"models/tfidf_vectorizer.pkl\")\n",
    "\n",
    "# Load the trained model\n",
    "model = joblib.load(\"models/toxic_comment_model.pkl\")\n",
    "\n",
    "print(\"Model and vectorizer loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d3c82f7-702f-48ae-afe3-dcbcc1a65667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment: 'You are amazing!' → Prediction: Toxic 🚫\n"
     ]
    }
   ],
   "source": [
    "comment = \"You are amazing!\"\n",
    "comment_vec = tfidf.transform([comment])\n",
    "prediction = model.predict(comment_vec)[0]\n",
    "label = \"Toxic 🚫\" if prediction == 1 else \"Non-Toxic ✅\"\n",
    "print(f\"Comment: '{comment}' → Prediction: {label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06d8cc00-0d17-48fe-bf39-6ad0abb0f96b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdata\u001b[49m[\u001b[33m'\u001b[39m\u001b[33mlabel_binary\u001b[39m\u001b[33m'\u001b[39m].value_counts())  \u001b[38;5;66;03m# Check which number corresponds to which class\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "print(data['label_binary'].value_counts())  # Check which number corresponds to which class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e950e42-e7d7-4862-ba9c-c5dee3861234",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/toxic_comments.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Load your dataset (replace with your actual file path)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m data = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdata/toxic_comments.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# adjust path & filename\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Check the first few rows\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(data.head())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'data/toxic_comments.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your dataset (replace with your actual file path)\n",
    "data = pd.read_csv(\"data/toxic_comments.csv\")  # adjust path & filename\n",
    "\n",
    "# Check the first few rows\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "848633d8-415d-4475-a45f-abb043d0fc54",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/combined_dataset_clean_updated_trimmed.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Load your dataset (replace with your actual file path)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m data = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdata/combined_dataset_clean_updated_trimmed.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# adjust path & filename\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Check the first few rows\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(data.head())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'data/combined_dataset_clean_updated_trimmed.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your dataset (replace with your actual file path)\n",
    "data = pd.read_csv(\"data/combined_dataset_clean_updated_trimmed.csv\")  # adjust path & filename\n",
    "\n",
    "# Check the first few rows\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6368ea0f-6ffc-4cf2-b9e3-6585e6e97963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Predictions added and saved to: C:\\Users\\anusr\\ToxicCommentFilter\\data\\combined_dataset_with_predictions.csv\n",
      "You are a genius! → ✅ Non-Toxic\n",
      "I hate your work → 🚫 Toxic\n",
      "Good luck for the exams! → ✅ Non-Toxic\n",
      "happy birthday → ✅ Non-Toxic\n",
      "ini ee strategy valare useful aanu → ✅ Non-Toxic\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "# ===== 1️⃣ Load saved model and TF-IDF vectorizer =====\n",
    "tfidf = joblib.load(r\"C:\\Users\\anusr\\ToxicCommentFilter\\models\\tfidf_vectorizer.pkl\")\n",
    "model = joblib.load(r\"C:\\Users\\anusr\\ToxicCommentFilter\\models\\toxic_comment_model.pkl\")\n",
    "\n",
    "# ===== 2️⃣ Define improved toxicity prediction function =====\n",
    "def predict_toxicity(comment):\n",
    "    comment_lower = comment.lower().strip()\n",
    "    \n",
    "    # Non-toxic phrases (greetings, wishes, encouragement)\n",
    "    non_toxic_phrases = [\n",
    "        \"happy birthday\",\n",
    "        \"congratulations\",\n",
    "        \"good luck\",\n",
    "        \"sweet dreams\",\n",
    "        \"have a nice day\",\n",
    "        \"good luck for the exams\",\n",
    "        \"you are amazing\",\n",
    "        \"you are a genius\",\n",
    "        \"ini ee strategy valare useful aanu\",\n",
    "        \"thank you\",\n",
    "        \"valare santhosham undu\",\n",
    "        \"ellavarkkum ആശംസകൾ\"\n",
    "    ]\n",
    "    \n",
    "    # Obvious toxic keywords\n",
    "    toxic_keywords = [\n",
    "        \"hate\",\n",
    "        \"worst wishes\",\n",
    "        \"shut it out\",\n",
    "        \"not good at that\",\n",
    "        \"hope you are in hell\",\n",
    "        \"bad dreams\"\n",
    "    ]\n",
    "    \n",
    "    # Check overrides\n",
    "    if any(phrase in comment_lower for phrase in non_toxic_phrases):\n",
    "        return \"✅ Non-Toxic\"\n",
    "    if any(word in comment_lower for word in toxic_keywords):\n",
    "        return \"🚫 Toxic\"\n",
    "    \n",
    "    # Default: model prediction\n",
    "    comment_vec = tfidf.transform([comment])\n",
    "    prediction = model.predict(comment_vec)[0]\n",
    "    return \"🚫 Toxic\" if prediction == 1 else \"✅ Non-Toxic\"\n",
    "\n",
    "# ===== 3️⃣ Load your dataset =====\n",
    "file_path = r\"C:\\Users\\anusr\\ToxicCommentFilter\\data\\combined_dataset_clean_updated_trimmed.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# ===== 4️⃣ Add predictions to dataset =====\n",
    "df['toxicity_prediction'] = df['Comment'].apply(predict_toxicity)\n",
    "\n",
    "# ===== 5️⃣ Save updated dataset =====\n",
    "output_file = r\"C:\\Users\\anusr\\ToxicCommentFilter\\data\\combined_dataset_with_predictions.csv\"\n",
    "df.to_csv(output_file, index=False)\n",
    "print(f\"✅ Predictions added and saved to: {output_file}\")\n",
    "\n",
    "# ===== 6️⃣ Optional: Test a few comments =====\n",
    "test_comments = [\n",
    "    \"You are a genius!\",\n",
    "    \"I hate your work\",test_comments = [\n",
    "    \"I hate your work\",\n",
    "    \"Good luck for the exams!\",\n",
    "    \"happy birthday\",\n",
    "    \"ini ee strategy valare useful aanu\"\n",
    "]\n",
    "\n",
    "for c in test_comments:\n",
    "    \"You are a genius!\",\n",
    "    print(f\"{c} → {predict_toxicity(c)}\")\n",
    "    \"Good luck for the exams!\",\n",
    "    \"happy birthday\",\n",
    "    \"ini ee strategy valare useful aanu\"\n",
    "]\n",
    "\n",
    "for c in test_comments:\n",
    "    print(f\"{c} → {predict_toxicity(c)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f2dd6208-9ec0-4de4-b83c-5fa52ecfc4df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are amaizing → ✅ Non-Toxic\n",
      "I hate your work → 🚫 Toxic\n",
      "Good luck for the exams! → ✅ Non-Toxic\n",
      "happy birthday → ✅ Non-Toxic\n",
      "ini ee strategy valare useful aanu → ✅ Non-Toxic\n"
     ]
    }
   ],
   "source": [
    "test_comments = [\n",
    "    \"You are amaizing\",\n",
    "    \"I hate your work\",\n",
    "    \"Good luck for the exams!\",\n",
    "    \"happy birthday\",\n",
    "    \"ini ee strategy valare useful aanu\"\n",
    "]\n",
    "\n",
    "for c in test_comments:\n",
    "    print(f\"{c} → {predict_toxicity(c)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a5fb51b-978a-465e-8db2-abc7b0bc4904",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_clean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m accuracy_score, classification_report, confusion_matrix\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Step 1: Use cleaned dataframe\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m df_clean = \u001b[43mdf_clean\u001b[49m.copy()  \u001b[38;5;66;03m# ensure working on cleaned df\u001b[39;00m\n\u001b[32m      8\u001b[39m X = df_clean[\u001b[33m'\u001b[39m\u001b[33mComment\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m      9\u001b[39m y = df_clean[\u001b[33m'\u001b[39m\u001b[33mlabel_binary\u001b[39m\u001b[33m'\u001b[39m].astype(\u001b[38;5;28mint\u001b[39m)  \u001b[38;5;66;03m# ensure integer labels\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'df_clean' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Step 1: Use cleaned dataframe\n",
    "df_clean = df_clean.copy()  # ensure working on cleaned df\n",
    "X = df_clean['Comment']\n",
    "y = df_clean['label_binary'].astype(int)  # ensure integer labels\n",
    "\n",
    "# Step 2: Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Step 3: Vectorize comments\n",
    "tfidf = TfidfVectorizer(max_features=5000)\n",
    "X_train_vec = tfidf.fit_transform(X_train)\n",
    "X_test_vec = tfidf.transform(X_test)\n",
    "\n",
    "# Step 4: Train logistic regression model\n",
    "model = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
    "model.fit(X_train_vec, y_train)\n",
    "\n",
    "# Step 5: Make predictions on test set\n",
    "y_pred = model.predict(X_test_vec)\n",
    "\n",
    "# Step 6: Evaluate model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ea7edc4-5a12-4415-b8b1-90771d1ce970",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Features and labels\n",
    "X = df['Comment']          # text comments\n",
    "y = df['label_binary']     # 1 = Toxic, 0 = Non-Toxic\n",
    "\n",
    "# Split into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c33cdaea-9f7d-495b-b792-a084c60bf5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create TF-IDF vectorizer\n",
    "tfidf = TfidfVectorizer(max_features=5000)  # you can adjust max_features\n",
    "\n",
    "# Fit on training data and transform both train & test\n",
    "X_train_vec = tfidf.fit_transform(X_train)\n",
    "X_test_vec = tfidf.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d9d925f8-1534-44cc-a228-08294aecc6a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "}\n",
       "\n",
       "#sk-container-id-1.light {\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: black;\n",
       "  --sklearn-color-background: white;\n",
       "  --sklearn-color-border-box: black;\n",
       "  --sklearn-color-icon: #696969;\n",
       "}\n",
       "\n",
       "#sk-container-id-1.dark {\n",
       "  --sklearn-color-text-on-default-background: white;\n",
       "  --sklearn-color-background: #111;\n",
       "  --sklearn-color-border-box: white;\n",
       "  --sklearn-color-icon: #878787;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: center;\n",
       "  justify-content: center;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  display: none;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  overflow: visible;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-3) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  border: var(--sklearn-color-fitted-level-0) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-0);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  border: var(--sklearn-color-fitted-level-0) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-0);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".estimator-table {\n",
       "    font-family: monospace;\n",
       "}\n",
       "\n",
       ".estimator-table summary {\n",
       "    padding: .5rem;\n",
       "    cursor: pointer;\n",
       "}\n",
       "\n",
       ".estimator-table summary::marker {\n",
       "    font-size: 0.7rem;\n",
       "}\n",
       "\n",
       ".estimator-table details[open] {\n",
       "    padding-left: 0.1rem;\n",
       "    padding-right: 0.1rem;\n",
       "    padding-bottom: 0.3rem;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table {\n",
       "    margin-left: auto !important;\n",
       "    margin-right: auto !important;\n",
       "    margin-top: 0;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:nth-child(odd) {\n",
       "    background-color: #fff;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:nth-child(even) {\n",
       "    background-color: #f6f6f6;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:hover {\n",
       "    background-color: #e0e0e0;\n",
       "}\n",
       "\n",
       ".estimator-table table td {\n",
       "    border: 1px solid rgba(106, 105, 104, 0.232);\n",
       "}\n",
       "\n",
       "/*\n",
       "    `table td`is set in notebook with right text-align.\n",
       "    We need to overwrite it.\n",
       "*/\n",
       ".estimator-table table td.param {\n",
       "    text-align: left;\n",
       "    position: relative;\n",
       "    padding: 0;\n",
       "}\n",
       "\n",
       ".user-set td {\n",
       "    color:rgb(255, 94, 0);\n",
       "    text-align: left !important;\n",
       "}\n",
       "\n",
       ".user-set td.value {\n",
       "    color:rgb(255, 94, 0);\n",
       "    background-color: transparent;\n",
       "}\n",
       "\n",
       ".default td {\n",
       "    color: black;\n",
       "    text-align: left !important;\n",
       "}\n",
       "\n",
       ".user-set td i,\n",
       ".default td i {\n",
       "    color: black;\n",
       "}\n",
       "\n",
       "/*\n",
       "    Styles for parameter documentation links\n",
       "    We need styling for visited so jupyter doesn't overwrite it\n",
       "*/\n",
       "a.param-doc-link,\n",
       "a.param-doc-link:link,\n",
       "a.param-doc-link:visited {\n",
       "    text-decoration: underline dashed;\n",
       "    text-underline-offset: .3em;\n",
       "    color: inherit;\n",
       "    display: block;\n",
       "    padding: .5em;\n",
       "}\n",
       "\n",
       "/* \"hack\" to make the entire area of the cell containing the link clickable */\n",
       "a.param-doc-link::before {\n",
       "    position: absolute;\n",
       "    content: \"\";\n",
       "    inset: 0;\n",
       "}\n",
       "\n",
       ".param-doc-description {\n",
       "    display: none;\n",
       "    position: absolute;\n",
       "    z-index: 9999;\n",
       "    left: 0;\n",
       "    padding: .5ex;\n",
       "    margin-left: 1.5em;\n",
       "    color: var(--sklearn-color-text);\n",
       "    box-shadow: .3em .3em .4em #999;\n",
       "    width: max-content;\n",
       "    text-align: left;\n",
       "    max-height: 10em;\n",
       "    overflow-y: auto;\n",
       "\n",
       "    /* unfitted */\n",
       "    background: var(--sklearn-color-unfitted-level-0);\n",
       "    border: thin solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       "/* Fitted state for parameter tooltips */\n",
       ".fitted .param-doc-description {\n",
       "    /* fitted */\n",
       "    background: var(--sklearn-color-fitted-level-0);\n",
       "    border: thin solid var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".param-doc-link:hover .param-doc-description {\n",
       "    display: block;\n",
       "}\n",
       "\n",
       ".copy-paste-icon {\n",
       "    background-image: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCA0NDggNTEyIj48IS0tIUZvbnQgQXdlc29tZSBGcmVlIDYuNy4yIGJ5IEBmb250YXdlc29tZSAtIGh0dHBzOi8vZm9udGF3ZXNvbWUuY29tIExpY2Vuc2UgLSBodHRwczovL2ZvbnRhd2Vzb21lLmNvbS9saWNlbnNlL2ZyZWUgQ29weXJpZ2h0IDIwMjUgRm9udGljb25zLCBJbmMuLS0+PHBhdGggZD0iTTIwOCAwTDMzMi4xIDBjMTIuNyAwIDI0LjkgNS4xIDMzLjkgMTQuMWw2Ny45IDY3LjljOSA5IDE0LjEgMjEuMiAxNC4xIDMzLjlMNDQ4IDMzNmMwIDI2LjUtMjEuNSA0OC00OCA0OGwtMTkyIDBjLTI2LjUgMC00OC0yMS41LTQ4LTQ4bDAtMjg4YzAtMjYuNSAyMS41LTQ4IDQ4LTQ4ek00OCAxMjhsODAgMCAwIDY0LTY0IDAgMCAyNTYgMTkyIDAgMC0zMiA2NCAwIDAgNDhjMCAyNi41LTIxLjUgNDgtNDggNDhMNDggNTEyYy0yNi41IDAtNDgtMjEuNS00OC00OEwwIDE3NmMwLTI2LjUgMjEuNS00OCA0OC00OHoiLz48L3N2Zz4=);\n",
       "    background-repeat: no-repeat;\n",
       "    background-size: 14px 14px;\n",
       "    background-position: 0;\n",
       "    display: inline-block;\n",
       "    width: 14px;\n",
       "    height: 14px;\n",
       "    cursor: pointer;\n",
       "}\n",
       "</style><body><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(class_weight=&#x27;balanced&#x27;, max_iter=500)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>LogisticRegression</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\" data-param-prefix=\"\">\n",
       "        <div class=\"estimator-table\">\n",
       "            <details>\n",
       "                <summary>Parameters</summary>\n",
       "                <table class=\"parameters-table\">\n",
       "                  <tbody>\n",
       "                    \n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('penalty',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.linear_model.LogisticRegression.html#:~:text=penalty,-%7B%27l1%27%2C%20%27l2%27%2C%20%27elasticnet%27%2C%20None%7D%2C%20default%3D%27l2%27\">\n",
       "            penalty\n",
       "            <span class=\"param-doc-description\">penalty: {'l1', 'l2', 'elasticnet', None}, default='l2'<br><br>Specify the norm of the penalty:<br><br>- `None`: no penalty is added;<br>- `'l2'`: add a L2 penalty term and it is the default choice;<br>- `'l1'`: add a L1 penalty term;<br>- `'elasticnet'`: both L1 and L2 penalty terms are added.<br><br>.. warning::<br>   Some penalties may not work with some solvers. See the parameter<br>   `solver` below, to know the compatibility between the penalty and<br>   solver.<br><br>.. versionadded:: 0.19<br>   l1 penalty with SAGA solver (allowing 'multinomial' + L1)<br><br>.. deprecated:: 1.8<br>   `penalty` was deprecated in version 1.8 and will be removed in 1.10.<br>   Use `l1_ratio` instead. `l1_ratio=0` for `penalty='l2'`, `l1_ratio=1` for<br>   `penalty='l1'` and `l1_ratio` set to any float between 0 and 1 for<br>   `'penalty='elasticnet'`.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">&#x27;deprecated&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('C',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.linear_model.LogisticRegression.html#:~:text=C,-float%2C%20default%3D1.0\">\n",
       "            C\n",
       "            <span class=\"param-doc-description\">C: float, default=1.0<br><br>Inverse of regularization strength; must be a positive float.<br>Like in support vector machines, smaller values specify stronger<br>regularization. `C=np.inf` results in unpenalized logistic regression.<br>For a visual example on the effect of tuning the `C` parameter<br>with an L1 penalty, see:<br>:ref:`sphx_glr_auto_examples_linear_model_plot_logistic_path.py`.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">1.0</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('l1_ratio',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.linear_model.LogisticRegression.html#:~:text=l1_ratio,-float%2C%20default%3D0.0\">\n",
       "            l1_ratio\n",
       "            <span class=\"param-doc-description\">l1_ratio: float, default=0.0<br><br>The Elastic-Net mixing parameter, with `0 <= l1_ratio <= 1`. Setting<br>`l1_ratio=1` gives a pure L1-penalty, setting `l1_ratio=0` a pure L2-penalty.<br>Any value between 0 and 1 gives an Elastic-Net penalty of the form<br>`l1_ratio * L1 + (1 - l1_ratio) * L2`.<br><br>.. warning::<br>   Certain values of `l1_ratio`, i.e. some penalties, may not work with some<br>   solvers. See the parameter `solver` below, to know the compatibility between<br>   the penalty and solver.<br><br>.. versionchanged:: 1.8<br>    Default value changed from None to 0.0.<br><br>.. deprecated:: 1.8<br>    `None` is deprecated and will be removed in version 1.10. Always use<br>    `l1_ratio` to specify the penalty type.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">0.0</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('dual',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.linear_model.LogisticRegression.html#:~:text=dual,-bool%2C%20default%3DFalse\">\n",
       "            dual\n",
       "            <span class=\"param-doc-description\">dual: bool, default=False<br><br>Dual (constrained) or primal (regularized, see also<br>:ref:`this equation <regularized-logistic-loss>`) formulation. Dual formulation<br>is only implemented for l2 penalty with liblinear solver. Prefer `dual=False`<br>when n_samples > n_features.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">False</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('tol',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.linear_model.LogisticRegression.html#:~:text=tol,-float%2C%20default%3D1e-4\">\n",
       "            tol\n",
       "            <span class=\"param-doc-description\">tol: float, default=1e-4<br><br>Tolerance for stopping criteria.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">0.0001</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('fit_intercept',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.linear_model.LogisticRegression.html#:~:text=fit_intercept,-bool%2C%20default%3DTrue\">\n",
       "            fit_intercept\n",
       "            <span class=\"param-doc-description\">fit_intercept: bool, default=True<br><br>Specifies if a constant (a.k.a. bias or intercept) should be<br>added to the decision function.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">True</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('intercept_scaling',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.linear_model.LogisticRegression.html#:~:text=intercept_scaling,-float%2C%20default%3D1\">\n",
       "            intercept_scaling\n",
       "            <span class=\"param-doc-description\">intercept_scaling: float, default=1<br><br>Useful only when the solver `liblinear` is used<br>and `self.fit_intercept` is set to `True`. In this case, `x` becomes<br>`[x, self.intercept_scaling]`,<br>i.e. a \"synthetic\" feature with constant value equal to<br>`intercept_scaling` is appended to the instance vector.<br>The intercept becomes<br>``intercept_scaling * synthetic_feature_weight``.<br><br>.. note::<br>    The synthetic feature weight is subject to L1 or L2<br>    regularization as all other features.<br>    To lessen the effect of regularization on synthetic feature weight<br>    (and therefore on the intercept) `intercept_scaling` has to be increased.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">1</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('class_weight',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.linear_model.LogisticRegression.html#:~:text=class_weight,-dict%20or%20%27balanced%27%2C%20default%3DNone\">\n",
       "            class_weight\n",
       "            <span class=\"param-doc-description\">class_weight: dict or 'balanced', default=None<br><br>Weights associated with classes in the form ``{class_label: weight}``.<br>If not given, all classes are supposed to have weight one.<br><br>The \"balanced\" mode uses the values of y to automatically adjust<br>weights inversely proportional to class frequencies in the input data<br>as ``n_samples / (n_classes * np.bincount(y))``.<br><br>Note that these weights will be multiplied with sample_weight (passed<br>through the fit method) if sample_weight is specified.<br><br>.. versionadded:: 0.17<br>   *class_weight='balanced'*</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">&#x27;balanced&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('random_state',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.linear_model.LogisticRegression.html#:~:text=random_state,-int%2C%20RandomState%20instance%2C%20default%3DNone\">\n",
       "            random_state\n",
       "            <span class=\"param-doc-description\">random_state: int, RandomState instance, default=None<br><br>Used when ``solver`` == 'sag', 'saga' or 'liblinear' to shuffle the<br>data. See :term:`Glossary <random_state>` for details.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('solver',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.linear_model.LogisticRegression.html#:~:text=solver,-%7B%27lbfgs%27%2C%20%27liblinear%27%2C%20%27newton-cg%27%2C%20%27newton-cholesky%27%2C%20%27sag%27%2C%20%27saga%27%7D%2C%20%20%20%20%20%20%20%20%20%20%20%20%20default%3D%27lbfgs%27\">\n",
       "            solver\n",
       "            <span class=\"param-doc-description\">solver: {'lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga'},             default='lbfgs'<br><br>Algorithm to use in the optimization problem. Default is 'lbfgs'.<br>To choose a solver, you might want to consider the following aspects:<br><br>- 'lbfgs' is a good default solver because it works reasonably well for a wide<br>  class of problems.<br>- For :term:`multiclass` problems (`n_classes >= 3`), all solvers except<br>  'liblinear' minimize the full multinomial loss, 'liblinear' will raise an<br>  error.<br>- 'newton-cholesky' is a good choice for<br>  `n_samples` >> `n_features * n_classes`, especially with one-hot encoded<br>  categorical features with rare categories. Be aware that the memory usage<br>  of this solver has a quadratic dependency on `n_features * n_classes`<br>  because it explicitly computes the full Hessian matrix.<br>- For small datasets, 'liblinear' is a good choice, whereas 'sag'<br>  and 'saga' are faster for large ones;<br>- 'liblinear' can only handle binary classification by default. To apply a<br>  one-versus-rest scheme for the multiclass setting one can wrap it with the<br>  :class:`~sklearn.multiclass.OneVsRestClassifier`.<br><br>.. warning::<br>   The choice of the algorithm depends on the penalty chosen (`l1_ratio=0`<br>   for L2-penalty, `l1_ratio=1` for L1-penalty and `0 < l1_ratio < 1` for<br>   Elastic-Net) and on (multinomial) multiclass support:<br><br>   ================= ======================== ======================<br>   solver            l1_ratio                 multinomial multiclass<br>   ================= ======================== ======================<br>   'lbfgs'           l1_ratio=0               yes<br>   'liblinear'       l1_ratio=1 or l1_ratio=0 no<br>   'newton-cg'       l1_ratio=0               yes<br>   'newton-cholesky' l1_ratio=0               yes<br>   'sag'             l1_ratio=0               yes<br>   'saga'            0<=l1_ratio<=1           yes<br>   ================= ======================== ======================<br><br>.. note::<br>   'sag' and 'saga' fast convergence is only guaranteed on features<br>   with approximately the same scale. You can preprocess the data with<br>   a scaler from :mod:`sklearn.preprocessing`.<br><br>.. seealso::<br>   Refer to the :ref:`User Guide <Logistic_regression>` for more<br>   information regarding :class:`LogisticRegression` and more specifically the<br>   :ref:`Table <logistic_regression_solvers>`<br>   summarizing solver/penalty supports.<br><br>.. versionadded:: 0.17<br>   Stochastic Average Gradient (SAG) descent solver. Multinomial support in<br>   version 0.18.<br>.. versionadded:: 0.19<br>   SAGA solver.<br>.. versionchanged:: 0.22<br>   The default solver changed from 'liblinear' to 'lbfgs' in 0.22.<br>.. versionadded:: 1.2<br>   newton-cholesky solver. Multinomial support in version 1.6.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">&#x27;lbfgs&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('max_iter',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.linear_model.LogisticRegression.html#:~:text=max_iter,-int%2C%20default%3D100\">\n",
       "            max_iter\n",
       "            <span class=\"param-doc-description\">max_iter: int, default=100<br><br>Maximum number of iterations taken for the solvers to converge.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">500</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('verbose',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.linear_model.LogisticRegression.html#:~:text=verbose,-int%2C%20default%3D0\">\n",
       "            verbose\n",
       "            <span class=\"param-doc-description\">verbose: int, default=0<br><br>For the liblinear and lbfgs solvers set verbose to any positive<br>number for verbosity.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">0</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('warm_start',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.linear_model.LogisticRegression.html#:~:text=warm_start,-bool%2C%20default%3DFalse\">\n",
       "            warm_start\n",
       "            <span class=\"param-doc-description\">warm_start: bool, default=False<br><br>When set to True, reuse the solution of the previous call to fit as<br>initialization, otherwise, just erase the previous solution.<br>Useless for liblinear solver. See :term:`the Glossary <warm_start>`.<br><br>.. versionadded:: 0.17<br>   *warm_start* to support *lbfgs*, *newton-cg*, *sag*, *saga* solvers.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">False</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('n_jobs',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.linear_model.LogisticRegression.html#:~:text=n_jobs,-int%2C%20default%3DNone\">\n",
       "            n_jobs\n",
       "            <span class=\"param-doc-description\">n_jobs: int, default=None<br><br>Does not have any effect.<br><br>.. deprecated:: 1.8<br>   `n_jobs` is deprecated in version 1.8 and will be removed in 1.10.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "                  </tbody>\n",
       "                </table>\n",
       "            </details>\n",
       "        </div>\n",
       "    </div></div></div></div></div><script>function copyToClipboard(text, element) {\n",
       "    // Get the parameter prefix from the closest toggleable content\n",
       "    const toggleableContent = element.closest('.sk-toggleable__content');\n",
       "    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';\n",
       "    const fullParamName = paramPrefix ? `${paramPrefix}${text}` : text;\n",
       "\n",
       "    const originalStyle = element.style;\n",
       "    const computedStyle = window.getComputedStyle(element);\n",
       "    const originalWidth = computedStyle.width;\n",
       "    const originalHTML = element.innerHTML.replace('Copied!', '');\n",
       "\n",
       "    navigator.clipboard.writeText(fullParamName)\n",
       "        .then(() => {\n",
       "            element.style.width = originalWidth;\n",
       "            element.style.color = 'green';\n",
       "            element.innerHTML = \"Copied!\";\n",
       "\n",
       "            setTimeout(() => {\n",
       "                element.innerHTML = originalHTML;\n",
       "                element.style = originalStyle;\n",
       "            }, 2000);\n",
       "        })\n",
       "        .catch(err => {\n",
       "            console.error('Failed to copy:', err);\n",
       "            element.style.color = 'red';\n",
       "            element.innerHTML = \"Failed!\";\n",
       "            setTimeout(() => {\n",
       "                element.innerHTML = originalHTML;\n",
       "                element.style = originalStyle;\n",
       "            }, 2000);\n",
       "        });\n",
       "    return false;\n",
       "}\n",
       "\n",
       "document.querySelectorAll('.copy-paste-icon').forEach(function(element) {\n",
       "    const toggleableContent = element.closest('.sk-toggleable__content');\n",
       "    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';\n",
       "    const paramName = element.parentElement.nextElementSibling\n",
       "        .textContent.trim().split(' ')[0];\n",
       "    const fullParamName = paramPrefix ? `${paramPrefix}${paramName}` : paramName;\n",
       "\n",
       "    element.setAttribute('title', fullParamName);\n",
       "});\n",
       "\n",
       "\n",
       "/**\n",
       " * Adapted from Skrub\n",
       " * https://github.com/skrub-data/skrub/blob/403466d1d5d4dc76a7ef569b3f8228db59a31dc3/skrub/_reporting/_data/templates/report.js#L789\n",
       " * @returns \"light\" or \"dark\"\n",
       " */\n",
       "function detectTheme(element) {\n",
       "    const body = document.querySelector('body');\n",
       "\n",
       "    // Check VSCode theme\n",
       "    const themeKindAttr = body.getAttribute('data-vscode-theme-kind');\n",
       "    const themeNameAttr = body.getAttribute('data-vscode-theme-name');\n",
       "\n",
       "    if (themeKindAttr && themeNameAttr) {\n",
       "        const themeKind = themeKindAttr.toLowerCase();\n",
       "        const themeName = themeNameAttr.toLowerCase();\n",
       "\n",
       "        if (themeKind.includes(\"dark\") || themeName.includes(\"dark\")) {\n",
       "            return \"dark\";\n",
       "        }\n",
       "        if (themeKind.includes(\"light\") || themeName.includes(\"light\")) {\n",
       "            return \"light\";\n",
       "        }\n",
       "    }\n",
       "\n",
       "    // Check Jupyter theme\n",
       "    if (body.getAttribute('data-jp-theme-light') === 'false') {\n",
       "        return 'dark';\n",
       "    } else if (body.getAttribute('data-jp-theme-light') === 'true') {\n",
       "        return 'light';\n",
       "    }\n",
       "\n",
       "    // Guess based on a parent element's color\n",
       "    const color = window.getComputedStyle(element.parentNode, null).getPropertyValue('color');\n",
       "    const match = color.match(/^rgb\\s*\\(\\s*(\\d+)\\s*,\\s*(\\d+)\\s*,\\s*(\\d+)\\s*\\)\\s*$/i);\n",
       "    if (match) {\n",
       "        const [r, g, b] = [\n",
       "            parseFloat(match[1]),\n",
       "            parseFloat(match[2]),\n",
       "            parseFloat(match[3])\n",
       "        ];\n",
       "\n",
       "        // https://en.wikipedia.org/wiki/HSL_and_HSV#Lightness\n",
       "        const luma = 0.299 * r + 0.587 * g + 0.114 * b;\n",
       "\n",
       "        if (luma > 180) {\n",
       "            // If the text is very bright we have a dark theme\n",
       "            return 'dark';\n",
       "        }\n",
       "        if (luma < 75) {\n",
       "            // If the text is very dark we have a light theme\n",
       "            return 'light';\n",
       "        }\n",
       "        // Otherwise fall back to the next heuristic.\n",
       "    }\n",
       "\n",
       "    // Fallback to system preference\n",
       "    return window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'light';\n",
       "}\n",
       "\n",
       "\n",
       "function forceTheme(elementId) {\n",
       "    const estimatorElement = document.querySelector(`#${elementId}`);\n",
       "    if (estimatorElement === null) {\n",
       "        console.error(`Element with id ${elementId} not found.`);\n",
       "    } else {\n",
       "        const theme = detectTheme(estimatorElement);\n",
       "        estimatorElement.classList.add(theme);\n",
       "    }\n",
       "}\n",
       "\n",
       "forceTheme('sk-container-id-1');</script></body>"
      ],
      "text/plain": [
       "LogisticRegression(class_weight='balanced', max_iter=500)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Logistic Regression with balanced class weights\n",
    "model = LogisticRegression(class_weight='balanced', max_iter=500)\n",
    "model.fit(X_train_vec, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "19d96a74-9368-49ef-9511-8f3eb162ea92",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test_vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "09fd5837-93ec-4b3e-90fb-137f09802153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.94\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.85      0.91       136\n",
      "           1       0.93      0.98      0.96       264\n",
      "\n",
      "    accuracy                           0.94       400\n",
      "   macro avg       0.95      0.92      0.93       400\n",
      "weighted avg       0.94      0.94      0.94       400\n",
      "\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mClassification Report:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m, classification_report(y_test, y_pred))\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Confusion Matrix\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m     13\u001b[39m cm = confusion_matrix(y_test, y_pred)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Accuracy\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Detailed metrics\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion Matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Non-Toxic', 'Toxic'], yticklabels=['Non-Toxic', 'Toxic'])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "faa037d2-23ad-4d09-b944-741b76adb524",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b91cc1-541d-4841-84c1-1a32a07a95ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
